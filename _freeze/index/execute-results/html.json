{
  "hash": "af1ba40c780073d9a5d7335b58cced0d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: REP or DEM <br> LSTM classification report\nauthors:\n  - name: Paul Elvis Otto\n    affiliation: Duke University / Hertie School\n    roles: writing\nbibliography: references.bib\n---\n\n## Intro\n\nClassifying tweets is among the most common applications of machine learning. This project focuses on distinguishing whether a tweet from the 112th U.S. Congress was authored by a Democrat or a Republican. The dataset, sourced from the Harvard Dataverse, includes the text of each post, information on its author, party affiliation, and whether it originated from the House or the Senate.\n\nThe classification task is carried out using a bidirectional LSTM model, described in detail in the model section. The model is trained from scratch without any pretraining, relying on pretrained embeddings imported via the MLX-Embeddings adaptation from huggingface. As a baseline, a naive Bayes classifier is implemented, and its performance is compared to the BiLSTM in the empirical assessment section.\n\nTo provide a general overview of the data, an exploratory analysis is presented in @sec-eda. The dataset has also been cleaned and prepared specifically for the BiLSTM model, with the cleaning steps detailed in @sec-data-prep.\n\n\n## Data Preparation {#sec-data-prep}\n\nTo ensure the data is usable and not influenced by elements such as dates, mentions, or links, each post was cleaned using a set of regular expressions. The cleaning script is available in the model’s repository. The cleaned dataset excludes the following:\n\n* Dates\n* Mentions\n* Hashtags\n\n\n\n## EDA {#sec-eda}\n\nThis section provides an initial overview of the dataset by examining its basic structure and several descriptive metrics.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\n\ndf <- read.csv(\"./data/congress_complete_clean.csv\")\nshape <- dim(df)\n```\n:::\n\n\nThe dataset has a dimension of 334606, 5. To begin, we look at how posts are distributed across chambers and parties. The overall distribution is visualized in @fig-post-dist.\n\n\n::: {#cell-fig-post-dist .cell}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndf_counts <- df %>%\n  count(organ, party)\n\nggplot(df_counts, aes(x = organ, y = n, fill = party)) +\n  geom_col(position = \"dodge\") +\n  labs(x = \"Chamber\", y = \"Number of posts\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![Distribution of posts by chamber and party](index_files/figure-html/fig-post-dist-1.png){#fig-post-dist width=672}\n:::\n:::\n\n\nA more detailed breakdown of total posts per chamber and party is provided in the table below:\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(gt)\n\ndf_table <- df %>%\n  count(organ, party) %>%\n  pivot_wider(\n    names_from = party,\n    values_from = n,\n    values_fill = 0\n  )\n\ndf_table %>%\n  gt(rowname_col = \"organ\") %>%\n  tab_header(\n    title = \"Number of Posts by Chamber and Party\"\n  ) %>%\n  cols_label(\n    organ = \"Chamber\"\n  )\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"uvwbajtdwn\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#uvwbajtdwn table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#uvwbajtdwn thead, #uvwbajtdwn tbody, #uvwbajtdwn tfoot, #uvwbajtdwn tr, #uvwbajtdwn td, #uvwbajtdwn th {\n  border-style: none;\n}\n\n#uvwbajtdwn p {\n  margin: 0;\n  padding: 0;\n}\n\n#uvwbajtdwn .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#uvwbajtdwn .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#uvwbajtdwn .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#uvwbajtdwn .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#uvwbajtdwn .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#uvwbajtdwn .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#uvwbajtdwn .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#uvwbajtdwn .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#uvwbajtdwn .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#uvwbajtdwn .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#uvwbajtdwn .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#uvwbajtdwn .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#uvwbajtdwn .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#uvwbajtdwn .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#uvwbajtdwn .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#uvwbajtdwn .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#uvwbajtdwn .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#uvwbajtdwn .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#uvwbajtdwn .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#uvwbajtdwn .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#uvwbajtdwn .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#uvwbajtdwn .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#uvwbajtdwn .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#uvwbajtdwn .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#uvwbajtdwn .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#uvwbajtdwn .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#uvwbajtdwn .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#uvwbajtdwn .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#uvwbajtdwn .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#uvwbajtdwn .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#uvwbajtdwn .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#uvwbajtdwn .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#uvwbajtdwn .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#uvwbajtdwn .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#uvwbajtdwn .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#uvwbajtdwn .gt_left {\n  text-align: left;\n}\n\n#uvwbajtdwn .gt_center {\n  text-align: center;\n}\n\n#uvwbajtdwn .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#uvwbajtdwn .gt_font_normal {\n  font-weight: normal;\n}\n\n#uvwbajtdwn .gt_font_bold {\n  font-weight: bold;\n}\n\n#uvwbajtdwn .gt_font_italic {\n  font-style: italic;\n}\n\n#uvwbajtdwn .gt_super {\n  font-size: 65%;\n}\n\n#uvwbajtdwn .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#uvwbajtdwn .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#uvwbajtdwn .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#uvwbajtdwn .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#uvwbajtdwn .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#uvwbajtdwn .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#uvwbajtdwn .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#uvwbajtdwn .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#uvwbajtdwn div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_heading\">\n      <td colspan=\"3\" class=\"gt_heading gt_title gt_font_normal gt_bottom_border\" style>Number of Posts by Chamber and Party</td>\n    </tr>\n    \n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a::stub\"></th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"democrat\">democrat</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"republican\">republican</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><th id=\"stub_1_1\" scope=\"row\" class=\"gt_row gt_left gt_stub\">house</th>\n<td headers=\"stub_1_1 democrat\" class=\"gt_row gt_right\">89808</td>\n<td headers=\"stub_1_1 republican\" class=\"gt_row gt_right\">167004</td></tr>\n    <tr><th id=\"stub_1_2\" scope=\"row\" class=\"gt_row gt_left gt_stub\">senate</th>\n<td headers=\"stub_1_2 democrat\" class=\"gt_row gt_right\">37890</td>\n<td headers=\"stub_1_2 republican\" class=\"gt_row gt_right\">39904</td></tr>\n  </tbody>\n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\nNext, the distribution of post lengths is examined. To limit the influence of outliers, the top one percent of longest posts are excluded:\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\n\ndf_clean <- df %>%\n  mutate(length = str_length(post)) %>%\n  group_by(party, organ) %>%\n  mutate(cutoff = quantile(length, 0.99, na.rm = TRUE)) %>%\n  ungroup() %>%\n  filter(length <= cutoff)\n\nggplot(df_clean, aes(x = party, y = length, fill = party)) +\n  geom_violin(trim = TRUE) +\n  facet_wrap(~organ) +\n  coord_cartesian(ylim = c(0, 200)) +\n  labs(\n    x = \"Party\",\n    y = \"Post length\",\n    title = \"Distribution of post lengths (top 1% removed)\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe dataset also allows identifying the most active members. The figure below displays the top five posters for each chamber–party combination:\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndf_top <- df %>%\n  count(organ, party, member, name = \"posts\") %>%\n  group_by(organ, party) %>%\n  slice_max(order_by = posts, n = 5, with_ties = FALSE) %>%\n  ungroup()\n\nggplot(df_top, aes(x = reorder(member, posts), y = posts, fill = party)) +\n  geom_col() +\n  coord_flip() +\n  facet_grid(organ ~ party, scales = \"free_y\") +\n  labs(\n    x = \"Member\",\n    y = \"Number of posts\",\n    title = \"Top 5 posters per chamber and party\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nFinally, the most frequent words used by each party are explored. After tokenizing posts, removing stopwords, and excluding a small set of custom stop terms, the top twenty terms per party are identified:\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(ggplot2)\nlibrary(stringr)\n\ncustom_stop <- tibble(word = c(\"rt\"))\n\ndf_words <- df %>%\n  mutate(post = str_to_lower(post)) %>%\n  unnest_tokens(word, post) %>%\n  anti_join(stop_words, by = \"word\") %>%\n  anti_join(custom_stop, by = \"word\") %>%\n  filter(word != \"\") %>%\n  count(party, word, sort = TRUE)\n\ntop_n_words <- 20\n\ndf_top <- df_words %>%\n  group_by(party) %>%\n  slice_max(order_by = n, n = top_n_words) %>%\n  ungroup()\n\nggplot(df_top, aes(x = reorder_within(word, n, party), y = n, fill = party)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~party, scales = \"free_y\") +\n  scale_x_reordered() +\n  labs(\n    x = \"Word\",\n    y = \"Frequency\",\n    title = \"Top word frequencies by party\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n## Model specification\n\n### Tokenization and feature extraction\n\nThe input pipeline leverages a pre-trained Transformer model, specifically `all-MiniLM-L6-v2`, to generate contextualized semantic features. Unlike traditional pipelines that feed token IDs directly to the classifier, this architecture utilizes the Transformer as a frozen feature extractor.\n\nRaw text is tokenized and padded to a fixed sequence length $T=128$. Let $\\mathbf{t}$ be the sequence of token identifiers. These are passed through the quantized (4-bit) Transformer model $\\mathcal{F}$ to yield a sequence of dense vectors:\n\n$$\n\\mathbf{X} = \\mathcal{F}(\\mathbf{t}) \\in \\mathbb{R}^{T \\times D},\n$${#eq-transformer-out}\n\nwhere $D$ is the embedding dimension (inferred from the Transformer output, typically 384 for MiniLM). Crucially, a stop-gradient operation is applied to $\\mathbf{X}$:\n\n$$\n\\mathbf{X}_{\\text{fixed}} = \\text{StopGradient}(\\mathbf{X}),\n$${#eq-stop-grad}\n\nensuring that gradients are not backpropagated into the Transformer layers, treating the embeddings as static inputs to the downstream LSTM.\n\n### Bidirectional LSTM encoder\n\nA custom implementation of a Bidirectional LSTM is employed to model the temporal dependencies within the sequence of embeddings $\\mathbf{X}_{\\text{fixed}}$.\n\nThe architecture consists of two independent LSTM layers: a forward pass ($\\overrightarrow{\\text{LSTM}}$) and a backward pass ($\\overleftarrow{\\text{LSTM}}$). For a single direction, the state update at timestep $t$, given input $\\mathbf{x}_t$ and previous hidden state $\\mathbf{h}_{t-1}$, is governed by the standard gate equations:\n\n$$\n\\begin{align}\n\\mathbf{i}_t &= \\sigma(W_i\\mathbf{x}_t + U_i\\mathbf{h}_{t-1} + \\mathbf{b}_i) \\\\\n\\mathbf{f}_t &= \\sigma(W_f\\mathbf{x}_t + U_f\\mathbf{h}_{t-1} + \\mathbf{b}_f) \\\\\n\\mathbf{o}_t &= \\sigma(W_o\\mathbf{x}_t + U_o\\mathbf{h}_{t-1} + \\mathbf{b}_o) \\\\\n\\mathbf{g}_t &= \\tanh(W_g\\mathbf{x}_t + U_g\\mathbf{h}_{t-1} + \\mathbf{b}_g) \\\\\n\\mathbf{c}_t &= \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\mathbf{g}_t \\\\\n\\mathbf{h}_t &= \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t)\n\\end{align}\n$${#eq-lstm-gates}\n\nwhere $\\sigma$ is the sigmoid function and $\\odot$ is the Hadamard product. The hidden state dimension is $H=128$. The backward LSTM processes the sequence in reverse order. The final representation at each timestep is the concatenation of both directional states:\n\n$$\n\\mathbf{h}_t = [\\overrightarrow{\\mathbf{h}}_t ; \\overleftarrow{\\mathbf{h}}_t] \\in \\mathbb{R}^{2H}.\n$${#eq-bilstm-out}\n\n### Masked mean pooling\n\nTo handle the padding artifacts present in the fixed-length sequences, a masked mean pooling operation is applied. Let $\\mathbf{m} \\in \\{0, 1\\}^T$ be the attention mask where $1$ denotes a valid token. The fixed-size sentence representation $\\mathbf{z}$ is computed as:\n\n$$\n\\mathbf{z} = \\frac{\\sum_{t=1}^{T} m_t \\mathbf{h}_t}{\\sum_{t=1}^{T} m_t + \\epsilon} \\in \\mathbb{R}^{2H},\n$${#eq-masked-pool}\n\nwhere $\\epsilon = 10^{-6}$ ensures numerical stability. This collapses the temporal dimension, resulting in a single vector capturing the global context of the post.\n\n### Classification head\n\nThe pooled vector $\\mathbf{z}$ serves as the input to the classification head. Regularization is applied via Dropout with probability $p=0.2$:\n\n$$\n\\tilde{\\mathbf{z}} = \\text{Dropout}(\\mathbf{z}).\n$${#eq-dropout}\n\nA linear projection layer maps the features to the unnormalized logits for the two classes:\n\n$$\n\\mathbf{o} = W_{out}\\tilde{\\mathbf{z}} + \\mathbf{b}_{out} \\in \\mathbb{R}^2.\n$${#eq-linear-head}\n\nThe predicted class $\\hat{y}$ is derived via the argmax of the logits.\n\n### Training objective\n\nThe model is trained using the Cross-Entropy loss function. For a batch of $N$ samples, the objective is to minimize:\n\n$$\n\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{CrossEntropy}(\\mathbf{o}^{(i)}, y^{(i)}).\n$${#eq-jross-entropy}\n\nOptimization is performed using the **Adam** optimizer with a learning rate $\\eta = 10^{-3}$. The training step, including the loss calculation and gradient update, is Just-In-Time (JIT) compiled into a comprehensive computation graph using `mx.compile` to maximize execution speed.\n\n### Computational considerations\n\nThe implementation is optimized for Apple Silicon using the MLX framework. Several specific strategies are employed for efficiency:\n\n1.  **Lazy Evaluation & Materialization:** MLX uses lazy evaluation. To prevent the computation graph from growing indefinitely during the iterative data loading process, `mx.eval` is explicitly called on batch inputs and loss values to force materialization.\n2.  **Quantized Feature Extraction:** The embedding model (`all-MiniLM-L6-v2`) utilizes 4-bit quantization, significantly reducing memory bandwidth requirements during the feature extraction phase.\n3.  **Precision:** The embeddings are cast to `float16` (`mx.float16`) before entering the LSTM, halving the memory footprint of the batch tensor compared to `float32` and leveraging the hardware's native half-precision performance.\n\n## Implementation details\n\n### Environment and reproducibility\n\n\n## Data Pipeline\nThe pipeline proceeds as:\n\n- Read CSV and keep relevant columns (text and party label). \n- Remove missing rows and normalize party strings.\n- Map party strings into binary labels and filter all others.\n- Stratified splitting into train/validation/test (80/10/10).\n- Pre-tokenize each split to input IDs and masks.\n\nPre-tokenization on CPU in large minibatches amortizes tokenizer overhead and prevents GPU under-utilization during training.\n\n### Training Loop\nThe training routine iterates for 10 epochs. Each epoch consists of:\n\n- Forward pass with autocasting.\n- Cross-entropy loss computation.\n- Backward pass on scaled loss.\n- AdamW parameter update.\n\n\nModel selection is based on validation accuracy; the best validation checkpoint is stored and reloaded before final test evaluation. DataLoaders use batch size $256$ with pinned memory and persistent workers for efficient host-device transfer.\n\n\n### Implementation in Code\n\nThe biLStM model has been in apples mlx framework to enable a bare metal run of the model on apple silicon hardware. The implementation has ben performd on top of the mlx framework provided `nn.Model` function.\n\n\nfor that a LSTM cell has been set up as follows\n\n```{{python}}\nclass LSTMCell(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.xh_to_gates = nn.Linear(input_size + hidden_size, 4 * hidden_size)\n\n    def __call__(self, x_t, state):\n        h_prev, c_prev = state\n        xh = mx.concatenate([x_t, h_prev], axis=-1)\n        gates = self.xh_to_gates(xh)\n        i, f, o, g = mx.split(gates, 4, axis=-1)\n\n        i = mx.sigmoid(i)\n        f = mx.sigmoid(f)\n        o = mx.sigmoid(o)\n        g = mx.tanh(g)\n\n        c_t = f * c_prev + i * g\n        h_t = o * mx.tanh(c_t)\n        return h_t, c_t\n```\n\nUsed in a single lstm model as follows:\n\n```{{python}}\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.cell = LSTMCell(input_size, hidden_size)\n\n    def __call__(self, x):\n        B, T, _ = x.shape\n        h = mx.zeros((B, self.hidden_size), dtype=mx.float32)\n        c = mx.zeros((B, self.hidden_size), dtype=mx.float32)\n\n        outputs = []\n        for t in range(T):\n            h, c = self.cell(x[:, t, :], (h, c))\n            outputs.append(h)\n\n        return mx.stack(outputs, axis=1), (h, c)\n\n```\n\nAnd finally the bidirectional lstm is set up along the text classification:\n\n```{{python}}\n\nclass BiLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.fwd = LSTM(input_size, hidden_size)\n        self.bwd = LSTM(input_size, hidden_size)\n\n    def __call__(self, x):\n        B, T, _ = x.shape\n\n        fwd_out, _ = self.fwd(x)\n\n        rev_idx = mx.arange(T - 1, -1, -1, dtype=mx.int32)\n        x_rev = mx.take(x, rev_idx, axis=1)\n\n        bwd_out_rev, _ = self.bwd(x_rev)\n        bwd_out = mx.take(bwd_out_rev, rev_idx, axis=1)\n\n        return mx.concatenate([fwd_out, bwd_out], axis=-1)  # (B, T, 2H)\n\n\nclass BiLSTMTextClassifier(nn.Module):\n    def __init__(self, embedding_dim, hidden_size, num_classes, dropout=0.1):\n        super().__init__()\n        self.bilstm = BiLSTM(embedding_dim, hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.head = nn.Linear(2 * hidden_size, num_classes)\n\n    def pool(self, seq_out, mask):\n        mask_f = mask.astype(mx.float32)[..., None]\n        summed = mx.sum(seq_out * mask_f, axis=1)\n        denom = mx.maximum(mx.sum(mask_f, axis=1), 1e-6)\n        return summed / denom\n\n    def __call__(self, x, mask):\n        # zero out padding before recurrence\n        x = x * mask.astype(mx.float32)[..., None]\n        seq_out = self.bilstm(x)\n        pooled = self.pool(seq_out, mask)\n        pooled = self.dropout(pooled)\n        return self.head(pooled)\n\n\n```\n\nTo use the framework to it's full poterntial the model its is compiled\n\n```{{python}}\n@partial(mx.compile, inputs=state, outputs=state)\ndef step(Xb, yb, Mb):\n    loss, grads = loss_and_grad(model, Xb, yb, Mb)\n    optimizer.update(model, grads)\n    return loss\n\n```\n\nThe description of the loss function is omitted but can be found in the complete code in the repository\n\n\n## Naive Bayes Baseline {#sec-nbb}\n\nAs a classical probabilistic benchmark, a Naive Bayes classifier is employed alongside the neural sequence model. To ensure comparability, all preprocessing steps prior to feature extraction—data cleaning, label normalization, and the exact train/validation/test split—are identical to those used for the BiLSTM classifier.\n\n### Model description\n\nIn contrast to the sequence-based neural architecture, the Naive Bayes model operates on a sparse bag-of-ngrams representation of the text. Each document $x$ is mapped to a TF–IDF feature vector\n$$\n\\mathbf{f}(x) = (f_1, f_2, \\ldots, f_d),\n$${#eq-eq-tfidf}\n\nwhere each $f_j$ reflects the TF–IDF weight of term $j$ after vocabulary truncation and filtering. The vectorizer uses standard preprocessing settings for text classification:\n\n* lowercasing,\n* English stop-word removal,\n* unigram and bigram features ((1,2)),\n* a maximum feature cap of (d = 50{,}000),\n* a minimum document frequency of (2).\n\nThe classifier itself is a Multinomial Naive Bayes model, which assumes conditional independence of features given a class label $y\\in{0,1}$. Under this assumption, the likelihood of observing the feature vector $\\mathbf{f}(x)$ given class $k$ factorizes as\n\n$$\np(\\mathbf{f}(x)\\mid y=k)\n;\\propto;\n\\prod_{j=1}^{d}\n\\theta_{kj}^{, f_j(x)},\n$${#eq-nb-likelihood}\n\nwhere $\\theta_{kj}$ are class-conditional feature probabilities estimated from the training set. Bayes’ rule then yields the posterior\n\n$$\np(y=k \\mid x)\n;\\propto;\np(y=k)\\prod_{j=1}^{d}\\theta_{kj}^{, f_j(x)},\n$${#eq-nb-posterior}\n\nand prediction is made via\n\n$$\n\\hat{y} = \\arg\\max_{k\\in{0,1}} p(y=k\\mid x).\n$${#eq-nb-prediction}\n\nAlthough the multinomial model is derived for count data, it is widely used with TF–IDF features and remains a strong linear baseline in high-dimensional text classification.\n\n\n## Empirical Assessments\n\nThe following section will show the outcomes of the different test summaries of the used Models, bilstm, and naive bayes\n\n### BILSTM\n\n#### Headline results\n\n\nOn the test partition, the model achieves:\n\n$$\n\\text{Test loss}=0.3307,\\qquad\n\\text{Test accuracy}=0.8635.\n$$\n\n\nThe closeness of validation-selected performance and test performance indicates stable generalization without severe overfitting.\n\n#### Class-wise metrics\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(tibble)\n\nclsrep <- tribble(\n  ~Class,\n  ~Precision,\n  ~Recall,\n  ~F1,\n  ~Support,\n  \"0\",\n  0.7621,\n  0.6748,\n  0.7158,\n  12729,\n  \"1\",\n  0.8121,\n  0.8697,\n  0.8399,\n  20573,\n  \"Accuracy\",\n  NA,\n  NA,\n  0.7952,\n  33302,\n  \"Macro avg\",\n  0.7871,\n  0.7723,\n  0.7779,\n  33302,\n  \"Weighted avg\",\n  0.7930,\n  0.7952,\n  0.7925,\n  33302\n)\n\nlibrary(knitr)\nkable(clsrep, caption = \"Test-set classification report.\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Test-set classification report.\n\n|Class        | Precision| Recall|     F1| Support|\n|:------------|---------:|------:|------:|-------:|\n|0            |    0.7621| 0.6748| 0.7158|   12729|\n|1            |    0.8121| 0.8697| 0.8399|   20573|\n|Accuracy     |        NA|     NA| 0.7952|   33302|\n|Macro avg    |    0.7871| 0.7723| 0.7779|   33302|\n|Weighted avg |    0.7930| 0.7952| 0.7925|   33302|\n\n\n:::\n\n```{.r .cell-code .hidden}\ntest_acc_lstm <- 0.7952074950453426\ntest_loss_lstm <- 0.5228507898691046\n```\n:::\n\n\nPerformance is stronger for class $1$, consistent with its higher prevalence. The macro-averaged F$_1$ is slightly lower than the weighted average, reflecting imbalance-driven asymmetry.\n\nThe overall Test accuracy is 0.7952075 and the overall Test loss 0.5228508.\n\n\n#### Confusion matrix and derived rates\n\n$$\n\\mathbf{C}=\n\\begin{pmatrix}\n8590 & 4139\\\\\n2681 & 17892\n\\end{pmatrix}.\n$$\n\nKey derived quantities (treating class $1$ as positive) are:\n\n$$\n\\begin{align}\n\\text{TPR (recall$_1$)} &= \\frac{18666}{18666+2025}=0.9021, \\\\\n\\text{TNR (specificity)} &= \\frac{10226}{10226+2544}=0.8008, \\\\\n\\text{FPR} &= \\frac{2544}{10226+2544}=0.1992, \\\\\n\\text{FNR} &= \\frac{2025}{18666+2025}=0.0979, \\\\\n\\text{Balanced accuracy} &= \\frac{0.9021+0.8008}{2}=0.8515, \\\\\n\\text{MCC} &= 0.7088.\n\\end{align}\n$$\n\n#### Error profile\n\nInspection of the confusion matrix suggests two dominant error types:\n\n- Class-0 $\\rightarrow$ Class-1 errors (2544 cases) likely driven by lexical overlap or stylistic convergence in the two parties’ rhetoric, aggravated by class imbalance.\n\n- Class-1 $\\rightarrow$ Class-0 errors (2025 cases) fewer and may correspond to moderate or cross-partisan language patterns.\n\nThe false-positive rate is roughly twice the false-negative rate, indicating that the classifier is more inclined to assign class 1 to items belonging to class 0 than the reverse. In scenarios where asymmetric error costs matter, this imbalance could be mitigated through threshold tuning, class-weighted loss functions, or calibration techniques.\n\n### Naive Bayes Baseline\n\nEvaluated on the held-out test set, the Naive Bayes classifier achieves an accuracy of\n\n$$\n\\text{Test accuracy} = 0.8447.\n$$\n\nThis performance is slightly below that of the BiLSTM model yet remains strong given its simplicity and lack of contextual or sequential modeling.\n\nA detailed breakdown of class-wise test metrics is as follows:\n\n* **Class 0** (Democrat)\n  precision: (0.8258), recall: (0.7517), F1: (0.7870), support: (12{,}770)\n\n* **Class 1** (Republican)\n  precision: (0.8548), recall: (0.9021), F1: (0.8778), support: (20{,}691)\n\nThe model shows a pronounced asymmetry: recall for class 1 is high (0.9021), whereas class-0 recall is comparatively low (0.7517). This indicates a stronger tendency to classify documents as class 1, consistent with the underlying class imbalance and the linear nature of the decision boundary.\n\nThe test confusion matrix\n$$\n\\begin{pmatrix}\n9599 & 3171 \\\n2025 & 18666\n\\end{pmatrix}\n$$\n\nreveals that false positives for class 1 (3171 cases) are moderately higher than in the BiLSTM model. False negatives for class 1 (2025 cases) are identical for both models, an artifact of the fixed data split.\n\nOverall, the Naive Bayes baseline provides a meaningful reference point: it captures strong lexical signals and performs well with minimal computational cost, yet falls short of the sequence model in reducing class-specific error rates and balancing performance across labels.\n\n\nthe model can be evaluated as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(tibble)\n\nnb <- tribble(\n  ~Class,\n  ~Precision,\n  ~Recall,\n  ~F1,\n  ~Support,\n  \"0\",\n  0.7698,\n  0.6214,\n  0.6877,\n  12729,\n  \"1\",\n  0.7907,\n  0.8850,\n  0.8352,\n  20573,\n  \"Accuracy\",\n  NA,\n  NA,\n  0.7843,\n  33302,\n  \"Macro avg\",\n  0.7803,\n  0.7532,\n  0.7615,\n  33302,\n  \"Weighted avg\",\n  0.7827,\n  0.7843,\n  0.7788,\n  33302\n)\n\nlibrary(knitr)\nkable(nb, caption = \"Test-set classification report.\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Test-set classification report.\n\n|Class        | Precision| Recall|     F1| Support|\n|:------------|---------:|------:|------:|-------:|\n|0            |    0.7698| 0.6214| 0.6877|   12729|\n|1            |    0.7907| 0.8850| 0.8352|   20573|\n|Accuracy     |        NA|     NA| 0.7843|   33302|\n|Macro avg    |    0.7803| 0.7532| 0.7615|   33302|\n|Weighted avg |    0.7827| 0.7843| 0.7788|   33302|\n\n\n:::\n:::\n\n\n\n\n### Comparative perspective\n\nTo analyse whether there is a improvement of the BiLSTM over the Naive Bayes model,  a side-by-side comparison of key metrics in @tbl-comp is presented\n\nA BiLSTM with masked pooling forms a strong sequential baseline, especially when combined with pretrained subword embeddings. Pretrained embeddings provide stable lexical and subword representations, reducing the burden on the model to learn low-level statistics from scratch. However, these embeddings are not contextual: they do not adapt representations based on surrounding tokens. As a result, the model cannot capture the full range of contextual nuances that transformer-based encoders provide. The observed performance suggests that pretrained embeddings are beneficial, but the absence of contextualization imposes an upper bound compared with modern transformer architectures.\n\n\n\n\n::: {#tbl-comp .cell tbl-cap='Model Comparison'}\n\n```{.r .cell-code .hidden}\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(gt)\n\n# 1. Setup Data -----------------------------------------------------------\n# (Same data setup as before)\n\nnb %>%\n  mutate(Class = clsrep$Class) %>%\n  invisible()\n\n# Merge data wide\ndf_wide <- clsrep %>%\n  rename_with(~ paste0(., \"_BiLSTM\"), -Class) %>%\n  left_join(nb %>% rename_with(~ paste0(., \"_NB\"), -Class), by = \"Class\")\n\n# 2. The GT Table ---------------------------------------------------------\n\ndf_wide %>%\n  gt() %>%\n  # --- 1. Structure & Labels ---\n  tab_spanner(label = \"BiLSTM\", columns = ends_with(\"_BiLSTM\")) %>%\n  tab_spanner(label = \"Naive Bayes\", columns = ends_with(\"_NB\")) %>%\n  cols_label(\n    Precision_BiLSTM = \"Precision\",\n    Recall_BiLSTM = \"Recall\",\n    F1_BiLSTM = \"F1\",\n    Support_BiLSTM = \"Support\",\n    Precision_NB = \"Precision\",\n    Recall_NB = \"Recall\",\n    F1_NB = \"F1\",\n    Support_NB = \"Support\"\n  ) %>%\n  sub_missing(missing_text = \"-\") %>%\n\n  # --- 2. Visual Separation ---\n  # Add a thick border to the right of the BiLSTM section to separate the models\n  tab_style(\n    style = cell_borders(sides = \"right\", color = \"#d3d3d3\", weight = px(2)),\n    locations = cells_body(columns = \"Support_BiLSTM\")\n  ) %>%\n\n  # --- 3. Highlight Logic (The Advantage Check) ---\n\n  # >>> PRECISION COMPARISON <<<\n  # Highlight BiLSTM if it is higher\n  tab_style(\n    style = cell_fill(color = \"#d4f0d4\"),\n    locations = cells_body(\n      columns = \"Precision_BiLSTM\",\n      rows = Precision_BiLSTM > Precision_NB\n    )\n  ) %>%\n  # Highlight NB if it is higher\n  tab_style(\n    style = cell_fill(color = \"#d4f0d4\"),\n    locations = cells_body(\n      columns = \"Precision_NB\",\n      rows = Precision_NB > Precision_BiLSTM\n    )\n  ) %>%\n\n  # >>> RECALL COMPARISON <<<\n  tab_style(\n    style = cell_fill(color = \"#d4f0d4\"),\n    locations = cells_body(\n      columns = \"Recall_BiLSTM\",\n      rows = Recall_BiLSTM > Recall_NB\n    )\n  ) %>%\n  tab_style(\n    style = cell_fill(color = \"#d4f0d4\"),\n    locations = cells_body(\n      columns = \"Recall_NB\",\n      rows = Recall_NB > Recall_BiLSTM\n    )\n  ) %>%\n\n  # >>> F1 COMPARISON <<<\n  tab_style(\n    style = cell_fill(color = \"#d4f0d4\"),\n    locations = cells_body(\n      columns = \"F1_BiLSTM\",\n      rows = F1_BiLSTM > F1_NB\n    )\n  ) %>%\n  tab_style(\n    style = cell_fill(color = \"#d4f0d4\"),\n    locations = cells_body(\n      columns = \"F1_NB\",\n      rows = F1_NB > F1_BiLSTM\n    )\n  ) %>%\n\n  # Add Headers\n  tab_header(\n    title = \"Model Comparison\",\n    subtitle = \"Cells highlighted indicate the winning model for that specific metric\"\n  )\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"pjprzgbdoq\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#pjprzgbdoq table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#pjprzgbdoq thead, #pjprzgbdoq tbody, #pjprzgbdoq tfoot, #pjprzgbdoq tr, #pjprzgbdoq td, #pjprzgbdoq th {\n  border-style: none;\n}\n\n#pjprzgbdoq p {\n  margin: 0;\n  padding: 0;\n}\n\n#pjprzgbdoq .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#pjprzgbdoq .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#pjprzgbdoq .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#pjprzgbdoq .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#pjprzgbdoq .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#pjprzgbdoq .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#pjprzgbdoq .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#pjprzgbdoq .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#pjprzgbdoq .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#pjprzgbdoq .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#pjprzgbdoq .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#pjprzgbdoq .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#pjprzgbdoq .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#pjprzgbdoq .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#pjprzgbdoq .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#pjprzgbdoq .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#pjprzgbdoq .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#pjprzgbdoq .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#pjprzgbdoq .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#pjprzgbdoq .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#pjprzgbdoq .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#pjprzgbdoq .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#pjprzgbdoq .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#pjprzgbdoq .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#pjprzgbdoq .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#pjprzgbdoq .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#pjprzgbdoq .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#pjprzgbdoq .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#pjprzgbdoq .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#pjprzgbdoq .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#pjprzgbdoq .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#pjprzgbdoq .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#pjprzgbdoq .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#pjprzgbdoq .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#pjprzgbdoq .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#pjprzgbdoq .gt_left {\n  text-align: left;\n}\n\n#pjprzgbdoq .gt_center {\n  text-align: center;\n}\n\n#pjprzgbdoq .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#pjprzgbdoq .gt_font_normal {\n  font-weight: normal;\n}\n\n#pjprzgbdoq .gt_font_bold {\n  font-weight: bold;\n}\n\n#pjprzgbdoq .gt_font_italic {\n  font-style: italic;\n}\n\n#pjprzgbdoq .gt_super {\n  font-size: 65%;\n}\n\n#pjprzgbdoq .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#pjprzgbdoq .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#pjprzgbdoq .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#pjprzgbdoq .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#pjprzgbdoq .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#pjprzgbdoq .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#pjprzgbdoq .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#pjprzgbdoq .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#pjprzgbdoq div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_heading\">\n      <td colspan=\"9\" class=\"gt_heading gt_title gt_font_normal\" style>Model Comparison</td>\n    </tr>\n    <tr class=\"gt_heading\">\n      <td colspan=\"9\" class=\"gt_heading gt_subtitle gt_font_normal gt_bottom_border\" style>Cells highlighted indicate the winning model for that specific metric</td>\n    </tr>\n    <tr class=\"gt_col_headings gt_spanner_row\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"2\" colspan=\"1\" scope=\"col\" id=\"Class\">Class</th>\n      <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"4\" scope=\"colgroup\" id=\"BiLSTM\">\n        <div class=\"gt_column_spanner\">BiLSTM</div>\n      </th>\n      <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"4\" scope=\"colgroup\" id=\"Naive Bayes\">\n        <div class=\"gt_column_spanner\">Naive Bayes</div>\n      </th>\n    </tr>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Precision_BiLSTM\">Precision</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Recall_BiLSTM\">Recall</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"F1_BiLSTM\">F1</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Support_BiLSTM\">Support</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Precision_NB\">Precision</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Recall_NB\">Recall</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"F1_NB\">F1</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Support_NB\">Support</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"Class\" class=\"gt_row gt_left\">0</td>\n<td headers=\"Precision_BiLSTM\" class=\"gt_row gt_right\">0.7621</td>\n<td headers=\"Recall_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.6748</td>\n<td headers=\"F1_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7158</td>\n<td headers=\"Support_BiLSTM\" class=\"gt_row gt_right\" style=\"border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3;\">12729</td>\n<td headers=\"Precision_NB\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7698</td>\n<td headers=\"Recall_NB\" class=\"gt_row gt_right\">0.6214</td>\n<td headers=\"F1_NB\" class=\"gt_row gt_right\">0.6877</td>\n<td headers=\"Support_NB\" class=\"gt_row gt_right\">12729</td></tr>\n    <tr><td headers=\"Class\" class=\"gt_row gt_left\">1</td>\n<td headers=\"Precision_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.8121</td>\n<td headers=\"Recall_BiLSTM\" class=\"gt_row gt_right\">0.8697</td>\n<td headers=\"F1_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.8399</td>\n<td headers=\"Support_BiLSTM\" class=\"gt_row gt_right\" style=\"border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3;\">20573</td>\n<td headers=\"Precision_NB\" class=\"gt_row gt_right\">0.7907</td>\n<td headers=\"Recall_NB\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.8850</td>\n<td headers=\"F1_NB\" class=\"gt_row gt_right\">0.8352</td>\n<td headers=\"Support_NB\" class=\"gt_row gt_right\">20573</td></tr>\n    <tr><td headers=\"Class\" class=\"gt_row gt_left\">Accuracy</td>\n<td headers=\"Precision_BiLSTM\" class=\"gt_row gt_right\">-</td>\n<td headers=\"Recall_BiLSTM\" class=\"gt_row gt_right\">-</td>\n<td headers=\"F1_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7952</td>\n<td headers=\"Support_BiLSTM\" class=\"gt_row gt_right\" style=\"border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3;\">33302</td>\n<td headers=\"Precision_NB\" class=\"gt_row gt_right\">-</td>\n<td headers=\"Recall_NB\" class=\"gt_row gt_right\">-</td>\n<td headers=\"F1_NB\" class=\"gt_row gt_right\">0.7843</td>\n<td headers=\"Support_NB\" class=\"gt_row gt_right\">33302</td></tr>\n    <tr><td headers=\"Class\" class=\"gt_row gt_left\">Macro avg</td>\n<td headers=\"Precision_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7871</td>\n<td headers=\"Recall_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7723</td>\n<td headers=\"F1_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7779</td>\n<td headers=\"Support_BiLSTM\" class=\"gt_row gt_right\" style=\"border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3;\">33302</td>\n<td headers=\"Precision_NB\" class=\"gt_row gt_right\">0.7803</td>\n<td headers=\"Recall_NB\" class=\"gt_row gt_right\">0.7532</td>\n<td headers=\"F1_NB\" class=\"gt_row gt_right\">0.7615</td>\n<td headers=\"Support_NB\" class=\"gt_row gt_right\">33302</td></tr>\n    <tr><td headers=\"Class\" class=\"gt_row gt_left\">Weighted avg</td>\n<td headers=\"Precision_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7930</td>\n<td headers=\"Recall_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7952</td>\n<td headers=\"F1_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7925</td>\n<td headers=\"Support_BiLSTM\" class=\"gt_row gt_right\" style=\"border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3;\">33302</td>\n<td headers=\"Precision_NB\" class=\"gt_row gt_right\">0.7827</td>\n<td headers=\"Recall_NB\" class=\"gt_row gt_right\">0.7843</td>\n<td headers=\"F1_NB\" class=\"gt_row gt_right\">0.7788</td>\n<td headers=\"Support_NB\" class=\"gt_row gt_right\">33302</td></tr>\n  </tbody>\n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\n\n\n\n## Conclusion\n\nThe implemented pipeline offers a computationally efficient neural baseline for binary party attribution in political texts. By combining subword token IDs with trainable embeddings and a BiLSTM encoder, the system captures sequential patterns and achieves solid test performance (accuracy $0.8635$, MCC $0.7088$). Evaluation reveals a mild bias toward predicting the majority class, visible in an elevated false-positive rate for class $0$. The model’s simplicity, GPU-friendly batching, and stable selection via validation accuracy make it a robust starting point. Future work should focus on incorporating pretrained contextual representations and imbalance-aware objectives to further improve minority-class recall and overall calibration.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
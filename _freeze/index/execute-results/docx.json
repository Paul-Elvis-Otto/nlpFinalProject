{
  "hash": "12a05f3ae192da08f30d331dc7222b9d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: REP or DEM <br> LSTM classification report\nauthors:\n  - name: Paul Elvis Otto\n    affiliation: Duke University / Hertie School\n    roles: writing\nbibliography: references.bib\n---\n\n## Intro\n\nThe presented report focuses on using a bilstm to classify tweets into wherther they are made by a republican or a democrat.\nFor the task the tweets of the 112th congress have been used, House and democracts.\nTo establish a baseline comparison a naive bayes classifier has been used.\n\n\n\n\n## EDA\n\nTo give an insisht into the dataset first there are some basic metrics\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ndf <- read.csv(\"./data/congress_complete_clean.csv\")\nshape <- dim(df)\n```\n:::\n\n\n\nThe dataset has the shape of 334606, 5 and the posts are distributed as follows:\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndf_counts <- df %>%\n  count(organ, party)\n\nggplot(df_counts, aes(x = organ, y = n, fill = party)) +\n  geom_col(position = \"dodge\") +\n  labs(x = \"Chamber\", y = \"Number of posts\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/unnamed-chunk-2-1.png)\n:::\n:::\n\n\n\n\nThe distribution of post lengths is as follows:\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\n\ndf_clean <- df %>%\n  mutate(length = str_length(post)) %>%\n  group_by(party, organ) %>%\n  mutate(cutoff = quantile(length, 0.99, na.rm = TRUE)) %>%\n  ungroup() %>%\n  filter(length <= cutoff)\n\nggplot(df_clean, aes(x = party, y = length, fill = party)) +\n  geom_violin(trim = TRUE) +\n  facet_wrap(~organ) +\n  coord_cartesian(ylim = c(0, 200)) + # set y-axis range\n  labs(\n    x = \"Party\",\n    y = \"Post length\",\n    title = \"Distribution of post lengths (top 1% removed)\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/unnamed-chunk-3-1.png)\n:::\n:::\n\n\n\nThe top posters in the dataset: \n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndf_top <- df %>%\n  count(organ, party, member, name = \"posts\") %>% # count posts\n  group_by(organ, party) %>% # group by chamber + party\n  slice_max(order_by = posts, n = 5, with_ties = FALSE) %>% # keep top 5 only\n  ungroup()\n\nggplot(df_top, aes(x = reorder(member, posts), y = posts, fill = party)) +\n  geom_col() +\n  coord_flip() +\n  facet_grid(organ ~ party, scales = \"free_y\") +\n  labs(\n    x = \"Member\",\n    y = \"Number of posts\",\n    title = \"Top 5 posters per chamber and party\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/unnamed-chunk-4-1.png)\n:::\n:::\n\n\n\nSome word frequencies by party\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(ggplot2)\nlibrary(stringr)\n\n# optional: custom stopwords\ncustom_stop <- tibble(word = c(\"rt\"))\n\n# prepare word frequencies\ndf_words <- df %>%\n  mutate(post = str_to_lower(post)) %>%\n  unnest_tokens(word, post) %>%\n  anti_join(stop_words, by = \"word\") %>%\n  anti_join(custom_stop, by = \"word\") %>%\n  filter(word != \"\") %>%\n  count(party, word, sort = TRUE)\n\n# select top N words per party\ntop_n_words <- 20\n\ndf_top <- df_words %>%\n  group_by(party) %>%\n  slice_max(order_by = n, n = top_n_words) %>%\n  ungroup()\n\n# plot side-by-side word frequency charts\nggplot(df_top, aes(x = reorder_within(word, n, party), y = n, fill = party)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~party, scales = \"free_y\") +\n  scale_x_reordered() +\n  labs(\n    x = \"Word\",\n    y = \"Frequency\",\n    title = \"Top word frequencies by party\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/unnamed-chunk-5-1.png)\n:::\n:::\n\n\n\n## New Modle chapter\n\n\n## Model specification\n\n\n\n### Tokenization and inputs\n\n\nEach text is mapped to a fixed-length sequence of subword token identifiers using a WordPiece-style tokenizer with vocabulary size $V$. After padding or truncation to length $T=128$, each sample is represented by:\n\n$$\n\\mathbf{t}=(t_1,\\dots,t_T),\\quad t_i \\in \\{1,\\dots,V\\},\n$${#eq-token-seq}\n\nand a binary mask\n\n$$\n\\mathbf{m}=(m_1,\\dots,m_T),\\quad m_i\\in\\{0,1\\},\n$${#eq-mask-seq}\n\nwhere $m_i=1$ indicates a true token and $m_i=0$ indicates padding. The tokenizer is used only to obtain indices; semantic representations are learned by the network.\n\n### Trainable embedding layer\n\nLet $E\\in\\mathbb{R}^{V\\times d}$ be the embedding matrix with $d=200$. Tokens are embedded as\n\n$$\n\\mathbf{x}_i = E[t_i]\\in\\mathbb{R}^{d},\n\\quad\n\\mathbf{X}=(\\mathbf{x}_1,\\dots,\\mathbf{x}_T)\\in\\mathbb{R}^{T\\times d}.\n$${#eq-embed-seq}\n\nThe padding index is set so that padded positions do not update embeddings.\n\n### Bidirectional LSTM encoder\n\nA forward LSTM and a backward LSTM encode $\\mathbf{X}$. For a single LSTM direction, gates and state updates at timestep $i$ are:\n\n$$\n\\begin{align}\n\\mathbf{i}_i &= \\sigma(W_i\\mathbf{x}_i + U_i\\mathbf{h}_{i-1} + \\mathbf{b}_i), \\\\\n\\mathbf{f}_i &= \\sigma(W_f\\mathbf{x}_i + U_f\\mathbf{h}_{i-1} + \\mathbf{b}_f), \\\\\n\\mathbf{o}_i &= \\sigma(W_o\\mathbf{x}_i + U_o\\mathbf{h}_{i-1} + \\mathbf{b}_o), \\\\\n\\tilde{\\mathbf{c}}_i &= \\tanh(W_c\\mathbf{x}_i + U_c\\mathbf{h}_{i-1} + \\mathbf{b}_c), \\\\\n\\mathbf{c}_i &= \\mathbf{f}_i\\odot \\mathbf{c}_{i-1} + \\mathbf{i}_i\\odot \\tilde{\\mathbf{c}}_i, \\\\\n\\mathbf{h}_i &= \\mathbf{o}_i\\odot \\tanh(\\mathbf{c}_i),\n\\end{align}\n$${#eq-lstm-gates}\n\nwith sigmoid $\\sigma(\\cdot)$ and elementwise product $\\odot$. The hidden size is $H=128$ per direction. The bidirectional representation at position $i$ is\n\n$$\n\\mathbf{h}_i = [\\overrightarrow{\\mathbf{h}}_i ; \\overleftarrow{\\mathbf{h}}_i] \\in \\mathbb{R}^{2H},\n$${#eq-bilstm-concat}\n\nyielding a sequence output\n\n$$\n\\mathbf{H}=(\\mathbf{h}_1,\\dots,\\mathbf{h}_T)\\in\\mathbb{R}^{T\\times 2H}.\n$${#eq-seq-output}\n\n### Masked mean pooling\n\n\nTo obtain a fixed-dimensional vector while excluding padding, the model computes:\n\n$$\n\\mathbf{z}=\n\\frac{\\sum_{i=1}^{T} m_i \\mathbf{h}_i}\n{\\sum_{i=1}^{T} m_i + \\varepsilon}\n\\in\\mathbb{R}^{2H},\n$${#eq-fix-dim-vex}\n\nwhere $\\varepsilon$ is a small constant to avoid division by zero.\n\n### Classification head\nDropout is applied to $\\mathbf{z}$ with probability $p=0.2$:\n\n$$\n\\tilde{\\mathbf{z}}=\\mathrm{Dropout}(\\mathbf{z}).\n$$\n\nA linear layer produces logits:\n\n$$\n\\mathbf{o}=W\\tilde{\\mathbf{z}}+\\mathbf{b}\\in\\mathbb{R}^{2}.\n$$\n\nThe predictive distribution is\n\n$$\n\\hat{\\mathbf{y}}=\\mathrm{softmax}(\\mathbf{o}),\n\\qquad\n\\hat{y}=\\arg\\max_{k\\in\\{0,1\\}} \\hat{\\mathbf{y}}_k.\n$$\n\n### Training objective\n\nParameters $\\theta$ are learned by minimizing the cross-entropy loss:\n\n$$\n\\mathcal{L}(\\theta)\n=\n-\\frac{1}{N}\n\\sum_{n=1}^{N}\n\\log \\hat{\\mathbf{y}}^{(n)}_{y^{(n)}}.\n$$\n\nOptimization uses AdamW with learning rate $10^{-3}$. Mixed-precision training (automatic casting to FP16 with dynamic gradient scaling) is employed to improve throughput and numerical stability on modern GPUs.\n\n### Computational considerations\nFor each batch, the dominant cost arises from the BiLSTM. For sequence length $T$ and hidden size $H$, time complexity scales as $\\mathcal{O}(T\\cdot H^2)$ per direction, and memory scales as $\\mathcal{O}(T\\cdot H)$ for storing activations. Fixing $T=128$ stabilizes runtime and enables cuDNN kernel autotuning.\n\n## Implementation details\n\n### Environment and reproducibility\n\nTraining is performed on CUDA hardware with cuDNN benchmarking activated, selecting optimized kernels for the fixed tensor shapes. Random seeds are set for Python, NumPy, and PyTorch (CPU and GPU) to reduce run-to-run variation.\n\n## Data Pipeline\nThe pipeline proceeds as:\n\n- Read CSV and keep relevant columns (text and party label). \n- Remove missing rows and normalize party strings.\n- Map party strings into binary labels and filter all others.\n- Stratified splitting into train/validation/test (80/10/10).\n- Pre-tokenize each split to input IDs and masks.\n\nPre-tokenization on CPU in large minibatches amortizes tokenizer overhead and prevents GPU under-utilization during training.\n\n### Training Loop\nThe training routine iterates for 10 epochs. Each epoch consists of:\n\n- Forward pass with autocasting.\n- Cross-entropy loss computation.\n- Backward pass on scaled loss.\n- AdamW parameter update.\n\n\nModel selection is based on validation accuracy; the best validation checkpoint is stored and reloaded before final test evaluation. DataLoaders use batch size $256$ with pinned memory and persistent workers for efficient host-device transfer.\n\n\n## Empirical Assessments\n\n\n### Headline results\n\n\nOn the test partition, the model achieves:\n\n$$\n\\text{Test loss}=0.3307,\\qquad\n\\text{Test accuracy}=0.8635.\n$$\n\n\nThe closeness of validation-selected performance and test performance indicates stable generalization without severe overfitting.\n\n### Class-wise metrics\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(tibble)\n\nlibrary(tibble)\n\nclsrep <- tribble(\n  ~Class,\n  ~Precision,\n  ~Recall,\n  ~F1,\n  ~Support,\n  \"0\",\n  0.8347,\n  0.8008,\n  0.8174,\n  12770,\n  \"1\",\n  0.8801,\n  0.9021,\n  0.8910,\n  20691,\n  \"Accuracy\",\n  0.8635,\n  NA,\n  NA,\n  33461,\n  \"Macro avg\",\n  0.8574,\n  0.8515,\n  0.8542,\n  33461,\n  \"Weighted avg\",\n  0.8627,\n  0.8635,\n  0.8629,\n  33461\n)\n\nlibrary(knitr)\nkable(clsrep, caption = \"Test-set classification report.\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Test-set classification report.\n\n|Class        | Precision| Recall|     F1| Support|\n|:------------|---------:|------:|------:|-------:|\n|0            |    0.8347| 0.8008| 0.8174|   12770|\n|1            |    0.8801| 0.9021| 0.8910|   20691|\n|Accuracy     |    0.8635|     NA|     NA|   33461|\n|Macro avg    |    0.8574| 0.8515| 0.8542|   33461|\n|Weighted avg |    0.8627| 0.8635| 0.8629|   33461|\n\n\n:::\n:::\n\n\nPerformance is stronger for class $1$, consistent with its higher prevalence. The macro-averaged F$_1$ is slightly lower than the weighted average, reflecting imbalance-driven asymmetry.\n\n\n### Confusion matrix and derived rates\n\n$$\n\\mathbf{C}=\n\\begin{pmatrix}\n10226 & 2544\\\\\n2025 & 18666\n\\end{pmatrix}.\n$$\n\nKey derived quantities (treating class $1$ as positive) are:\n\n$$\n\\begin{align}\n\\text{TPR (recall$_1$)} &= \\frac{18666}{18666+2025}=0.9021, \\\\\n\\text{TNR (specificity)} &= \\frac{10226}{10226+2544}=0.8008, \\\\\n\\text{FPR} &= \\frac{2544}{10226+2544}=0.1992, \\\\\n\\text{FNR} &= \\frac{2025}{18666+2025}=0.0979, \\\\\n\\text{Balanced accuracy} &= \\frac{0.9021+0.8008}{2}=0.8515, \\\\\n\\text{MCC} &= 0.7088.\n\\end{align}\n$$\n\n### Error profile\n\nInspection of the confusion matrix suggests two dominant error types:\n\n- Class-0 $\\rightarrow$ Class-1 errors (2544 cases) likely driven by lexical overlap or stylistic convergence in the two parties’ rhetoric, aggravated by class imbalance.\n\n- Class-1 $\\rightarrow$ Class-0 errors (2025 cases) fewer and may correspond to moderate or cross-partisan language patterns.\n\nGiven that embeddings are learned from scratch, the model relies heavily on dataset-specific word usage rather than external semantic priors, which can increase confusion in stylistically similar texts.\n\n\nThe false-positive rate is about twice the false-negative rate, showing that the classifier is more prone to labeling class-$0$ items as class $1$ than vice versa. In applications where mislabeling class $0$ has higher cost, this skew may require threshold adjustment or reweighting.\n\n### Comparative perspective\n\nA BiLSTM with masked pooling is a strong sequential baseline. However, architectures initialized with pretrained contextual encoders typically outperform purely task-trained embeddings on political text tasks because they embed broader semantic and syntactic regularities. The present performance indicates that the dataset is large enough to train effective representations, but also hints at a ceiling imposed by the lack of pretrained knowledge.\n\n## Limitations and possible extensions\n\n\n### Limitations\n\n\n\n- **No pretrained representations:** learning $E$ from scratch can underutilize semantic information that pretrained models capture.\n- **Fixed truncation length:** texts longer than 128 tokens lose tail content, potentially discarding discriminative cues.\n- **Moderate imbalance:** the decision boundary shifts toward class $1$, as seen in higher FPR.\n- **Pooling simplicity:** mean pooling assumes equal importance of token positions and may dilute localized signals.\n\n\n\n### Extensions\n\nSeveral modifications could plausibly improve accuracy or reduce skew:\n\n- **Class-weighted loss:** set weights $w_0,w_1$ in cross-entropy to penalize minority errors more.\n- **Pretrained encoder swap:** replace the embedding+BiLSTM block with a frozen or fine-tuned transformer encoder while keeping masked pooling and the head.\n- **Attention pooling:** learn token weights $\\alpha_i$ to compute $\\mathbf{z}=\\sum_i \\alpha_i \\mathbf{h}_i$ with masking.\n- **Longer or adaptive $T$:** increase $T$ or bucket by length to preserve more context without undue padding cost.\n- **Calibration and thresholding:** tune the probability cutoff using validation curves to trade off FPR vs.\\ FNR for downstream objectives.\n\n\n## Conclusion\n\nThe implemented pipeline offers a computationally efficient neural baseline for binary party attribution in political texts. By combining subword token IDs with trainable embeddings and a BiLSTM encoder, the system captures sequential patterns and achieves solid test performance (accuracy $0.8635$, MCC $0.7088$). Evaluation reveals a mild bias toward predicting the majority class, visible in an elevated false-positive rate for class $0$. The model’s simplicity, GPU-friendly batching, and stable selection via validation accuracy make it a robust starting point. Future work should focus on incorporating pretrained contextual representations and imbalance-aware objectives to further improve minority-class recall and overall calibration.\n\n\n\n\n## Parsing of the Dataset\n\nThe data is messied as delivered, to ensure to not train onto links or dates the data is cleaned and removed of such things. \n\n\n## Model Description\n\n\nTo classify the posts into the perspective categories a bilstm is used, in the following this model and the assumoptions behind it are described and displayed\n\n\n### Turn Text into sequence of vectors\n\nLet a post be a token sequence\n\n$$\nx = (w_1, w_2, ..., w_T)\n$${#eq-black-scholes}\n\nAfter tokenization and truncation/padding to length $T$ a Vocabulary of size $V$ is build, where eacht token $w_t$ is an index in {$1, ..., V$}.\n\nThe Embeddint layer is represented as:\n\n$$\nE \\in \\mathbb{R}^{V \\times d}\n$$\n\n\nThen the embedded input is\n\n$$\n\\mathbf{x_T} = E[w_t] \\in R^d\n$$\n\nSo the post matrix becomes:\n\n$$\n\\mathbf{X} = \\mathbb{R}^{T \\times d}\n$$\n\n\nThe following conceptual assumptions are made:\n\ntext is lowercase, normalized, no special characters or mentions \nMax length $T$ has been chosen\n\n\n### Core LSTM Model\n\nThis describes the core LSTM model that will be used for the biLSTM\n\nGiven input\n\n$$\n\\mathbf{x} \\in \\mathbb{R}^d\n$$\n\nPrevious hidden state \n$$\nh_{t-1} \\in \\mathbb{R}^h\n$$\n\n\nAnd previous cell:\n\n$$\n\\mathbf{c}_{t-1} \\in \\mathbb{R}^h\n$$\n\nInput and hidden state are concatenated:\n\n$$\n\\mathbb{z_t} = [\\mathbf{x_t}, \\mathbf{h_{t-1}}] \\in \\mathbb{R}^{d+h}\n$$\n\n\n\nFour gates are used (sigmoid/logistic) $\\sigma$ and a candidate cell (tanh) $\\phi$:\n\n$$\n\\begin{aligned}\n\\mathbf{f}_t &= \\sigma(W_f \\mathbf{z}_t + \\mathbf{b}_f) && \\text{(forget gate)} \\\\\n\\mathbf{i}_t &= \\sigma(W_i \\mathbf{z}_t + \\mathbf{b}_i) && \\text{(input gate)} \\\\\n\\mathbf{o}_t &= \\sigma(W_o \\mathbf{z}_t + \\mathbf{b}_o) && \\text{(output gate)} \\\\\n\\tilde{\\mathbf{c}}_t &= \\tanh(W_c \\mathbf{z}_t + \\mathbf{b}_c) && \\text{(candidate)}\n\\end{aligned}\n$$\n\nwhere each is \n$$\nW_{*} \\in \\mathbb{R}^{h \\times x (d+h)}, \\mathbf{b}_{*} \\in \\mathbb{R}^h\n$$\n\n\nImplemented are these cell and hidden state updates:\n\n$$\n\\begin{aligned}\n\\mathbf{c}_t &= \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{c}}_t \\\\\n\\mathbf{h}_t &= \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t)\n\\end{aligned}\n$$\n\n### Bidirectional LSTM\n\nOne LSTM is run forward over the sequence, another one backwards:\n\n$$\n\\vec{\\mathbf{h}}_t,\\ \\overleftarrow{\\mathbf{h}}_t \\in \\mathbb{R}^h\n$$\n\n\nThen concatenated:\n\n$$\n\\mathbf{h}_t^{\\mathrm{bi}} =\n\\begin{bmatrix}\n\\vec{\\mathbf{h}}_t \\\\\n\\overleftarrow{\\mathbf{h}}_t\n\\end{bmatrix}\n\\in \\mathbb{R}^{2h}\n$$\n\nSo the whole sequence of hidden states is:\n\n$$\nH^{\\mathrm{bi}} = (\\mathbf{h}_1^{\\mathrm{bi}}, \\ldots, \\mathbf{h}_T^{\\mathrm{bi}}) \\in \\mathbb{R}^{T \\times 2h}.\n$$\n\n\n### Reduce sequence to a fixed vector\n\n\n#### **Polling** {.unnumbered .unlisted}\n\n$$\n\\mathbf{v} =\n\\left[\n\\max_{t}\\, \\mathbf{h}_t^{\\mathrm{bi}}\n\\ ;\\\n\\frac{1}{T} \\sum_{t=1}^{T} \\mathbf{h}_t^{\\mathrm{bi}}\n\\right]\n\\in \\mathbb{R}^{4h}\n$$\n\n### Classification Header\n\nLinear Layer to scalar \n\n$$\ns = w^\\top v + b\n$$\n\n\n### Loss function\n\n$$\n\\mathcal{L}\n= -\\frac{1}{N}\n\\sum_{n=1}^{N}\n\\left[\ny^{(n)} \\log \\bigl( \\hat{y}^{(n)} \\bigr)\n+\n\\bigl(1 - y^{(n)}\\bigr)\n\\log \\bigl(1 - \\hat{y}^{(n)} \\bigr)\n\\right].\n$$\n\n## Model Build\n\nThis part will cover how the model was build\n\n\n### Embeddings\n\n\n### Naive Bayes\n\nHow the baseline works\n\n\n## Model Evaluation\n\n\nModel evaluation goes here, duh\n\n\n## Section\nThis is a simple placeholder for the manuscript's main document [@knuth84].\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}
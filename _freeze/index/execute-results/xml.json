{
  "hash": "aae3f906131aabbf8d7b3c29a442f320",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: REP or DEM <br> LSTM classification report\nauthors:\n  - name: Paul Elvis Otto\n    affiliation: Duke University / Hertie School\n    roles: writing\nbibliography: references.bib\n---\n\n## Intro\n\nClassifying short social-media texts is a standard NLP task and a useful testbed for comparing lexical and sequence-based models. This project predicts whether a tweet posted by a member of the 112th U.S. Congress was authored by a Democrat or a Republican. The dataset, sourced from Harvard Dataverse, contains each post’s text as well as metadata on the author’s party and chamber (House vs. Senate).\n\nThe main model is a bidirectional LSTM classifier operating over contextual token embeddings. Instead of learning word representations from scratch, the pipeline uses a pretrained MiniLM Transformer as a frozen feature extractor (loaded via the MLX-Embeddings/Hugging Face adaptation). The BiLSTM and linear classification head are trained on top of these fixed embeddings. As a transparent baseline, a Multinomial Naive Bayes model with TF–IDF n-gram features is implemented on the same train/validation/test split, and its performance is compared to the BiLSTM in the empirical assessment section.\n\nTo situate the task, an exploratory data analysis is presented in @sec-eda. The text preprocessing and label normalization used to construct the modeling dataset are described in @sec-data-prep.\n\n## Data Preparation {#sec-data-prep}\n\nTo ensure the data is usable and not influenced by elements such as dates, mentions, or links, each post was cleaned using a set of regular expressions. The cleaning script is available in the model’s repository. The cleaned dataset excludes the following:\n\n* Dates\n* Mentions\n* Hashtags\n* URLs\n\nThe data is read in already cleaned into the model pipeline.\n\n\n\n## EDA {#sec-eda}\n\nThis section provides an initial overview of the dataset by examining its basic structure and several descriptive metrics.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ndf <- read.csv(\"./data/congress_complete_clean.csv\")\nshape <- dim(df)\n```\n:::\n\n\nThe dataset has a dimension of 334606, 5. To begin, we look at how posts are distributed across chambers and parties. The overall distribution is visualized in @fig-post-dist.\n\n\n::: {#cell-fig-post-dist .cell layout-align=\"center\"}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndf_counts <- df %>%\n  count(organ, party)\n\nggplot(df_counts, aes(x = organ, y = n, fill = party)) +\n  geom_col(position = \"dodge\") +\n  labs(x = \"Chamber\", y = \"Number of posts\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![Distribution of posts by chamber and party](index_files/figure-jats/fig-post-dist-1.png){#fig-post-dist fig-align='center'}\n:::\n:::\n\n\nA more detailed breakdown of total posts per chamber and party is provided in the table below:\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(gt)\n\ndf_table <- df %>%\n  count(organ, party) %>%\n  pivot_wider(\n    names_from = party,\n    values_from = n,\n    values_fill = 0\n  )\n\ndf_table %>%\n  gt(rowname_col = \"organ\") %>%\n  tab_header(\n    title = \"Number of Posts by Chamber and Party\"\n  ) %>%\n  cols_label(\n    organ = \"Chamber\"\n  )\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"kojfcocywq\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#kojfcocywq table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#kojfcocywq thead, #kojfcocywq tbody, #kojfcocywq tfoot, #kojfcocywq tr, #kojfcocywq td, #kojfcocywq th {\n  border-style: none;\n}\n\n#kojfcocywq p {\n  margin: 0;\n  padding: 0;\n}\n\n#kojfcocywq .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#kojfcocywq .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#kojfcocywq .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#kojfcocywq .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#kojfcocywq .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#kojfcocywq .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#kojfcocywq .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#kojfcocywq .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#kojfcocywq .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#kojfcocywq .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#kojfcocywq .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#kojfcocywq .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#kojfcocywq .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#kojfcocywq .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#kojfcocywq .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#kojfcocywq .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#kojfcocywq .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#kojfcocywq .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#kojfcocywq .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#kojfcocywq .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#kojfcocywq .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#kojfcocywq .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#kojfcocywq .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#kojfcocywq .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#kojfcocywq .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#kojfcocywq .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#kojfcocywq .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#kojfcocywq .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#kojfcocywq .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#kojfcocywq .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#kojfcocywq .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#kojfcocywq .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#kojfcocywq .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#kojfcocywq .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#kojfcocywq .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#kojfcocywq .gt_left {\n  text-align: left;\n}\n\n#kojfcocywq .gt_center {\n  text-align: center;\n}\n\n#kojfcocywq .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#kojfcocywq .gt_font_normal {\n  font-weight: normal;\n}\n\n#kojfcocywq .gt_font_bold {\n  font-weight: bold;\n}\n\n#kojfcocywq .gt_font_italic {\n  font-style: italic;\n}\n\n#kojfcocywq .gt_super {\n  font-size: 65%;\n}\n\n#kojfcocywq .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#kojfcocywq .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#kojfcocywq .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#kojfcocywq .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#kojfcocywq .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#kojfcocywq .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#kojfcocywq .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#kojfcocywq .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#kojfcocywq div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_heading\">\n      <td colspan=\"3\" class=\"gt_heading gt_title gt_font_normal gt_bottom_border\" style>Number of Posts by Chamber and Party</td>\n    </tr>\n    \n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"a::stub\"></th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"democrat\">democrat</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"republican\">republican</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><th id=\"stub_1_1\" scope=\"row\" class=\"gt_row gt_left gt_stub\">house</th>\n<td headers=\"stub_1_1 democrat\" class=\"gt_row gt_right\">89808</td>\n<td headers=\"stub_1_1 republican\" class=\"gt_row gt_right\">167004</td></tr>\n    <tr><th id=\"stub_1_2\" scope=\"row\" class=\"gt_row gt_left gt_stub\">senate</th>\n<td headers=\"stub_1_2 democrat\" class=\"gt_row gt_right\">37890</td>\n<td headers=\"stub_1_2 republican\" class=\"gt_row gt_right\">39904</td></tr>\n  </tbody>\n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\nNext, the distribution of post lengths is examined. To limit the influence of outliers, the top one percent of longest posts are excluded:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\n\ndf_clean <- df %>%\n  mutate(length = str_length(post)) %>%\n  group_by(party, organ) %>%\n  mutate(cutoff = quantile(length, 0.99, na.rm = TRUE)) %>%\n  ungroup() %>%\n  filter(length <= cutoff)\n\nggplot(df_clean, aes(x = party, y = length, fill = party)) +\n  geom_violin(trim = TRUE) +\n  facet_wrap(~organ) +\n  coord_cartesian(ylim = c(0, 200)) +\n  labs(\n    x = \"Party\",\n    y = \"Post length\",\n    title = \"Distribution of post lengths (top 1% removed)\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-jats/unnamed-chunk-2-1.png){fig-align='center'}\n:::\n:::\n\n\nThe dataset also allows identifying the most active members. The figure below displays the top five posters for each chamber–party combination:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndf_top <- df %>%\n  count(organ, party, member, name = \"posts\") %>%\n  group_by(organ, party) %>%\n  slice_max(order_by = posts, n = 5, with_ties = FALSE) %>%\n  ungroup()\n\nggplot(df_top, aes(x = reorder(member, posts), y = posts, fill = party)) +\n  geom_col() +\n  coord_flip() +\n  facet_grid(organ ~ party, scales = \"free_y\") +\n  labs(\n    x = \"Member\",\n    y = \"Number of posts\",\n    title = \"Top 5 posters per chamber and party\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-jats/unnamed-chunk-3-1.png){fig-align='center'}\n:::\n:::\n\n\nFinally, the most frequent words used by each party are explored. After tokenizing posts, removing stopwords, and excluding a small set of custom stop terms, the top twenty terms per party are identified:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(tidytext)\nlibrary(ggplot2)\nlibrary(stringr)\n\ncustom_stop <- tibble(word = c(\"rt\"))\n\ndf_words <- df %>%\n  mutate(post = str_to_lower(post)) %>%\n  unnest_tokens(word, post) %>%\n  anti_join(stop_words, by = \"word\") %>%\n  anti_join(custom_stop, by = \"word\") %>%\n  filter(word != \"\") %>%\n  count(party, word, sort = TRUE)\n\ntop_n_words <- 20\n\ndf_top <- df_words %>%\n  group_by(party) %>%\n  slice_max(order_by = n, n = top_n_words) %>%\n  ungroup()\n\nggplot(df_top, aes(x = reorder_within(word, n, party), y = n, fill = party)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~party, scales = \"free_y\") +\n  scale_x_reordered() +\n  labs(\n    x = \"Word\",\n    y = \"Frequency\",\n    title = \"Top word frequencies by party\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-jats/unnamed-chunk-4-1.png){fig-align='center'}\n:::\n:::\n\n\n\n## Model specification\n\n### Tokenization and feature extraction\n\nThe input pipeline leverages a pre-trained Transformer model, specifically `all-MiniLM-L6-v2`, to generate contextualized semantic features. Unlike traditional pipelines that feed token IDs directly to the classifier, this architecture utilizes the Transformer as a frozen feature extractor.\n\nRaw text is tokenized and padded to a fixed sequence length $T=128$. Let $\\mathbf{t}$ be the sequence of token identifiers. These are passed through the quantized (4-bit) Transformer model $\\mathcal{F}$ to yield a sequence of dense vectors:\n\n$$\n\\mathbf{X} = \\mathcal{F}(\\mathbf{t}) \\in \\mathbb{R}^{T \\times D},\n$${#eq-transformer-out}\n\nwhere $D$ is the embedding dimension (inferred from the Transformer output, typically 384 for MiniLM). Crucially, a stop-gradient operation is applied to $\\mathbf{X}$:\n\n$$\n\\mathbf{X}_{\\text{fixed}} = \\text{StopGradient}(\\mathbf{X}),\n$${#eq-stop-grad}\n\nensuring that gradients are not backpropagated into the Transformer layers, treating the embeddings as static inputs to the downstream LSTM.\n\n### Bidirectional LSTM encoder\n\nA custom implementation of a Bidirectional LSTM is employed to model the temporal dependencies within the sequence of embeddings $\\mathbf{X}_{\\text{fixed}}$.\n\nThe architecture consists of two independent LSTM layers: a forward pass ($\\overrightarrow{\\text{LSTM}}$) and a backward pass ($\\overleftarrow{\\text{LSTM}}$). For a single direction, the state update at timestep $t$, given input $\\mathbf{x}_t$ and previous hidden state $\\mathbf{h}_{t-1}$, is governed by the standard gate equations:\n\n$$\n\\begin{align}\n\\mathbf{i}_t &= \\sigma(W_i\\mathbf{x}_t + U_i\\mathbf{h}_{t-1} + \\mathbf{b}_i) \\\\\n\\mathbf{f}_t &= \\sigma(W_f\\mathbf{x}_t + U_f\\mathbf{h}_{t-1} + \\mathbf{b}_f) \\\\\n\\mathbf{o}_t &= \\sigma(W_o\\mathbf{x}_t + U_o\\mathbf{h}_{t-1} + \\mathbf{b}_o) \\\\\n\\mathbf{g}_t &= \\tanh(W_g\\mathbf{x}_t + U_g\\mathbf{h}_{t-1} + \\mathbf{b}_g) \\\\\n\\mathbf{c}_t &= \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\mathbf{g}_t \\\\\n\\mathbf{h}_t &= \\mathbf{o}_t \\odot \\tanh(\\mathbf{c}_t)\n\\end{align}\n$${#eq-lstm-gates}\n\nwhere $\\sigma$ is the sigmoid function and $\\odot$ is the Hadamard product. The hidden state dimension is $H=128$. The backward LSTM processes the sequence in reverse order. The final representation at each timestep is the concatenation of both directional states:\n\n$$\n\\mathbf{h}_t = [\\overrightarrow{\\mathbf{h}}_t ; \\overleftarrow{\\mathbf{h}}_t] \\in \\mathbb{R}^{2H}.\n$${#eq-bilstm-out}\n\n### Masked mean pooling\n\nTo handle the padding artifacts present in the fixed-length sequences, a masked mean pooling operation is applied. Let $\\mathbf{m} \\in \\{0, 1\\}^T$ be the attention mask where $1$ denotes a valid token. The fixed-size sentence representation $\\mathbf{z}$ is computed as:\n\n$$\n\\mathbf{z} = \\frac{\\sum_{t=1}^{T} m_t \\mathbf{h}_t}{\\sum_{t=1}^{T} m_t + \\epsilon} \\in \\mathbb{R}^{2H},\n$${#eq-masked-pool}\n\nwhere $\\epsilon = 10^{-6}$ ensures numerical stability. This collapses the temporal dimension, resulting in a single vector capturing the global context of the post.\n\n### Classification head\n\nThe pooled vector $\\mathbf{z}$ serves as the input to the classification head. Regularization is applied via Dropout with probability $p=0.1$:\n\n$$\n\\tilde{\\mathbf{z}} = \\text{Dropout}(\\mathbf{z}).\n$${#eq-dropout}\n\nA linear projection layer maps the features to the unnormalized logits for the two classes:\n\n$$\n\\mathbf{o} = W_{out}\\tilde{\\mathbf{z}} + \\mathbf{b}_{out} \\in \\mathbb{R}^2.\n$${#eq-linear-head}\n\nThe predicted class $\\hat{y}$ is derived via the argmax of the logits.\n\n### Training objective\n\nThe model is trained using the Cross-Entropy loss function. For a batch of $N$ samples, the objective is to minimize:\n\n$$\n\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{CrossEntropy}(\\mathbf{o}^{(i)}, y^{(i)}).\n$${#eq-jross-entropy}\n\nOptimization is performed using the **Adam** optimizer with a learning rate $\\eta = 10^{-3}$. The training step, including the loss calculation and gradient update, is Just-In-Time (JIT) compiled into a comprehensive computation graph using `mx.compile` to maximize execution speed.\n\n### Computational considerations\n\nThe implementation is optimized for Apple Silicon using the MLX framework. Several specific strategies are employed for efficiency:\n\n1.  **Lazy Evaluation & Materialization:** MLX uses lazy evaluation. To prevent the computation graph from growing indefinitely during the iterative data loading process, `mx.eval` is explicitly called on batch inputs and loss values to force materialization.\n2.  **Quantized Feature Extraction:** The embedding model (`all-MiniLM-L6-v2`) utilizes 4-bit quantization, significantly reducing memory bandwidth requirements during the feature extraction phase.\n3.  **Precision:** The embeddings are cast to `float16` (`mx.float16`) before entering the LSTM, halving the memory footprint of the batch tensor compared to `float32` and leveraging the hardware's native half-precision performance.\n\n## Data Pipeline\n\nThe pipeline proceeds as:\n\n* Read the cleaned CSV (`congress_complete.csv`) and keep the relevant columns (`post` and `party`).\n* Drop rows with missing text or party labels.\n* Normalize party strings and map them to binary labels (Democrat = 0, Republican = 1), discarding any other labels.\n* Perform a stratified train/validation/test split with proportions 80/10/10 using a fixed random seed.\n* During training and evaluation, tokenize **on the fly** per batch and pad/truncate to a fixed length (T=128).\n* For each minibatch, pass token IDs and attention masks through a **frozen, quantized MiniLM encoder** to obtain contextual token embeddings. Gradients are stopped at this stage, so only the downstream BiLSTM is trained.\n* Cast embeddings to `float16` to reduce memory use; keep attention masks to zero out padding tokens before recurrence and for masked mean pooling.\n\nThis design avoids storing large precomputed embedding tensors in RAM. Instead, embeddings are computed batch-wise and materialized with `mx.eval` so MLX graphs do not accumulate across iterations.\n\n### Training Loop\n\nThe training routine runs for 10 epochs with a fixed batch size of 64. In each epoch:\n\n* Set the model to training mode.\n* Iterate over batches produced by `batch_iterate_text(..., drop_last=True)` so every training step has identical shape (required for `mx.compile`).\n* For each batch:\n\n  * Compute logits with the BiLSTM classifier.\n  * Compute mean cross-entropy loss.\n  * Backpropagate to obtain gradients for the BiLSTM parameters.\n  * Update parameters using the **Adam** optimizer with learning rate (10^{-3}).\n* After each epoch, switch to evaluation mode and compute validation loss and accuracy over the full validation split (`drop_last=False`).\n\nThe training step is JIT-compiled via `mx.compile` for speed. There is no checkpointing or best-model reload in the current code; the reported test results come from the final epoch’s parameters.\n\n\n\n### Implementation in Code\n\nThe biLStM model has been in apples mlx framework to enable a bare metal run of the model on apple silicon hardware. The implementation has ben performd on top of the mlx framework provided `nn.Module` function.\n\n\nfor that a LSTM cell has been set up as follows\n\n```{{python}}\nclass LSTMCell(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.xh_to_gates = nn.Linear(input_size + hidden_size, 4 * hidden_size)\n\n    def __call__(self, x_t, state):\n        h_prev, c_prev = state\n        xh = mx.concatenate([x_t, h_prev], axis=-1)\n        gates = self.xh_to_gates(xh)\n        i, f, o, g = mx.split(gates, 4, axis=-1)\n\n        i = mx.sigmoid(i)\n        f = mx.sigmoid(f)\n        o = mx.sigmoid(o)\n        g = mx.tanh(g)\n\n        c_t = f * c_prev + i * g\n        h_t = o * mx.tanh(c_t)\n        return h_t, c_t\n```\n\nUsed in a single lstm model as follows:\n\n```{{python}}\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.cell = LSTMCell(input_size, hidden_size)\n\n    def __call__(self, x):\n        B, T, _ = x.shape\n        h = mx.zeros((B, self.hidden_size), dtype=mx.float32)\n        c = mx.zeros((B, self.hidden_size), dtype=mx.float32)\n\n        outputs = []\n        for t in range(T):\n            h, c = self.cell(x[:, t, :], (h, c))\n            outputs.append(h)\n\n        return mx.stack(outputs, axis=1), (h, c)\n\n```\n\nAnd finally the bidirectional lstm is set up along the text classification:\n\n```{{python}}\n\nclass BiLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.fwd = LSTM(input_size, hidden_size)\n        self.bwd = LSTM(input_size, hidden_size)\n\n    def __call__(self, x):\n        B, T, _ = x.shape\n\n        fwd_out, _ = self.fwd(x)\n\n        rev_idx = mx.arange(T - 1, -1, -1, dtype=mx.int32)\n        x_rev = mx.take(x, rev_idx, axis=1)\n\n        bwd_out_rev, _ = self.bwd(x_rev)\n        bwd_out = mx.take(bwd_out_rev, rev_idx, axis=1)\n\n        return mx.concatenate([fwd_out, bwd_out], axis=-1)  # (B, T, 2H)\n\n\nclass BiLSTMTextClassifier(nn.Module):\n    def __init__(self, embedding_dim, hidden_size, num_classes, dropout=0.1):\n        super().__init__()\n        self.bilstm = BiLSTM(embedding_dim, hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.head = nn.Linear(2 * hidden_size, num_classes)\n\n    def pool(self, seq_out, mask):\n        mask_f = mask.astype(mx.float32)[..., None]\n        summed = mx.sum(seq_out * mask_f, axis=1)\n        denom = mx.maximum(mx.sum(mask_f, axis=1), 1e-6)\n        return summed / denom\n\n    def __call__(self, x, mask):\n        # zero out padding before recurrence\n        x = x * mask.astype(mx.float32)[..., None]\n        seq_out = self.bilstm(x)\n        pooled = self.pool(seq_out, mask)\n        pooled = self.dropout(pooled)\n        return self.head(pooled)\n\n\n```\n\nTo use the framework to it's full poterntial the model its is compiled\n\n```{{python}}\n@partial(mx.compile, inputs=state, outputs=state)\ndef step(Xb, yb, Mb):\n    loss, grads = loss_and_grad(model, Xb, yb, Mb)\n    optimizer.update(model, grads)\n    return loss\n\n```\n\nThe description of the loss function is omitted but can be found in the complete code in the repository\n\n\n## Naive Bayes Baseline {#sec-nbb}\n\nAs a classical probabilistic benchmark, a Naive Bayes classifier is employed alongside the neural sequence model. To ensure comparability, all preprocessing steps prior to feature extraction—data cleaning, label normalization, and the exact train/validation/test split—are identical to those used for the BiLSTM classifier.\n\n### Model description\n\nIn contrast to the sequence-based neural architecture, the Naive Bayes model operates on a sparse bag-of-ngrams representation of the text. Each document $x$ is mapped to a TF–IDF feature vector\n$$\n\\mathbf{f}(x) = (f_1, f_2, \\ldots, f_d),\n$${#eq-eq-tfidf}\n\nwhere each $f_j$ reflects the TF–IDF weight of term $j$ after vocabulary truncation and filtering. The vectorizer uses standard preprocessing settings for text classification:\n\n* lowercasing\n* English stop-word removal\n* unigram and bigram features $(1,2)$\n* a maximum feature cap of $d = 50{,}000$\n* a minimum document frequency of $2$\n\nThe classifier itself is a Multinomial Naive Bayes model, which assumes conditional independence of features given a class label $y\\in{0,1}$. Under this assumption, the likelihood of observing the feature vector $\\mathbf{f}(x)$ given class $k$ factorizes as\n\n$$\np(\\mathbf{f}(x)\\mid y=k)\n;\\propto;\n\\prod_{j=1}^{d}\n\\theta_{kj}^{, f_j(x)},\n$${#eq-nb-likelihood}\n\nwhere $\\theta_{kj}$ are class-conditional feature probabilities estimated from the training set. Bayes’ rule then yields the posterior\n\n$$\np(y=k \\mid x)\n;\\propto;\np(y=k)\\prod_{j=1}^{d}\\theta_{kj}^{, f_j(x)},\n$${#eq-nb-posterior}\n\nand prediction is made via\n\n$$\n\\hat{y} = \\arg\\max_{k\\in{0,1}} p(y=k\\mid x).\n$${#eq-nb-prediction}\n\nAlthough the multinomial model is derived for count data, it is widely used with TF–IDF features and remains a strong linear baseline in high-dimensional text classification.\n\n\n## Empirical Assessments\n\nThis section reports the empirical performance of the two main classifiers: the BiLSTM sequence model and the Naive Bayes baseline.\n\n### BiLSTM\n\n#### Headline results\n\nOn the held-out test partition, the BiLSTM achieves:\n\n$$\n\\text{Test loss} = 0.5229,\\qquad\n\\text{Test accuracy} = 0.7952.\n$${#ewq-bilstm-acc}\n\nValidation- and test-set performance are closely aligned, suggesting stable generalization without pronounced overfitting.\n\n#### Class-wise metrics\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(tibble)\n\nclsrep <- tribble(\n  ~Class,\n  ~Precision,\n  ~Recall,\n  ~F1,\n  ~Support,\n  \"0\",\n  0.7621,\n  0.6748,\n  0.7158,\n  12729,\n  \"1\",\n  0.8121,\n  0.8697,\n  0.8399,\n  20573,\n  \"Accuracy\",\n  NA,\n  NA,\n  0.7952,\n  33302,\n  \"Macro avg\",\n  0.7871,\n  0.7723,\n  0.7779,\n  33302,\n  \"Weighted avg\",\n  0.7930,\n  0.7952,\n  0.7925,\n  33302\n)\n\nlibrary(knitr)\nkable(clsrep, caption = \"BiLSTM: test-set classification report.\")\n```\n\n::: {.cell-output-display}\n\n\nTable: BiLSTM: test-set classification report.\n\n|Class        | Precision| Recall|     F1| Support|\n|:------------|---------:|------:|------:|-------:|\n|0            |    0.7621| 0.6748| 0.7158|   12729|\n|1            |    0.8121| 0.8697| 0.8399|   20573|\n|Accuracy     |        NA|     NA| 0.7952|   33302|\n|Macro avg    |    0.7871| 0.7723| 0.7779|   33302|\n|Weighted avg |    0.7930| 0.7952| 0.7925|   33302|\n\n\n:::\n\n```{.r .cell-code .hidden}\ntest_acc_lstm <- 0.7952074950453426\ntest_loss_lstm <- 0.5228507898691046\n```\n:::\n\n\nPerformance is systematically stronger for class 1, which is also the majority class in the data and therefore this behaviour is somewhat expected. The macro-averaged $F_1$ is slightly lower than the weighted average, reflecting the impact of class imbalance on aggregate metrics.\n\n#### Confusion matrix and derived rates\n\nThe test-set confusion matrix for the BiLSTM is:\n\n$$\n\\mathbf{C} =\n\\begin{pmatrix}\n8590 & 4139 \\\\\n2681 & 17892\n\\end{pmatrix}.\n$${#eq-bilstm-conf-mat}\n\nTreating class 1 as the positive class, the following quantities are obtained:\n\n$$\n\\begin{aligned}\n\\text{TPR (recall$_1$)} &= \\frac{17892}{17892+2681}=0.8697, \\\\\n\\text{TNR (specificity)} &= \\frac{8590}{8590+4139}=0.6748, \\\\\n\\text{FPR} &= \\frac{4139}{8590+4139}=0.3252, \\\\\n\\text{FNR} &= \\frac{2681}{17892+2681}=0.1303, \\\\\n\\text{Balanced accuracy} &= \\frac{0.8697+0.6748}{2}=0.7723, \\\\\n\\text{MCC} &= 0.5592.\n\\end{aligned}\n$${#eq-bilstm-derived-metrics}\n\n#### Error profile\n\nThe confusion matrix indicates two main error patterns:\n\n- **Class 0 → Class 1**: A comparatively large number of false positives (items from class 0 predicted as class 1) likely arises from shared lexical or stylistic features between the two parties, amplified by class imbalance which then leads to a bigger set of possible patterns are beeing labeled as class 1.\n\n- **Class 1 → Class 0**: Fewer false negatives for class 1, which may correspond to more moderate or cross-partisan language that is less prototypical of the majority class.\n\nThe false-positive rate is roughly twice the false-negative rate, implying that the classifier tends to err toward predicting class 1. In applications with asymmetric error costs, this tendency could be counteracted by threshold adjustment, class-weighted loss functions, or post-hoc calibration.\n\nThe distribution of posts lengths does not have an impact on the error profile as throughout the dataset the post length distribution is similar across classes as shown in @sec-eda .\n\n### Qualitative Analysis\n\nTo show the qualitative implications of the model a sample of posts is classified here.\n\n\n#### Single words\n\nThe model is given a list of the most comon english words and their predicted probabilities for each class. The results are visualized in @fig-word-bias, which plots the margin (p_dem − p_repub) for each word against its rank in the sorted list. Points are colored by margin and sized by confidence (maximum predicted probability). The twenty most extreme words on either end of the spectrum are labeled.\n\n\n::: {#cell-fig-word-bias .cell layout-align=\"center\"}\n\n```{.r .cell-code .hidden}\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggrepel)\n\n# read predictions\ndf <- read_csv(\"eval/words.csv\", show_col_types = FALSE) %>%\n  mutate(\n    p_dem = as.numeric(p_dem),\n    p_repup = as.numeric(p_repub),\n    margin = p_dem - p_repub,\n    confidence = pmax(p_dem, p_repub)\n  ) %>%\n  arrange(margin) %>%\n  mutate(rank = row_number())\n\n# label the extreme words\nk <- 20\nto_label <- bind_rows(\n  df %>% slice_head(n = k),\n  df %>% slice_tail(n = k)\n)\n\nggplot(df, aes(x = rank, y = margin)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", linewidth = 0.4) +\n  geom_point(aes(size = confidence, color = margin), alpha = 0.8) +\n  scale_color_gradient2(\n    low = \"#b2182b\",\n    mid = \"grey70\",\n    high = \"#2166ac\",\n    midpoint = 0,\n    name = \"p_dem - p_repub\"\n  ) +\n  scale_size_continuous(range = c(1.2, 4), name = \"confidence\") +\n  geom_text_repel(\n    data = to_label,\n    aes(label = word),\n    size = 3,\n    max.overlaps = Inf,\n    box.padding = 0.25,\n    point.padding = 0.2\n  ) +\n  labs(\n    title = \"Model bias on common words\",\n    subtitle = \"Each point is a word; margin = p_dem − p_repub. Extremes labeled.\",\n    x = \"Words sorted by margin (Rep-leaning → Dem-leaning)\",\n    y = \"Margin (p_dem − p_repub)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\")\n  )\n```\n\n::: {.cell-output-display}\n![Model bias on common words](index_files/figure-jats/fig-word-bias-1.png){#fig-word-bias fig-align='center'}\n:::\n:::\n\n\n\nAs Figure @fig-word-bias indicates, the classifier shows pronounced partisan leanings even when evaluated on isolated tokens.\n\nA natural next step is to test whether this behavior is stable over time. In particular, one could examine whether applying the model to contemporary Republican and Democratic posts changes its performance, given well-documented shifts in political communication and issue framing over recent years.\n\n\n### Naive Bayes Baseline\n\n#### Headline results\n\nAs a bag-of-words baseline, the Naive Bayes classifier attains the following accuracy on the test set:\n\n$$\n\\text{Test accuracy} = 0.7843\n$${#eq-nb-acc}\n\nGiven its simplicity and lack of contextual or sequential modeling, this level of performance is comparatively strong, but still below that of the BiLSTM.\n\n#### Class-wise metrics\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(tibble)\n\nnb <- tribble(\n  ~Class,\n  ~Precision,\n  ~Recall,\n  ~F1,\n  ~Support,\n  \"0\",\n  0.7698,\n  0.6214,\n  0.6877,\n  12729,\n  \"1\",\n  0.7907,\n  0.8850,\n  0.8352,\n  20573,\n  \"Accuracy\",\n  NA,\n  NA,\n  0.7843,\n  33302,\n  \"Macro avg\",\n  0.7803,\n  0.7532,\n  0.7615,\n  33302,\n  \"Weighted avg\",\n  0.7827,\n  0.7843,\n  0.7788,\n  33302\n)\n\nlibrary(knitr)\nkable(nb, caption = \"Naive Bayes: test-set classification report.\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Naive Bayes: test-set classification report.\n\n|Class        | Precision| Recall|     F1| Support|\n|:------------|---------:|------:|------:|-------:|\n|0            |    0.7698| 0.6214| 0.6877|   12729|\n|1            |    0.7907| 0.8850| 0.8352|   20573|\n|Accuracy     |        NA|     NA| 0.7843|   33302|\n|Macro avg    |    0.7803| 0.7532| 0.7615|   33302|\n|Weighted avg |    0.7827| 0.7843| 0.7788|   33302|\n\n\n:::\n:::\n\n\nAs with the BiLSTM, performance is higher for class 1 than for class 0. Precision and recall for class 1 are clearly stronger, indicating a tendency to assign documents to the majority class and to capture its lexical cues more reliably.\n\n#### Confusion matrix and error profile\n\nThe corresponding test-set confusion matrix for the Naive Bayes classifier is\n\n$$\n\\begin{pmatrix}\n9599 & 3171 \\\\\n2025 & 18666\n\\end{pmatrix}.\n$${#eq-nb-conf-mat}\n\nFalse positives for class 1 (3171 cases) are moderately more frequent than in the BiLSTM. Overall, the baseline captures strong lexical signals at low computational cost but offers less balanced performance across classes than the sequence model.\n\n\n### Comparative perspective\n\nTo assess whether the BiLSTM improves upon the Naive Bayes baseline in a systematic way, a side-by-side comparison of key metrics is provided in @tbl-comp. The BiLSTM offers modest gains in overall accuracy and more favorable error balance across classes, while the Naive Bayes classifier remains a competitive and interpretable lexical baseline.\n\n\n\n::: {#tbl-comp .cell tbl-cap='Model Comparison'}\n\n```{.r .cell-code .hidden}\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(gt)\n\n# 1. Setup Data -----------------------------------------------------------\n# (Same data setup as before)\n\nnb %>%\n  mutate(Class = clsrep$Class) %>%\n  invisible()\n\n# Merge data wide\ndf_wide <- clsrep %>%\n  rename_with(~ paste0(., \"_BiLSTM\"), -Class) %>%\n  left_join(nb %>% rename_with(~ paste0(., \"_NB\"), -Class), by = \"Class\")\n\n# 2. The GT Table ---------------------------------------------------------\n\ndf_wide %>%\n  gt() %>%\n  # --- 1. Structure & Labels ---\n  tab_spanner(label = \"BiLSTM\", columns = ends_with(\"_BiLSTM\")) %>%\n  tab_spanner(label = \"Naive Bayes\", columns = ends_with(\"_NB\")) %>%\n  cols_label(\n    Precision_BiLSTM = \"Precision\",\n    Recall_BiLSTM = \"Recall\",\n    F1_BiLSTM = \"F1\",\n    Support_BiLSTM = \"Support\",\n    Precision_NB = \"Precision\",\n    Recall_NB = \"Recall\",\n    F1_NB = \"F1\",\n    Support_NB = \"Support\"\n  ) %>%\n  sub_missing(missing_text = \"-\") %>%\n\n  # --- 2. Visual Separation ---\n  # Add a thick border to the right of the BiLSTM section to separate the models\n  tab_style(\n    style = cell_borders(sides = \"right\", color = \"#d3d3d3\", weight = px(2)),\n    locations = cells_body(columns = \"Support_BiLSTM\")\n  ) %>%\n\n  # --- 3. Highlight Logic (The Advantage Check) ---\n\n  # >>> PRECISION COMPARISON <<<\n  # Highlight BiLSTM if it is higher\n  tab_style(\n    style = cell_fill(color = \"#d4f0d4\"),\n    locations = cells_body(\n      columns = \"Precision_BiLSTM\",\n      rows = Precision_BiLSTM > Precision_NB\n    )\n  ) %>%\n  # Highlight NB if it is higher\n  tab_style(\n    style = cell_fill(color = \"#d4f0d4\"),\n    locations = cells_body(\n      columns = \"Precision_NB\",\n      rows = Precision_NB > Precision_BiLSTM\n    )\n  ) %>%\n\n  # >>> RECALL COMPARISON <<<\n  tab_style(\n    style = cell_fill(color = \"#d4f0d4\"),\n    locations = cells_body(\n      columns = \"Recall_BiLSTM\",\n      rows = Recall_BiLSTM > Recall_NB\n    )\n  ) %>%\n  tab_style(\n    style = cell_fill(color = \"#d4f0d4\"),\n    locations = cells_body(\n      columns = \"Recall_NB\",\n      rows = Recall_NB > Recall_BiLSTM\n    )\n  ) %>%\n\n  # >>> F1 COMPARISON <<<\n  tab_style(\n    style = cell_fill(color = \"#d4f0d4\"),\n    locations = cells_body(\n      columns = \"F1_BiLSTM\",\n      rows = F1_BiLSTM > F1_NB\n    )\n  ) %>%\n  tab_style(\n    style = cell_fill(color = \"#d4f0d4\"),\n    locations = cells_body(\n      columns = \"F1_NB\",\n      rows = F1_NB > F1_BiLSTM\n    )\n  ) %>%\n\n  # Add Headers\n  tab_header(\n    title = \"Model Comparison\",\n    subtitle = \"Cells highlighted indicate the winning model for that specific metric\"\n  )\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"rstrirtqgl\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#rstrirtqgl table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#rstrirtqgl thead, #rstrirtqgl tbody, #rstrirtqgl tfoot, #rstrirtqgl tr, #rstrirtqgl td, #rstrirtqgl th {\n  border-style: none;\n}\n\n#rstrirtqgl p {\n  margin: 0;\n  padding: 0;\n}\n\n#rstrirtqgl .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#rstrirtqgl .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#rstrirtqgl .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#rstrirtqgl .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#rstrirtqgl .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#rstrirtqgl .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#rstrirtqgl .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#rstrirtqgl .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#rstrirtqgl .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#rstrirtqgl .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#rstrirtqgl .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#rstrirtqgl .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#rstrirtqgl .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#rstrirtqgl .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#rstrirtqgl .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#rstrirtqgl .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#rstrirtqgl .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#rstrirtqgl .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#rstrirtqgl .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#rstrirtqgl .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#rstrirtqgl .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#rstrirtqgl .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#rstrirtqgl .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#rstrirtqgl .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#rstrirtqgl .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#rstrirtqgl .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#rstrirtqgl .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#rstrirtqgl .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#rstrirtqgl .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#rstrirtqgl .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#rstrirtqgl .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#rstrirtqgl .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#rstrirtqgl .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#rstrirtqgl .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#rstrirtqgl .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#rstrirtqgl .gt_left {\n  text-align: left;\n}\n\n#rstrirtqgl .gt_center {\n  text-align: center;\n}\n\n#rstrirtqgl .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#rstrirtqgl .gt_font_normal {\n  font-weight: normal;\n}\n\n#rstrirtqgl .gt_font_bold {\n  font-weight: bold;\n}\n\n#rstrirtqgl .gt_font_italic {\n  font-style: italic;\n}\n\n#rstrirtqgl .gt_super {\n  font-size: 65%;\n}\n\n#rstrirtqgl .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#rstrirtqgl .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#rstrirtqgl .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#rstrirtqgl .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#rstrirtqgl .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#rstrirtqgl .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#rstrirtqgl .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#rstrirtqgl .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#rstrirtqgl div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_heading\">\n      <td colspan=\"9\" class=\"gt_heading gt_title gt_font_normal\" style>Model Comparison</td>\n    </tr>\n    <tr class=\"gt_heading\">\n      <td colspan=\"9\" class=\"gt_heading gt_subtitle gt_font_normal gt_bottom_border\" style>Cells highlighted indicate the winning model for that specific metric</td>\n    </tr>\n    <tr class=\"gt_col_headings gt_spanner_row\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"2\" colspan=\"1\" scope=\"col\" id=\"Class\">Class</th>\n      <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"4\" scope=\"colgroup\" id=\"BiLSTM\">\n        <div class=\"gt_column_spanner\">BiLSTM</div>\n      </th>\n      <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"4\" scope=\"colgroup\" id=\"Naive Bayes\">\n        <div class=\"gt_column_spanner\">Naive Bayes</div>\n      </th>\n    </tr>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Precision_BiLSTM\">Precision</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Recall_BiLSTM\">Recall</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"F1_BiLSTM\">F1</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Support_BiLSTM\">Support</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Precision_NB\">Precision</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Recall_NB\">Recall</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"F1_NB\">F1</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Support_NB\">Support</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"Class\" class=\"gt_row gt_left\">0</td>\n<td headers=\"Precision_BiLSTM\" class=\"gt_row gt_right\">0.7621</td>\n<td headers=\"Recall_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.6748</td>\n<td headers=\"F1_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7158</td>\n<td headers=\"Support_BiLSTM\" class=\"gt_row gt_right\" style=\"border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3;\">12729</td>\n<td headers=\"Precision_NB\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7698</td>\n<td headers=\"Recall_NB\" class=\"gt_row gt_right\">0.6214</td>\n<td headers=\"F1_NB\" class=\"gt_row gt_right\">0.6877</td>\n<td headers=\"Support_NB\" class=\"gt_row gt_right\">12729</td></tr>\n    <tr><td headers=\"Class\" class=\"gt_row gt_left\">1</td>\n<td headers=\"Precision_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.8121</td>\n<td headers=\"Recall_BiLSTM\" class=\"gt_row gt_right\">0.8697</td>\n<td headers=\"F1_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.8399</td>\n<td headers=\"Support_BiLSTM\" class=\"gt_row gt_right\" style=\"border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3;\">20573</td>\n<td headers=\"Precision_NB\" class=\"gt_row gt_right\">0.7907</td>\n<td headers=\"Recall_NB\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.8850</td>\n<td headers=\"F1_NB\" class=\"gt_row gt_right\">0.8352</td>\n<td headers=\"Support_NB\" class=\"gt_row gt_right\">20573</td></tr>\n    <tr><td headers=\"Class\" class=\"gt_row gt_left\">Accuracy</td>\n<td headers=\"Precision_BiLSTM\" class=\"gt_row gt_right\">-</td>\n<td headers=\"Recall_BiLSTM\" class=\"gt_row gt_right\">-</td>\n<td headers=\"F1_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7952</td>\n<td headers=\"Support_BiLSTM\" class=\"gt_row gt_right\" style=\"border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3;\">33302</td>\n<td headers=\"Precision_NB\" class=\"gt_row gt_right\">-</td>\n<td headers=\"Recall_NB\" class=\"gt_row gt_right\">-</td>\n<td headers=\"F1_NB\" class=\"gt_row gt_right\">0.7843</td>\n<td headers=\"Support_NB\" class=\"gt_row gt_right\">33302</td></tr>\n    <tr><td headers=\"Class\" class=\"gt_row gt_left\">Macro avg</td>\n<td headers=\"Precision_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7871</td>\n<td headers=\"Recall_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7723</td>\n<td headers=\"F1_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7779</td>\n<td headers=\"Support_BiLSTM\" class=\"gt_row gt_right\" style=\"border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3;\">33302</td>\n<td headers=\"Precision_NB\" class=\"gt_row gt_right\">0.7803</td>\n<td headers=\"Recall_NB\" class=\"gt_row gt_right\">0.7532</td>\n<td headers=\"F1_NB\" class=\"gt_row gt_right\">0.7615</td>\n<td headers=\"Support_NB\" class=\"gt_row gt_right\">33302</td></tr>\n    <tr><td headers=\"Class\" class=\"gt_row gt_left\">Weighted avg</td>\n<td headers=\"Precision_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7930</td>\n<td headers=\"Recall_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7952</td>\n<td headers=\"F1_BiLSTM\" class=\"gt_row gt_right\" style=\"background-color: #D4F0D4;\">0.7925</td>\n<td headers=\"Support_BiLSTM\" class=\"gt_row gt_right\" style=\"border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3;\">33302</td>\n<td headers=\"Precision_NB\" class=\"gt_row gt_right\">0.7827</td>\n<td headers=\"Recall_NB\" class=\"gt_row gt_right\">0.7843</td>\n<td headers=\"F1_NB\" class=\"gt_row gt_right\">0.7788</td>\n<td headers=\"Support_NB\" class=\"gt_row gt_right\">33302</td></tr>\n  </tbody>\n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\n\n## Limitations and possible extensions\n\n### Limitations\n\nThe primary constraint of the current model is its reliance on frozen feature extraction. Because the Transformer $\\mathcal{F}$ functions purely as a static input generator with stop-gradient operations, the embedding space is unable to adapt to the specific political vernacular of the congressional dataset during backpropagation. This lack of domain adaptation is compounded by precision limitations; the utilization of a 4-bit quantized Transformer and the subsequent casting of embeddings to `float16` introduces quantization noise that, while efficient, may discard subtle semantic distinctions found in full-precision representations. Furthermore, the architecture exhibits potential redundancy by stacking a BiLSTM on top of a powerful contextual encoder. Since the MiniLM Transformer already captures long-range dependencies via self-attention, the addition of recurrence introduces a sequential bottleneck without necessarily increasing modeling capacity significantly. Finally, the masked mean pooling strategy treats all valid tokens as equally important, a simplification that risks diluting strong, localized discriminative signals within longer posts.\n\n### Extensions\n\nTo address these limitations, several architectural and training modifications could be implemented. A significant performance boost could be achieved through end-to-end fine-tuning, where the Transformer layers are unfrozen to allow gradients to flow through $\\mathcal{F}$. To manage the memory overhead of this approach on Apple Silicon, techniques such as Low-Rank Adaptation (LoRA) could be employed to fine-tune the encoder efficiently. Alternatively, efficiency could be prioritized by moving to a Transformer-only architecture, removing the BiLSTM layer entirely and projecting the Transformer's `[CLS]` token directly to the classification head; this would simplify the compute graph and eliminate the sequential dependency. Regarding feature aggregation, replacing mean pooling with a learnable attention mechanism would allow the model to weigh informative keywords more heavily than neutral connective text. Finally, while the current model truncates inputs at 128 tokens, implementing a sliding window approach or utilizing the full 512-token capacity of MiniLM would better capture tail-end context in lengthy statements.\n\n## Conclusion\n\nThe implemented pipeline offers a computationally efficient neural baseline for binary party attribution in political texts. By combining subword token IDs with frozen embeddings and a BiLSTM encoder, the system captures sequential patterns and achieves solid test performance. Evaluation reveals a mild bias toward predicting the majority class, visible in an elevated false-positive rate for class $0$. \n\n\n## External sources used\n\n\n- [list of most common english words](https://gist.githubusercontent.com/deekayen/4148741/raw/98d35708fa344717d8eee15d11987de6c8e26d7d/1-1000.txt)\n\n- [Dataset](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2FBOK1CF&utm_source=chatgpt.com)\n\n- [MLX-Embeddings repo](https://github.com/Blaizzy/mlx-embeddings)\n\n- [MLX Repositories](https://github.com/ml-explore/mlx)\n\n- [MLX Implementation of xLSTM](https://github.com/abeleinin/mlx-xLSTM)\n\n- [Pytorch implementaino of LSTM](https://docs.pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n\n\\newpage{}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}
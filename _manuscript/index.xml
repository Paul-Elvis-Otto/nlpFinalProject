<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving
and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
  <front>
    <article-meta>
      <title-group>
        <article-title>REP or DEM LSTM classification report</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Otto</surname>
            <given-names>Paul Elvis</given-names>
          </name>
          <string-name>Paul Elvis Otto</string-name>
          <role vocab="https://credit.niso.org" vocab-term="writing ‚Äì original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">writing</role>
          <xref ref-type="aff" rid="aff-1">a</xref>
        </contrib>
      </contrib-group>
      <aff id="aff-1">
        <institution-wrap>
          <institution>Duke University / Hertie School</institution>
        </institution-wrap>
      </aff>
      <history/>
    </article-meta>
  </front>
  <body>
    <sec id="intro">
      <title>Intro</title>
      <p>Classifying short social-media texts is a standard NLP task and a
  useful testbed for comparing lexical and sequence-based models. This
  project predicts whether a tweet posted by a member of the 112th U.S.
  Congress was authored by a Democrat or a Republican. The dataset,
  sourced from Harvard Dataverse, contains each post‚Äôs text as well as
  metadata on the author‚Äôs party and chamber (House vs.¬†Senate).</p>
      <p>The main model is a bidirectional LSTM classifier operating over
  contextual token embeddings. Instead of learning word representations
  from scratch, the pipeline uses a pretrained MiniLM Transformer as a
  frozen feature extractor (loaded via the MLX-Embeddings/Hugging Face
  adaptation). The BiLSTM and linear classification head are trained on
  top of these fixed embeddings. As a transparent baseline, a
  Multinomial Naive Bayes model with TF‚ÄìIDF n-gram features is
  implemented on the same train/validation/test split, and its
  performance is compared to the BiLSTM in the empirical assessment
  section.</p>
      <p>To situate the task, an exploratory data analysis is presented in
  <xref alt="Section¬†3" rid="sec-eda">Section¬†3</xref>. The text
  preprocessing and label normalization used to construct the modeling
  dataset are described in
  <xref alt="Section¬†2" rid="sec-data-prep">Section¬†2</xref>.</p>
    </sec>
    <sec id="sec-data-prep">
      <title>Data Preparation</title>
      <p>To ensure the data is usable and not influenced by elements such as
  dates, mentions, or links, each post was cleaned using a set of
  regular expressions. The cleaning script is available in the model‚Äôs
  repository. The cleaned dataset excludes the following:</p>
      <list list-type="bullet">
        <list-item>
          <p>Dates</p>
        </list-item>
        <list-item>
          <p>Mentions</p>
        </list-item>
        <list-item>
          <p>Hashtags</p>
        </list-item>
        <list-item>
          <p>URLs</p>
        </list-item>
      </list>
      <p>The data is read in already cleaned into the model pipeline.</p>
    </sec>
    <sec id="sec-eda">
      <title>EDA</title>
      <p>This section provides an initial overview of the dataset by
  examining its basic structure and several descriptive metrics.</p>
      <p>The dataset has a dimension of 334606, 5. To begin, we look at how
  posts are distributed across chambers and parties. The overall
  distribution is visualized in
  <xref alt="Figure¬†1" rid="fig-post-dist">Figure¬†1</xref>.</p>
      <fig id="fig-post-dist">
        <caption>
          <p>Figure¬†1: Distribution of posts by chamber and
    party</p>
        </caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-post-dist-1.png"/>
      </fig>
      <p>A more detailed breakdown of total posts per chamber and party is
  provided in the table below:</p>
      <table-wrap>
        <table>
          <thead>
            <tr>
              <th colspan="3">Number of Posts by Chamber and Party</th>
            </tr>
            <tr>
              <th id="aU003AU003Astub" scope="col"/>
              <th id="democrat" scope="col">democrat</th>
              <th id="republican" scope="col">republican</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td id="stub_1_1" scope="row">house</td>
              <td headers="stub_1_1 democrat">89808</td>
              <td headers="stub_1_1 republican">167004</td>
            </tr>
            <tr>
              <td id="stub_1_2" scope="row">senate</td>
              <td headers="stub_1_2 democrat">37890</td>
              <td headers="stub_1_2 republican">39904</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Next, the distribution of post lengths is examined. To limit the
  influence of outliers, the top one percent of longest posts are
  excluded:</p>
      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-2-1.png"/>
      <p>The dataset also allows identifying the most active members. The
  figure below displays the top five posters for each chamber‚Äìparty
  combination:</p>
      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-3-1.png"/>
      <p>Finally, the most frequent words used by each party are explored.
  After tokenizing posts, removing stopwords, and excluding a small set
  of custom stop terms, the top twenty terms per party are
  identified:</p>
      <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-4-1.png"/>
    </sec>
    <sec id="model-specification">
      <title>Model specification</title>
      <sec id="tokenization-and-feature-extraction">
        <title>Tokenization and feature extraction</title>
        <p>The input pipeline leverages a pre-trained Transformer model,
    specifically <monospace>all-MiniLM-L6-v2</monospace>, to generate
    contextualized semantic features. Unlike traditional pipelines that
    feed token IDs directly to the classifier, this architecture
    utilizes the Transformer as a frozen feature extractor.</p>
        <p>Raw text is tokenized and padded to a fixed sequence length
    <inline-formula><alternatives><tex-math><![CDATA[T=128]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.
    Let <inline-formula><alternatives><tex-math><![CDATA[\mathbf{t}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê≠</mml:mi></mml:math></alternatives></inline-formula>
    be the sequence of token identifiers. These are passed through the
    quantized (4-bit) Transformer model <inline-formula><alternatives><tex-math><![CDATA[\mathcal{F}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>‚Ñ±</mml:mi></mml:math></alternatives></inline-formula>
    to yield a sequence of dense vectors:</p>
        <p>
          <styled-content id="eq-transformer-out">
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[
    \mathbf{X} = \mathcal{F}(\mathbf{t}) \in \mathbb{R}^{T \times D},
     \qquad(1)]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mi>ùêó</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mi>‚Ñ±</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mi>ùê≠</mml:mi>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                    <mml:mo>‚àà</mml:mo>
                    <mml:msup>
                      <mml:mi>‚Ñù</mml:mi>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mo>√ó</mml:mo>
                        <mml:mi>D</mml:mi>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mo>,</mml:mo>
                    <mml:mspace width="2.0em"/>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mn>1</mml:mn>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </styled-content>
        </p>
        <p>where <inline-formula><alternatives><tex-math><![CDATA[D]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>D</mml:mi></mml:math></alternatives></inline-formula>
    is the embedding dimension (inferred from the Transformer output,
    typically 384 for MiniLM). Crucially, a stop-gradient operation is
    applied to <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>:</p>
        <p>
          <styled-content id="eq-stop-grad">
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[
    \mathbf{X}_{\text{fixed}} = \text{StopGradient}(\mathbf{X}),
     \qquad(2)]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>ùêó</mml:mi>
                      <mml:mtext mathvariant="normal">fixed</mml:mtext>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mtext mathvariant="normal">StopGradient</mml:mtext>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mi>ùêó</mml:mi>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                    <mml:mo>,</mml:mo>
                    <mml:mspace width="2.0em"/>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mn>2</mml:mn>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </styled-content>
        </p>
        <p>ensuring that gradients are not backpropagated into the
    Transformer layers, treating the embeddings as static inputs to the
    downstream LSTM.</p>
      </sec>
      <sec id="bidirectional-lstm-encoder">
        <title>Bidirectional LSTM encoder</title>
        <p>A custom implementation of a Bidirectional LSTM is employed to
    model the temporal dependencies within the sequence of embeddings
    <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}_{\text{fixed}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>ùêó</mml:mi><mml:mtext mathvariant="normal">fixed</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>.</p>
        <p>The architecture consists of two independent LSTM layers: a
    forward pass (<inline-formula><alternatives><tex-math><![CDATA[\overrightarrow{\text{LSTM}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mover><mml:mtext mathvariant="normal">LSTM</mml:mtext><mml:mo accent="true">‚Üí</mml:mo></mml:mover></mml:math></alternatives></inline-formula>)
    and a backward pass (<inline-formula><alternatives><tex-math><![CDATA[\overleftarrow{\text{LSTM}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mover><mml:mtext mathvariant="normal">LSTM</mml:mtext><mml:mo accent="true">‚Éñ</mml:mo></mml:mover></mml:math></alternatives></inline-formula>).
    For a single direction, the state update at timestep
    <inline-formula><alternatives><tex-math><![CDATA[t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>,
    given input <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}_t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>ùê±</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
    and previous hidden state <inline-formula><alternatives><tex-math><![CDATA[\mathbf{h}_{t-1}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>ùê°</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>,
    is governed by the standard gate equations:</p>
        <p>
          <styled-content id="eq-lstm-gates">
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[
    \begin{align}
    \mathbf{i}_t &= \sigma(W_i\mathbf{x}_t + U_i\mathbf{h}_{t-1} + \mathbf{b}_i) \\
    \mathbf{f}_t &= \sigma(W_f\mathbf{x}_t + U_f\mathbf{h}_{t-1} + \mathbf{b}_f) \\
    \mathbf{o}_t &= \sigma(W_o\mathbf{x}_t + U_o\mathbf{h}_{t-1} + \mathbf{b}_o) \\
    \mathbf{g}_t &= \tanh(W_g\mathbf{x}_t + U_g\mathbf{h}_{t-1} + \mathbf{b}_g) \\
    \mathbf{c}_t &= \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \mathbf{g}_t \\
    \mathbf{h}_t &= \mathbf{o}_t \odot \tanh(\mathbf{c}_t)
    \end{align}
     \qquad(3)]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mtable>
                      <mml:mtr>
                        <mml:mtd columnalign="right" style="text-align: right">
                          <mml:msub>
                            <mml:mi>ùê¢</mml:mi>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                        </mml:mtd>
                        <mml:mtd columnalign="left" style="text-align: left">
                          <mml:mo>=</mml:mo>
                          <mml:mi>œÉ</mml:mi>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">(</mml:mo>
                            <mml:msub>
                              <mml:mi>W</mml:mi>
                              <mml:mi>i</mml:mi>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>ùê±</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                            <mml:mo>+</mml:mo>
                            <mml:msub>
                              <mml:mi>U</mml:mi>
                              <mml:mi>i</mml:mi>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>ùê°</mml:mi>
                              <mml:mrow>
                                <mml:mi>t</mml:mi>
                                <mml:mo>‚àí</mml:mo>
                                <mml:mn>1</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                            <mml:mo>+</mml:mo>
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mi>i</mml:mi>
                            </mml:msub>
                            <mml:mo stretchy="true" form="postfix">)</mml:mo>
                          </mml:mrow>
                        </mml:mtd>
                      </mml:mtr>
                      <mml:mtr>
                        <mml:mtd columnalign="right" style="text-align: right">
                          <mml:msub>
                            <mml:mi>ùêü</mml:mi>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                        </mml:mtd>
                        <mml:mtd columnalign="left" style="text-align: left">
                          <mml:mo>=</mml:mo>
                          <mml:mi>œÉ</mml:mi>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">(</mml:mo>
                            <mml:msub>
                              <mml:mi>W</mml:mi>
                              <mml:mi>f</mml:mi>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>ùê±</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                            <mml:mo>+</mml:mo>
                            <mml:msub>
                              <mml:mi>U</mml:mi>
                              <mml:mi>f</mml:mi>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>ùê°</mml:mi>
                              <mml:mrow>
                                <mml:mi>t</mml:mi>
                                <mml:mo>‚àí</mml:mo>
                                <mml:mn>1</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                            <mml:mo>+</mml:mo>
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mi>f</mml:mi>
                            </mml:msub>
                            <mml:mo stretchy="true" form="postfix">)</mml:mo>
                          </mml:mrow>
                        </mml:mtd>
                      </mml:mtr>
                      <mml:mtr>
                        <mml:mtd columnalign="right" style="text-align: right">
                          <mml:msub>
                            <mml:mi>ùê®</mml:mi>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                        </mml:mtd>
                        <mml:mtd columnalign="left" style="text-align: left">
                          <mml:mo>=</mml:mo>
                          <mml:mi>œÉ</mml:mi>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">(</mml:mo>
                            <mml:msub>
                              <mml:mi>W</mml:mi>
                              <mml:mi>o</mml:mi>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>ùê±</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                            <mml:mo>+</mml:mo>
                            <mml:msub>
                              <mml:mi>U</mml:mi>
                              <mml:mi>o</mml:mi>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>ùê°</mml:mi>
                              <mml:mrow>
                                <mml:mi>t</mml:mi>
                                <mml:mo>‚àí</mml:mo>
                                <mml:mn>1</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                            <mml:mo>+</mml:mo>
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mi>o</mml:mi>
                            </mml:msub>
                            <mml:mo stretchy="true" form="postfix">)</mml:mo>
                          </mml:mrow>
                        </mml:mtd>
                      </mml:mtr>
                      <mml:mtr>
                        <mml:mtd columnalign="right" style="text-align: right">
                          <mml:msub>
                            <mml:mi>ùê†</mml:mi>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                        </mml:mtd>
                        <mml:mtd columnalign="left" style="text-align: left">
                          <mml:mo>=</mml:mo>
                          <mml:mo>tanh</mml:mo>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">(</mml:mo>
                            <mml:msub>
                              <mml:mi>W</mml:mi>
                              <mml:mi>g</mml:mi>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>ùê±</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                            <mml:mo>+</mml:mo>
                            <mml:msub>
                              <mml:mi>U</mml:mi>
                              <mml:mi>g</mml:mi>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>ùê°</mml:mi>
                              <mml:mrow>
                                <mml:mi>t</mml:mi>
                                <mml:mo>‚àí</mml:mo>
                                <mml:mn>1</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                            <mml:mo>+</mml:mo>
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mi>g</mml:mi>
                            </mml:msub>
                            <mml:mo stretchy="true" form="postfix">)</mml:mo>
                          </mml:mrow>
                        </mml:mtd>
                      </mml:mtr>
                      <mml:mtr>
                        <mml:mtd columnalign="right" style="text-align: right">
                          <mml:msub>
                            <mml:mi>ùêú</mml:mi>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                        </mml:mtd>
                        <mml:mtd columnalign="left" style="text-align: left">
                          <mml:mo>=</mml:mo>
                          <mml:msub>
                            <mml:mi>ùêü</mml:mi>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                          <mml:mo>‚äô</mml:mo>
                          <mml:msub>
                            <mml:mi>ùêú</mml:mi>
                            <mml:mrow>
                              <mml:mi>t</mml:mi>
                              <mml:mo>‚àí</mml:mo>
                              <mml:mn>1</mml:mn>
                            </mml:mrow>
                          </mml:msub>
                          <mml:mo>+</mml:mo>
                          <mml:msub>
                            <mml:mi>ùê¢</mml:mi>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                          <mml:mo>‚äô</mml:mo>
                          <mml:msub>
                            <mml:mi>ùê†</mml:mi>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                        </mml:mtd>
                      </mml:mtr>
                      <mml:mtr>
                        <mml:mtd columnalign="right" style="text-align: right">
                          <mml:msub>
                            <mml:mi>ùê°</mml:mi>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                        </mml:mtd>
                        <mml:mtd columnalign="left" style="text-align: left">
                          <mml:mo>=</mml:mo>
                          <mml:msub>
                            <mml:mi>ùê®</mml:mi>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                          <mml:mo>‚äô</mml:mo>
                          <mml:mo>tanh</mml:mo>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">(</mml:mo>
                            <mml:msub>
                              <mml:mi>ùêú</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                            <mml:mo stretchy="true" form="postfix">)</mml:mo>
                          </mml:mrow>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                    <mml:mspace width="2.0em"/>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mn>3</mml:mn>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </styled-content>
        </p>
        <p>where <inline-formula><alternatives><tex-math><![CDATA[\sigma]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>œÉ</mml:mi></mml:math></alternatives></inline-formula>
    is the sigmoid function and <inline-formula><alternatives><tex-math><![CDATA[\odot]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>‚äô</mml:mi></mml:math></alternatives></inline-formula>
    is the Hadamard product. The hidden state dimension is
    <inline-formula><alternatives><tex-math><![CDATA[H=128]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.
    The backward LSTM processes the sequence in reverse order. The final
    representation at each timestep is the concatenation of both
    directional states:</p>
        <p>
          <styled-content id="eq-bilstm-out">
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[
    \mathbf{h}_t = [\overrightarrow{\mathbf{h}}_t ; \overleftarrow{\mathbf{h}}_t] \in \mathbb{R}^{2H}.
     \qquad(4)]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>ùê°</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">[</mml:mo>
                      <mml:msub>
                        <mml:mover>
                          <mml:mi>ùê°</mml:mi>
                          <mml:mo accent="true">‚Üí</mml:mo>
                        </mml:mover>
                        <mml:mi>t</mml:mi>
                      </mml:msub>
                      <mml:mo>;</mml:mo>
                      <mml:msub>
                        <mml:mover>
                          <mml:mi>ùê°</mml:mi>
                          <mml:mo accent="true">‚Éñ</mml:mo>
                        </mml:mover>
                        <mml:mi>t</mml:mi>
                      </mml:msub>
                      <mml:mo stretchy="true" form="postfix">]</mml:mo>
                    </mml:mrow>
                    <mml:mo>‚àà</mml:mo>
                    <mml:msup>
                      <mml:mi>‚Ñù</mml:mi>
                      <mml:mrow>
                        <mml:mn>2</mml:mn>
                        <mml:mi>H</mml:mi>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mi>.</mml:mi>
                    <mml:mspace width="2.0em"/>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mn>4</mml:mn>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </styled-content>
        </p>
      </sec>
      <sec id="masked-mean-pooling">
        <title>Masked mean pooling</title>
        <p>To handle the padding artifacts present in the fixed-length
    sequences, a masked mean pooling operation is applied. Let
    <inline-formula><alternatives><tex-math><![CDATA[\mathbf{m} \in \{0, 1\}^T]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùê¶</mml:mi><mml:mo>‚àà</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false" form="postfix">}</mml:mo><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
    be the attention mask where <inline-formula><alternatives><tex-math><![CDATA[1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula>
    denotes a valid token. The fixed-size sentence representation
    <inline-formula><alternatives><tex-math><![CDATA[\mathbf{z}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê≥</mml:mi></mml:math></alternatives></inline-formula>
    is computed as:</p>
        <p>
          <styled-content id="eq-masked-pool">
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[
    \mathbf{z} = \frac{\sum_{t=1}^{T} m_t \mathbf{h}_t}{\sum_{t=1}^{T} m_t + \epsilon} \in \mathbb{R}^{2H},
     \qquad(5)]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mi>ùê≥</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:munderover>
                          <mml:mo>‚àë</mml:mo>
                          <mml:mrow>
                            <mml:mi>t</mml:mi>
                            <mml:mo>=</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                          <mml:mi>T</mml:mi>
                        </mml:munderover>
                        <mml:msub>
                          <mml:mi>m</mml:mi>
                          <mml:mi>t</mml:mi>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>ùê°</mml:mi>
                          <mml:mi>t</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:munderover>
                          <mml:mo>‚àë</mml:mo>
                          <mml:mrow>
                            <mml:mi>t</mml:mi>
                            <mml:mo>=</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                          <mml:mi>T</mml:mi>
                        </mml:munderover>
                        <mml:msub>
                          <mml:mi>m</mml:mi>
                          <mml:mi>t</mml:mi>
                        </mml:msub>
                        <mml:mo>+</mml:mo>
                        <mml:mi>œµ</mml:mi>
                      </mml:mrow>
                    </mml:mfrac>
                    <mml:mo>‚àà</mml:mo>
                    <mml:msup>
                      <mml:mi>‚Ñù</mml:mi>
                      <mml:mrow>
                        <mml:mn>2</mml:mn>
                        <mml:mi>H</mml:mi>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mo>,</mml:mo>
                    <mml:mspace width="2.0em"/>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mn>5</mml:mn>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </styled-content>
        </p>
        <p>where <inline-formula><alternatives><tex-math><![CDATA[\epsilon = 10^{-6}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>œµ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
    ensures numerical stability. This collapses the temporal dimension,
    resulting in a single vector capturing the global context of the
    post.</p>
      </sec>
      <sec id="classification-head">
        <title>Classification head</title>
        <p>The pooled vector <inline-formula><alternatives><tex-math><![CDATA[\mathbf{z}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê≥</mml:mi></mml:math></alternatives></inline-formula>
    serves as the input to the classification head. Regularization is
    applied via Dropout with probability <inline-formula><alternatives><tex-math><![CDATA[p=0.1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>:</p>
        <p>
          <styled-content id="eq-dropout">
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[
    \tilde{\mathbf{z}} = \text{Dropout}(\mathbf{z}).
     \qquad(6)]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mover>
                      <mml:mi>ùê≥</mml:mi>
                      <mml:mo accent="true">ÃÉ</mml:mo>
                    </mml:mover>
                    <mml:mo>=</mml:mo>
                    <mml:mtext mathvariant="normal">Dropout</mml:mtext>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mi>ùê≥</mml:mi>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                    <mml:mi>.</mml:mi>
                    <mml:mspace width="2.0em"/>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mn>6</mml:mn>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </styled-content>
        </p>
        <p>A linear projection layer maps the features to the unnormalized
    logits for the two classes:</p>
        <p>
          <styled-content id="eq-linear-head">
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[
    \mathbf{o} = W_{out}\tilde{\mathbf{z}} + \mathbf{b}_{out} \in \mathbb{R}^2.
     \qquad(7)]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mi>ùê®</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:msub>
                      <mml:mi>W</mml:mi>
                      <mml:mrow>
                        <mml:mi>o</mml:mi>
                        <mml:mi>u</mml:mi>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mover>
                      <mml:mi>ùê≥</mml:mi>
                      <mml:mo accent="true">ÃÉ</mml:mo>
                    </mml:mover>
                    <mml:mo>+</mml:mo>
                    <mml:msub>
                      <mml:mi>ùêõ</mml:mi>
                      <mml:mrow>
                        <mml:mi>o</mml:mi>
                        <mml:mi>u</mml:mi>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>‚àà</mml:mo>
                    <mml:msup>
                      <mml:mi>‚Ñù</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msup>
                    <mml:mi>.</mml:mi>
                    <mml:mspace width="2.0em"/>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mn>7</mml:mn>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </styled-content>
        </p>
        <p>The predicted class <inline-formula><alternatives><tex-math><![CDATA[\hat{y}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mover><mml:mi>y</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover></mml:math></alternatives></inline-formula>
    is derived via the argmax of the logits.</p>
      </sec>
      <sec id="training-objective">
        <title>Training objective</title>
        <p>The model is trained using the Cross-Entropy loss function. For a
    batch of <inline-formula><alternatives><tex-math><![CDATA[N]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula>
    samples, the objective is to minimize:</p>
        <p>
          <styled-content id="eq-jross-entropy">
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[
    \mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} \text{CrossEntropy}(\mathbf{o}^{(i)}, y^{(i)}).
     \qquad(8)]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mi>‚Ñí</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:mn>1</mml:mn>
                      <mml:mi>N</mml:mi>
                    </mml:mfrac>
                    <mml:munderover>
                      <mml:mo>‚àë</mml:mo>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>N</mml:mi>
                    </mml:munderover>
                    <mml:mtext mathvariant="normal">CrossEntropy</mml:mtext>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:msup>
                        <mml:mi>ùê®</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mi>i</mml:mi>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mo>,</mml:mo>
                      <mml:msup>
                        <mml:mi>y</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mi>i</mml:mi>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                    <mml:mi>.</mml:mi>
                    <mml:mspace width="2.0em"/>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mn>8</mml:mn>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </styled-content>
        </p>
        <p>Optimization is performed using the <bold>Adam</bold> optimizer
    with a learning rate <inline-formula><alternatives><tex-math><![CDATA[\eta = 10^{-3}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>Œ∑</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.
    The training step, including the loss calculation and gradient
    update, is Just-In-Time (JIT) compiled into a comprehensive
    computation graph using <monospace>mx.compile</monospace> to
    maximize execution speed.</p>
      </sec>
      <sec id="computational-considerations">
        <title>Computational considerations</title>
        <p>The implementation is optimized for Apple Silicon using the MLX
    framework. Several specific strategies are employed for
    efficiency:</p>
        <list list-type="order">
          <list-item>
            <p><bold>Lazy Evaluation &amp; Materialization:</bold> MLX uses
        lazy evaluation. To prevent the computation graph from growing
        indefinitely during the iterative data loading process,
        <monospace>mx.eval</monospace> is explicitly called on batch
        inputs and loss values to force materialization.</p>
          </list-item>
          <list-item>
            <p><bold>Quantized Feature Extraction:</bold> The embedding
        model (<monospace>all-MiniLM-L6-v2</monospace>) utilizes 4-bit
        quantization, significantly reducing memory bandwidth
        requirements during the feature extraction phase.</p>
          </list-item>
          <list-item>
            <p><bold>Precision:</bold> The embeddings are cast to
        <monospace>float16</monospace>
        (<monospace>mx.float16</monospace>) before entering the LSTM,
        halving the memory footprint of the batch tensor compared to
        <monospace>float32</monospace> and leveraging the hardware‚Äôs
        native half-precision performance.</p>
          </list-item>
        </list>
      </sec>
    </sec>
    <sec id="data-pipeline">
      <title>Data Pipeline</title>
      <p>The pipeline proceeds as:</p>
      <list list-type="bullet">
        <list-item>
          <p>Read the cleaned CSV
      (<monospace>congress_complete.csv</monospace>) and keep the
      relevant columns (<monospace>post</monospace> and
      <monospace>party</monospace>).</p>
        </list-item>
        <list-item>
          <p>Drop rows with missing text or party labels.</p>
        </list-item>
        <list-item>
          <p>Normalize party strings and map them to binary labels (Democrat
      = 0, Republican = 1), discarding any other labels.</p>
        </list-item>
        <list-item>
          <p>Perform a stratified train/validation/test split with
      proportions 80/10/10 using a fixed random seed.</p>
        </list-item>
        <list-item>
          <p>During training and evaluation, tokenize <bold>on the
      fly</bold> per batch and pad/truncate to a fixed length
      (T=128).</p>
        </list-item>
        <list-item>
          <p>For each minibatch, pass token IDs and attention masks through
      a <bold>frozen, quantized MiniLM encoder</bold> to obtain
      contextual token embeddings. Gradients are stopped at this stage,
      so only the downstream BiLSTM is trained.</p>
        </list-item>
        <list-item>
          <p>Cast embeddings to <monospace>float16</monospace> to reduce
      memory use; keep attention masks to zero out padding tokens before
      recurrence and for masked mean pooling.</p>
        </list-item>
      </list>
      <p>This design avoids storing large precomputed embedding tensors in
  RAM. Instead, embeddings are computed batch-wise and materialized with
  <monospace>mx.eval</monospace> so MLX graphs do not accumulate across
  iterations.</p>
      <sec id="training-loop">
        <title>Training Loop</title>
        <p>The training routine runs for 10 epochs with a fixed batch size
    of 64. In each epoch:</p>
        <list list-type="bullet">
          <list-item>
            <p>Set the model to training mode.</p>
          </list-item>
          <list-item>
            <p>Iterate over batches produced by
        <monospace>batch_iterate_text(..., drop_last=True)</monospace>
        so every training step has identical shape (required for
        <monospace>mx.compile</monospace>).</p>
          </list-item>
          <list-item>
            <p>For each batch:</p>
            <list list-type="bullet">
              <list-item>
                <p>Compute logits with the BiLSTM classifier.</p>
              </list-item>
              <list-item>
                <p>Compute mean cross-entropy loss.</p>
              </list-item>
              <list-item>
                <p>Backpropagate to obtain gradients for the BiLSTM
            parameters.</p>
              </list-item>
              <list-item>
                <p>Update parameters using the <bold>Adam</bold> optimizer
            with learning rate (10^{-3}).</p>
              </list-item>
            </list>
          </list-item>
          <list-item>
            <p>After each epoch, switch to evaluation mode and compute
        validation loss and accuracy over the full validation split
        (<monospace>drop_last=False</monospace>).</p>
          </list-item>
        </list>
        <p>The training step is JIT-compiled via
    <monospace>mx.compile</monospace> for speed. There is no
    checkpointing or best-model reload in the current code; the reported
    test results come from the final epoch‚Äôs parameters.</p>
      </sec>
      <sec id="implementation-in-code">
        <title>Implementation in Code</title>
        <p>The biLStM model has been in apples mlx framework to enable a
    bare metal run of the model on apple silicon hardware. The
    implementation has ben performd on top of the mlx framework provided
    <monospace>nn.Module</monospace> function.</p>
        <p>for that a LSTM cell has been set up as follows</p>
        <code language="markdown">```{python}
class LSTMCell(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.xh_to_gates = nn.Linear(input_size + hidden_size, 4 * hidden_size)

    def __call__(self, x_t, state):
        h_prev, c_prev = state
        xh = mx.concatenate([x_t, h_prev], axis=-1)
        gates = self.xh_to_gates(xh)
        i, f, o, g = mx.split(gates, 4, axis=-1)

        i = mx.sigmoid(i)
        f = mx.sigmoid(f)
        o = mx.sigmoid(o)
        g = mx.tanh(g)

        c_t = f * c_prev + i * g
        h_t = o * mx.tanh(c_t)
        return h_t, c_t
```</code>
        <p>Used in a single lstm model as follows:</p>
        <code language="markdown">```{python}
class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.cell = LSTMCell(input_size, hidden_size)

    def __call__(self, x):
        B, T, _ = x.shape
        h = mx.zeros((B, self.hidden_size), dtype=mx.float32)
        c = mx.zeros((B, self.hidden_size), dtype=mx.float32)

        outputs = []
        for t in range(T):
            h, c = self.cell(x[:, t, :], (h, c))
            outputs.append(h)

        return mx.stack(outputs, axis=1), (h, c)

```</code>
        <p>And finally the bidirectional lstm is set up along the text
    classification:</p>
        <code language="markdown">```{python}

class BiLSTM(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.fwd = LSTM(input_size, hidden_size)
        self.bwd = LSTM(input_size, hidden_size)

    def __call__(self, x):
        B, T, _ = x.shape

        fwd_out, _ = self.fwd(x)

        rev_idx = mx.arange(T - 1, -1, -1, dtype=mx.int32)
        x_rev = mx.take(x, rev_idx, axis=1)

        bwd_out_rev, _ = self.bwd(x_rev)
        bwd_out = mx.take(bwd_out_rev, rev_idx, axis=1)

        return mx.concatenate([fwd_out, bwd_out], axis=-1)  # (B, T, 2H)


class BiLSTMTextClassifier(nn.Module):
    def __init__(self, embedding_dim, hidden_size, num_classes, dropout=0.1):
        super().__init__()
        self.bilstm = BiLSTM(embedding_dim, hidden_size)
        self.dropout = nn.Dropout(dropout)
        self.head = nn.Linear(2 * hidden_size, num_classes)

    def pool(self, seq_out, mask):
        mask_f = mask.astype(mx.float32)[..., None]
        summed = mx.sum(seq_out * mask_f, axis=1)
        denom = mx.maximum(mx.sum(mask_f, axis=1), 1e-6)
        return summed / denom

    def __call__(self, x, mask):
        # zero out padding before recurrence
        x = x * mask.astype(mx.float32)[..., None]
        seq_out = self.bilstm(x)
        pooled = self.pool(seq_out, mask)
        pooled = self.dropout(pooled)
        return self.head(pooled)


```</code>
        <p>To use the framework to it‚Äôs full poterntial the model its is
    compiled</p>
        <code language="markdown">```{python}
@partial(mx.compile, inputs=state, outputs=state)
def step(Xb, yb, Mb):
    loss, grads = loss_and_grad(model, Xb, yb, Mb)
    optimizer.update(model, grads)
    return loss

```</code>
        <p>The description of the loss function is omitted but can be found
    in the complete code in the repository</p>
      </sec>
    </sec>
    <sec id="sec-nbb">
      <title>Naive Bayes Baseline</title>
      <p>As a classical probabilistic benchmark, a Naive Bayes classifier is
  employed alongside the neural sequence model. To ensure comparability,
  all preprocessing steps prior to feature extraction‚Äîdata cleaning,
  label normalization, and the exact train/validation/test split‚Äîare
  identical to those used for the BiLSTM classifier.</p>
      <sec id="model-description">
        <title>Model description</title>
        <p>In contrast to the sequence-based neural architecture, the Naive
    Bayes model operates on a sparse bag-of-ngrams representation of the
    text. Each document <inline-formula><alternatives><tex-math><![CDATA[x]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
    is mapped to a TF‚ÄìIDF feature vector
    <styled-content id="eq-eq-tfidf"><disp-formula><alternatives><tex-math><![CDATA[
    \mathbf{f}(x) = (f_1, f_2, \ldots, f_d),
     \qquad(9)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mrow><mml:mi>ùêü</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>‚Ä¶</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="2.0em"/><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>9</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></styled-content></p>
        <p>where each <inline-formula><alternatives><tex-math><![CDATA[f_j]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
    reflects the TF‚ÄìIDF weight of term <inline-formula><alternatives><tex-math><![CDATA[j]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>j</mml:mi></mml:math></alternatives></inline-formula>
    after vocabulary truncation and filtering. The vectorizer uses
    standard preprocessing settings for text classification:</p>
        <list list-type="bullet">
          <list-item>
            <p>lowercasing</p>
          </list-item>
          <list-item>
            <p>English stop-word removal</p>
          </list-item>
          <list-item>
            <p>unigram and bigram features <inline-formula><alternatives><tex-math><![CDATA[(1,2)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula></p>
          </list-item>
          <list-item>
            <p>a maximum feature cap of <inline-formula><alternatives><tex-math><![CDATA[d = 50{,}000]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></p>
          </list-item>
          <list-item>
            <p>a minimum document frequency of
        <inline-formula><alternatives><tex-math><![CDATA[2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>2</mml:mn></mml:math></alternatives></inline-formula></p>
          </list-item>
        </list>
        <p>The classifier itself is a Multinomial Naive Bayes model, which
    assumes conditional independence of features given a class label
    <inline-formula><alternatives><tex-math><![CDATA[y\in{0,1}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>y</mml:mi><mml:mo>‚àà</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.
    Under this assumption, the likelihood of observing the feature
    vector <inline-formula><alternatives><tex-math><![CDATA[\mathbf{f}(x)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêü</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
    given class <inline-formula><alternatives><tex-math><![CDATA[k]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>k</mml:mi></mml:math></alternatives></inline-formula>
    factorizes as</p>
        <p>
          <styled-content id="eq-nb-likelihood">
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[
    p(\mathbf{f}(x)\mid y=k)
    ;\propto;
    \prod_{j=1}^{d}
    \theta_{kj}^{, f_j(x)},
     \qquad(10)]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mi>p</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mi>ùêü</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>x</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>‚à£</mml:mo>
                      <mml:mi>y</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mi>k</mml:mi>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                    <mml:mo>;</mml:mo>
                    <mml:mo>‚àù</mml:mo>
                    <mml:mo>;</mml:mo>
                    <mml:munderover>
                      <mml:mo>‚àè</mml:mo>
                      <mml:mrow>
                        <mml:mi>j</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>d</mml:mi>
                    </mml:munderover>
                    <mml:msubsup>
                      <mml:mi>Œ∏</mml:mi>
                      <mml:mrow>
                        <mml:mi>k</mml:mi>
                        <mml:mi>j</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>,</mml:mo>
                        <mml:msub>
                          <mml:mi>f</mml:mi>
                          <mml:mi>j</mml:mi>
                        </mml:msub>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mi>x</mml:mi>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>,</mml:mo>
                    <mml:mspace width="2.0em"/>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mn>10</mml:mn>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </styled-content>
        </p>
        <p>where <inline-formula><alternatives><tex-math><![CDATA[\theta_{kj}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>Œ∏</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
    are class-conditional feature probabilities estimated from the
    training set. Bayes‚Äô rule then yields the posterior</p>
        <p>
          <styled-content id="eq-nb-posterior">
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[
    p(y=k \mid x)
    ;\propto;
    p(y=k)\prod_{j=1}^{d}\theta_{kj}^{, f_j(x)},
     \qquad(11)]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mi>p</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mi>y</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mi>k</mml:mi>
                      <mml:mo>‚à£</mml:mo>
                      <mml:mi>x</mml:mi>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                    <mml:mo>;</mml:mo>
                    <mml:mo>‚àù</mml:mo>
                    <mml:mo>;</mml:mo>
                    <mml:mi>p</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mi>y</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mi>k</mml:mi>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                    <mml:munderover>
                      <mml:mo>‚àè</mml:mo>
                      <mml:mrow>
                        <mml:mi>j</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>d</mml:mi>
                    </mml:munderover>
                    <mml:msubsup>
                      <mml:mi>Œ∏</mml:mi>
                      <mml:mrow>
                        <mml:mi>k</mml:mi>
                        <mml:mi>j</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>,</mml:mo>
                        <mml:msub>
                          <mml:mi>f</mml:mi>
                          <mml:mi>j</mml:mi>
                        </mml:msub>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mi>x</mml:mi>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>,</mml:mo>
                    <mml:mspace width="2.0em"/>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mn>11</mml:mn>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </styled-content>
        </p>
        <p>and prediction is made via</p>
        <p>
          <styled-content id="eq-nb-prediction">
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[
    \hat{y} = \arg\max_{k\in{0,1}} p(y=k\mid x).
     \qquad(12)]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mover>
                      <mml:mi>y</mml:mi>
                      <mml:mo accent="true">ÃÇ</mml:mo>
                    </mml:mover>
                    <mml:mo>=</mml:mo>
                    <mml:mo>arg</mml:mo>
                    <mml:munder>
                      <mml:mo>max</mml:mo>
                      <mml:mrow>
                        <mml:mi>k</mml:mi>
                        <mml:mo>‚àà</mml:mo>
                        <mml:mrow>
                          <mml:mn>0</mml:mn>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:munder>
                    <mml:mi>p</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mi>y</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mi>k</mml:mi>
                      <mml:mo>‚à£</mml:mo>
                      <mml:mi>x</mml:mi>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                    <mml:mi>.</mml:mi>
                    <mml:mspace width="2.0em"/>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mn>12</mml:mn>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </styled-content>
        </p>
        <p>Although the multinomial model is derived for count data, it is
    widely used with TF‚ÄìIDF features and remains a strong linear
    baseline in high-dimensional text classification.</p>
      </sec>
    </sec>
    <sec id="empirical-assessments">
      <title>Empirical Assessments</title>
      <p>This section reports the empirical performance of the two main
  classifiers: the BiLSTM sequence model and the Naive Bayes
  baseline.</p>
      <sec id="bilstm">
        <title>BiLSTM</title>
        <sec id="headline-results">
          <title>Headline results</title>
          <p>On the held-out test partition, the BiLSTM achieves:</p>
          <p><disp-formula><alternatives><tex-math><![CDATA[
      \text{Test loss} = 0.5229,\qquad
      \text{Test accuracy} = 0.7952.
      ]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mrow><mml:mtext mathvariant="normal">Test loss</mml:mtext><mml:mo>=</mml:mo><mml:mn>0.5229</mml:mn><mml:mo>,</mml:mo><mml:mspace width="2.0em"/><mml:mtext mathvariant="normal">Test accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mn>0.7952</mml:mn><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>{#ewq-bilstm-acc}</p>
          <p>Validation- and test-set performance are closely aligned,
      suggesting stable generalization without pronounced
      overfitting.</p>
        </sec>
        <sec id="class-wise-metrics">
          <title>Class-wise metrics</title>
          <table-wrap>
            <caption>
              <p>BiLSTM: test-set classification report.</p>
            </caption>
            <table>
              <thead>
                <tr>
                  <th align="left">Class</th>
                  <th align="right">Precision</th>
                  <th align="right">Recall</th>
                  <th align="right">F1</th>
                  <th align="right">Support</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left">0</td>
                  <td align="right">0.7621</td>
                  <td align="right">0.6748</td>
                  <td align="right">0.7158</td>
                  <td align="right">12729</td>
                </tr>
                <tr>
                  <td align="left">1</td>
                  <td align="right">0.8121</td>
                  <td align="right">0.8697</td>
                  <td align="right">0.8399</td>
                  <td align="right">20573</td>
                </tr>
                <tr>
                  <td align="left">Accuracy</td>
                  <td align="right">NA</td>
                  <td align="right">NA</td>
                  <td align="right">0.7952</td>
                  <td align="right">33302</td>
                </tr>
                <tr>
                  <td align="left">Macro avg</td>
                  <td align="right">0.7871</td>
                  <td align="right">0.7723</td>
                  <td align="right">0.7779</td>
                  <td align="right">33302</td>
                </tr>
                <tr>
                  <td align="left">Weighted avg</td>
                  <td align="right">0.7930</td>
                  <td align="right">0.7952</td>
                  <td align="right">0.7925</td>
                  <td align="right">33302</td>
                </tr>
              </tbody>
            </table>
          </table-wrap>
          <p>Performance is systematically stronger for class 1, which is
      also the majority class in the data and therefore this behaviour
      is somewhat expected. The macro-averaged
      <inline-formula><alternatives><tex-math><![CDATA[F_1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
      is slightly lower than the weighted average, reflecting the impact
      of class imbalance on aggregate metrics.</p>
        </sec>
        <sec id="confusion-matrix-and-derived-rates">
          <title>Confusion matrix and derived rates</title>
          <p>The test-set confusion matrix for the BiLSTM is:</p>
          <p>
            <styled-content id="eq-bilstm-conf-mat">
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
      \mathbf{C} =
      \begin{pmatrix}
      8590 & 4139 \\
      2681 & 17892
      \end{pmatrix}.
       \qquad(13)]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mi>ùêÇ</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mtable>
                          <mml:mtr>
                            <mml:mtd columnalign="center" style="text-align: center">
                              <mml:mn>8590</mml:mn>
                            </mml:mtd>
                            <mml:mtd columnalign="center" style="text-align: center">
                              <mml:mn>4139</mml:mn>
                            </mml:mtd>
                          </mml:mtr>
                          <mml:mtr>
                            <mml:mtd columnalign="center" style="text-align: center">
                              <mml:mn>2681</mml:mn>
                            </mml:mtd>
                            <mml:mtd columnalign="center" style="text-align: center">
                              <mml:mn>17892</mml:mn>
                            </mml:mtd>
                          </mml:mtr>
                        </mml:mtable>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mi>.</mml:mi>
                      <mml:mspace width="2.0em"/>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>13</mml:mn>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </styled-content>
          </p>
          <p>Treating class 1 as the positive class, the following
      quantities are obtained:</p>
          <p>
            <styled-content id="eq-bilstm-derived-metrics">
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
      \begin{aligned}
      \text{TPR (recall$_1$)} &= \frac{17892}{17892+2681}=0.8697, \\
      \text{TNR (specificity)} &= \frac{8590}{8590+4139}=0.6748, \\
      \text{FPR} &= \frac{4139}{8590+4139}=0.3252, \\
      \text{FNR} &= \frac{2681}{17892+2681}=0.1303, \\
      \text{Balanced accuracy} &= \frac{0.8697+0.6748}{2}=0.7723, \\
      \text{MCC} &= 0.5592.
      \end{aligned}
       \qquad(14)]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mtable>
                        <mml:mtr>
                          <mml:mtd columnalign="right" style="text-align: right">
                            <mml:mtext mathvariant="normal">TPR (recall</mml:mtext>
                            <mml:msub>
                              <mml:mi/>
                              <mml:mn>1</mml:mn>
                            </mml:msub>
                            <mml:mtext mathvariant="normal">)</mml:mtext>
                          </mml:mtd>
                          <mml:mtd columnalign="left" style="text-align: left">
                            <mml:mo>=</mml:mo>
                            <mml:mfrac>
                              <mml:mn>17892</mml:mn>
                              <mml:mrow>
                                <mml:mn>17892</mml:mn>
                                <mml:mo>+</mml:mo>
                                <mml:mn>2681</mml:mn>
                              </mml:mrow>
                            </mml:mfrac>
                            <mml:mo>=</mml:mo>
                            <mml:mn>0.8697</mml:mn>
                            <mml:mo>,</mml:mo>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="right" style="text-align: right">
                            <mml:mtext mathvariant="normal">TNR (specificity)</mml:mtext>
                          </mml:mtd>
                          <mml:mtd columnalign="left" style="text-align: left">
                            <mml:mo>=</mml:mo>
                            <mml:mfrac>
                              <mml:mn>8590</mml:mn>
                              <mml:mrow>
                                <mml:mn>8590</mml:mn>
                                <mml:mo>+</mml:mo>
                                <mml:mn>4139</mml:mn>
                              </mml:mrow>
                            </mml:mfrac>
                            <mml:mo>=</mml:mo>
                            <mml:mn>0.6748</mml:mn>
                            <mml:mo>,</mml:mo>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="right" style="text-align: right">
                            <mml:mtext mathvariant="normal">FPR</mml:mtext>
                          </mml:mtd>
                          <mml:mtd columnalign="left" style="text-align: left">
                            <mml:mo>=</mml:mo>
                            <mml:mfrac>
                              <mml:mn>4139</mml:mn>
                              <mml:mrow>
                                <mml:mn>8590</mml:mn>
                                <mml:mo>+</mml:mo>
                                <mml:mn>4139</mml:mn>
                              </mml:mrow>
                            </mml:mfrac>
                            <mml:mo>=</mml:mo>
                            <mml:mn>0.3252</mml:mn>
                            <mml:mo>,</mml:mo>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="right" style="text-align: right">
                            <mml:mtext mathvariant="normal">FNR</mml:mtext>
                          </mml:mtd>
                          <mml:mtd columnalign="left" style="text-align: left">
                            <mml:mo>=</mml:mo>
                            <mml:mfrac>
                              <mml:mn>2681</mml:mn>
                              <mml:mrow>
                                <mml:mn>17892</mml:mn>
                                <mml:mo>+</mml:mo>
                                <mml:mn>2681</mml:mn>
                              </mml:mrow>
                            </mml:mfrac>
                            <mml:mo>=</mml:mo>
                            <mml:mn>0.1303</mml:mn>
                            <mml:mo>,</mml:mo>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="right" style="text-align: right">
                            <mml:mtext mathvariant="normal">Balanced accuracy</mml:mtext>
                          </mml:mtd>
                          <mml:mtd columnalign="left" style="text-align: left">
                            <mml:mo>=</mml:mo>
                            <mml:mfrac>
                              <mml:mrow>
                                <mml:mn>0.8697</mml:mn>
                                <mml:mo>+</mml:mo>
                                <mml:mn>0.6748</mml:mn>
                              </mml:mrow>
                              <mml:mn>2</mml:mn>
                            </mml:mfrac>
                            <mml:mo>=</mml:mo>
                            <mml:mn>0.7723</mml:mn>
                            <mml:mo>,</mml:mo>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="right" style="text-align: right">
                            <mml:mtext mathvariant="normal">MCC</mml:mtext>
                          </mml:mtd>
                          <mml:mtd columnalign="left" style="text-align: left">
                            <mml:mo>=</mml:mo>
                            <mml:mn>0.5592</mml:mn>
                            <mml:mi>.</mml:mi>
                          </mml:mtd>
                        </mml:mtr>
                      </mml:mtable>
                      <mml:mspace width="2.0em"/>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>14</mml:mn>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </styled-content>
          </p>
        </sec>
        <sec id="error-profile">
          <title>Error profile</title>
          <p>The confusion matrix indicates two main error patterns:</p>
          <list list-type="bullet">
            <list-item>
              <p><bold>Class 0 ‚Üí Class 1</bold>: A comparatively large
          number of false positives (items from class 0 predicted as
          class 1) likely arises from shared lexical or stylistic
          features between the two parties, amplified by class imbalance
          which then leads to a bigger set of possible patterns are
          beeing labeled as class 1.</p>
            </list-item>
            <list-item>
              <p><bold>Class 1 ‚Üí Class 0</bold>: Fewer false negatives for
          class 1, which may correspond to more moderate or
          cross-partisan language that is less prototypical of the
          majority class.</p>
            </list-item>
          </list>
          <p>The false-positive rate is roughly twice the false-negative
      rate, implying that the classifier tends to err toward predicting
      class 1. In applications with asymmetric error costs, this
      tendency could be counteracted by threshold adjustment,
      class-weighted loss functions, or post-hoc calibration.</p>
          <p>The distribution of posts lengths does not have an impact on
      the error profile as throughout the dataset the post length
      distribution is similar across classes as shown in
      <xref alt="Section¬†3" rid="sec-eda">Section¬†3</xref> .</p>
        </sec>
      </sec>
      <sec id="qualitative-analysis">
        <title>Qualitative Analysis</title>
        <p>To show the qualitative implications of the model a sample of
    posts is classified here.</p>
        <sec id="single-words">
          <title>Single words</title>
          <p>The model is given a list of the most comon english words and
      their predicted probabilities for each class. The results are
      visualized in
      <xref alt="Figure¬†2" rid="fig-word-bias">Figure¬†2</xref>, which
      plots the margin (p_dem ‚àí p_repub) for each word against its rank
      in the sorted list. Points are colored by margin and sized by
      confidence (maximum predicted probability). The twenty most
      extreme words on either end of the spectrum are labeled.</p>
          <fig id="fig-word-bias">
            <caption>
              <p>Figure¬†2: Model bias on common words</p>
            </caption>
            <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-word-bias-1.png"/>
          </fig>
          <p>As Figure
      <xref alt="Figure¬†2" rid="fig-word-bias">Figure¬†2</xref>
      indicates, the classifier shows pronounced partisan leanings even
      when evaluated on isolated tokens.</p>
          <p>A natural next step is to test whether this behavior is stable
      over time. In particular, one could examine whether applying the
      model to contemporary Republican and Democratic posts changes its
      performance, given well-documented shifts in political
      communication and issue framing over recent years.</p>
        </sec>
      </sec>
      <sec id="naive-bayes-baseline">
        <title>Naive Bayes Baseline</title>
        <sec id="headline-results-1">
          <title>Headline results</title>
          <p>As a bag-of-words baseline, the Naive Bayes classifier attains
      the following accuracy on the test set:</p>
          <p>
            <styled-content id="eq-nb-acc">
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
      \text{Test accuracy} = 0.7843
       \qquad(15)]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mtext mathvariant="normal">Test accuracy</mml:mtext>
                      <mml:mo>=</mml:mo>
                      <mml:mn>0.7843</mml:mn>
                      <mml:mspace width="2.0em"/>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>15</mml:mn>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </styled-content>
          </p>
          <p>Given its simplicity and lack of contextual or sequential
      modeling, this level of performance is comparatively strong, but
      still below that of the BiLSTM.</p>
        </sec>
        <sec id="class-wise-metrics-1">
          <title>Class-wise metrics</title>
          <table-wrap>
            <caption>
              <p>Naive Bayes: test-set classification report.</p>
            </caption>
            <table>
              <thead>
                <tr>
                  <th align="left">Class</th>
                  <th align="right">Precision</th>
                  <th align="right">Recall</th>
                  <th align="right">F1</th>
                  <th align="right">Support</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left">0</td>
                  <td align="right">0.7698</td>
                  <td align="right">0.6214</td>
                  <td align="right">0.6877</td>
                  <td align="right">12729</td>
                </tr>
                <tr>
                  <td align="left">1</td>
                  <td align="right">0.7907</td>
                  <td align="right">0.8850</td>
                  <td align="right">0.8352</td>
                  <td align="right">20573</td>
                </tr>
                <tr>
                  <td align="left">Accuracy</td>
                  <td align="right">NA</td>
                  <td align="right">NA</td>
                  <td align="right">0.7843</td>
                  <td align="right">33302</td>
                </tr>
                <tr>
                  <td align="left">Macro avg</td>
                  <td align="right">0.7803</td>
                  <td align="right">0.7532</td>
                  <td align="right">0.7615</td>
                  <td align="right">33302</td>
                </tr>
                <tr>
                  <td align="left">Weighted avg</td>
                  <td align="right">0.7827</td>
                  <td align="right">0.7843</td>
                  <td align="right">0.7788</td>
                  <td align="right">33302</td>
                </tr>
              </tbody>
            </table>
          </table-wrap>
          <p>As with the BiLSTM, performance is higher for class 1 than for
      class 0. Precision and recall for class 1 are clearly stronger,
      indicating a tendency to assign documents to the majority class
      and to capture its lexical cues more reliably.</p>
        </sec>
        <sec id="confusion-matrix-and-error-profile">
          <title>Confusion matrix and error profile</title>
          <p>The corresponding test-set confusion matrix for the Naive Bayes
      classifier is</p>
          <p>
            <styled-content id="eq-nb-conf-mat">
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
      \begin{pmatrix}
      9599 & 3171 \\
      2025 & 18666
      \end{pmatrix}.
       \qquad(16)]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mtable>
                          <mml:mtr>
                            <mml:mtd columnalign="center" style="text-align: center">
                              <mml:mn>9599</mml:mn>
                            </mml:mtd>
                            <mml:mtd columnalign="center" style="text-align: center">
                              <mml:mn>3171</mml:mn>
                            </mml:mtd>
                          </mml:mtr>
                          <mml:mtr>
                            <mml:mtd columnalign="center" style="text-align: center">
                              <mml:mn>2025</mml:mn>
                            </mml:mtd>
                            <mml:mtd columnalign="center" style="text-align: center">
                              <mml:mn>18666</mml:mn>
                            </mml:mtd>
                          </mml:mtr>
                        </mml:mtable>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mi>.</mml:mi>
                      <mml:mspace width="2.0em"/>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>16</mml:mn>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </styled-content>
          </p>
          <p>False positives for class 1 (3171 cases) are moderately more
      frequent than in the BiLSTM. Overall, the baseline captures strong
      lexical signals at low computational cost but offers less balanced
      performance across classes than the sequence model.</p>
        </sec>
      </sec>
      <sec id="comparative-perspective">
        <title>Comparative perspective</title>
        <p>To assess whether the BiLSTM improves upon the Naive Bayes
    baseline in a systematic way, a side-by-side comparison of key
    metrics is provided in
    <xref alt="Table¬†1" rid="tbl-comp">Table¬†1</xref>. The BiLSTM offers
    modest gains in overall accuracy and more favorable error balance
    across classes, while the Naive Bayes classifier remains a
    competitive and interpretable lexical baseline.</p>
        <fig id="tbl-comp">
          <caption>
            <p>Table¬†1: Model Comparison</p>
          </caption>
          <table-wrap>
            <table>
              <colgroup>
                <col width="11%"/>
                <col width="11%"/>
                <col width="11%"/>
                <col width="11%"/>
                <col width="11%"/>
                <col width="11%"/>
                <col width="11%"/>
                <col width="11%"/>
                <col width="11%"/>
              </colgroup>
              <thead>
                <tr>
                  <th colspan="9">Model Comparison</th>
                </tr>
                <tr>
                  <th colspan="9">Cells highlighted indicate the winning
              model for that specific metric</th>
                </tr>
                <tr>
                  <th rowspan="2" id="Class" scope="col">Class</th>
                  <th colspan="4" id="BiLSTM" scope="colgroup">BiLSTM</th>
                  <th colspan="4" id="NaiveU0020Bayes" scope="colgroup">Naive
              Bayes</th>
                </tr>
                <tr>
                  <th id="Precision_BiLSTM" scope="col">Precision</th>
                  <th id="Recall_BiLSTM" scope="col">Recall</th>
                  <th id="F1_BiLSTM" scope="col">F1</th>
                  <th id="Support_BiLSTM" scope="col">Support</th>
                  <th id="Precision_NB" scope="col">Precision</th>
                  <th id="Recall_NB" scope="col">Recall</th>
                  <th id="F1_NB" scope="col">F1</th>
                  <th id="Support_NB" scope="col">Support</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td headers="Class">0</td>
                  <td headers="Precision_BiLSTM">0.7621</td>
                  <td headers="Recall_BiLSTM" style="background-color: #D4F0D4">0.6748</td>
                  <td headers="F1_BiLSTM" style="background-color: #D4F0D4">0.7158</td>
                  <td headers="Support_BiLSTM" style="border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3">12729</td>
                  <td headers="Precision_NB" style="background-color: #D4F0D4">0.7698</td>
                  <td headers="Recall_NB">0.6214</td>
                  <td headers="F1_NB">0.6877</td>
                  <td headers="Support_NB">12729</td>
                </tr>
                <tr>
                  <td headers="Class">1</td>
                  <td headers="Precision_BiLSTM" style="background-color: #D4F0D4">0.8121</td>
                  <td headers="Recall_BiLSTM">0.8697</td>
                  <td headers="F1_BiLSTM" style="background-color: #D4F0D4">0.8399</td>
                  <td headers="Support_BiLSTM" style="border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3">20573</td>
                  <td headers="Precision_NB">0.7907</td>
                  <td headers="Recall_NB" style="background-color: #D4F0D4">0.8850</td>
                  <td headers="F1_NB">0.8352</td>
                  <td headers="Support_NB">20573</td>
                </tr>
                <tr>
                  <td headers="Class">Accuracy</td>
                  <td headers="Precision_BiLSTM">-</td>
                  <td headers="Recall_BiLSTM">-</td>
                  <td headers="F1_BiLSTM" style="background-color: #D4F0D4">0.7952</td>
                  <td headers="Support_BiLSTM" style="border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3">33302</td>
                  <td headers="Precision_NB">-</td>
                  <td headers="Recall_NB">-</td>
                  <td headers="F1_NB">0.7843</td>
                  <td headers="Support_NB">33302</td>
                </tr>
                <tr>
                  <td headers="Class">Macro avg</td>
                  <td headers="Precision_BiLSTM" style="background-color: #D4F0D4">0.7871</td>
                  <td headers="Recall_BiLSTM" style="background-color: #D4F0D4">0.7723</td>
                  <td headers="F1_BiLSTM" style="background-color: #D4F0D4">0.7779</td>
                  <td headers="Support_BiLSTM" style="border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3">33302</td>
                  <td headers="Precision_NB">0.7803</td>
                  <td headers="Recall_NB">0.7532</td>
                  <td headers="F1_NB">0.7615</td>
                  <td headers="Support_NB">33302</td>
                </tr>
                <tr>
                  <td headers="Class">Weighted avg</td>
                  <td headers="Precision_BiLSTM" style="background-color: #D4F0D4">0.7930</td>
                  <td headers="Recall_BiLSTM" style="background-color: #D4F0D4">0.7952</td>
                  <td headers="F1_BiLSTM" style="background-color: #D4F0D4">0.7925</td>
                  <td headers="Support_BiLSTM" style="border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3">33302</td>
                  <td headers="Precision_NB">0.7827</td>
                  <td headers="Recall_NB">0.7843</td>
                  <td headers="F1_NB">0.7788</td>
                  <td headers="Support_NB">33302</td>
                </tr>
              </tbody>
            </table>
          </table-wrap>
        </fig>
      </sec>
    </sec>
    <sec id="limitations-and-possible-extensions">
      <title>Limitations and possible extensions</title>
      <sec id="limitations">
        <title>Limitations</title>
        <p>The primary constraint of the current model is its reliance on
    frozen feature extraction. Because the Transformer
    <inline-formula><alternatives><tex-math><![CDATA[\mathcal{F}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>‚Ñ±</mml:mi></mml:math></alternatives></inline-formula>
    functions purely as a static input generator with stop-gradient
    operations, the embedding space is unable to adapt to the specific
    political vernacular of the congressional dataset during
    backpropagation. This lack of domain adaptation is compounded by
    precision limitations; the utilization of a 4-bit quantized
    Transformer and the subsequent casting of embeddings to
    <monospace>float16</monospace> introduces quantization noise that,
    while efficient, may discard subtle semantic distinctions found in
    full-precision representations. Furthermore, the architecture
    exhibits potential redundancy by stacking a BiLSTM on top of a
    powerful contextual encoder. Since the MiniLM Transformer already
    captures long-range dependencies via self-attention, the addition of
    recurrence introduces a sequential bottleneck without necessarily
    increasing modeling capacity significantly. Finally, the masked mean
    pooling strategy treats all valid tokens as equally important, a
    simplification that risks diluting strong, localized discriminative
    signals within longer posts.</p>
      </sec>
      <sec id="extensions">
        <title>Extensions</title>
        <p>To address these limitations, several architectural and training
    modifications could be implemented. A significant performance boost
    could be achieved through end-to-end fine-tuning, where the
    Transformer layers are unfrozen to allow gradients to flow through
    <inline-formula><alternatives><tex-math><![CDATA[\mathcal{F}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>‚Ñ±</mml:mi></mml:math></alternatives></inline-formula>.
    To manage the memory overhead of this approach on Apple Silicon,
    techniques such as Low-Rank Adaptation (LoRA) could be employed to
    fine-tune the encoder efficiently. Alternatively, efficiency could
    be prioritized by moving to a Transformer-only architecture,
    removing the BiLSTM layer entirely and projecting the Transformer‚Äôs
    <monospace>[CLS]</monospace> token directly to the classification
    head; this would simplify the compute graph and eliminate the
    sequential dependency. Regarding feature aggregation, replacing mean
    pooling with a learnable attention mechanism would allow the model
    to weigh informative keywords more heavily than neutral connective
    text. Finally, while the current model truncates inputs at 128
    tokens, implementing a sliding window approach or utilizing the full
    512-token capacity of MiniLM would better capture tail-end context
    in lengthy statements.</p>
      </sec>
    </sec>
    <sec id="conclusion">
      <title>Conclusion</title>
      <p>The implemented pipeline offers a computationally efficient neural
  baseline for binary party attribution in political texts. By combining
  subword token IDs with frozen embeddings and a BiLSTM encoder, the
  system captures sequential patterns and achieves solid test
  performance. Evaluation reveals a mild bias toward predicting the
  majority class, visible in an elevated false-positive rate for class
  <inline-formula><alternatives><tex-math><![CDATA[0]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>0</mml:mn></mml:math></alternatives></inline-formula>.</p>
    </sec>
    <sec id="external-sources-used">
      <title>External sources used</title>
      <list list-type="bullet">
        <list-item>
          <p>
            <ext-link ext-link-type="uri" xlink:href="https://gist.githubusercontent.com/deekayen/4148741/raw/98d35708fa344717d8eee15d11987de6c8e26d7d/1-1000.txt">list
      of most common english words</ext-link>
          </p>
        </list-item>
        <list-item>
          <p>
            <ext-link ext-link-type="uri" xlink:href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2FBOK1CF&amp;utm_source=chatgpt.com">Dataset</ext-link>
          </p>
        </list-item>
        <list-item>
          <p>
            <ext-link ext-link-type="uri" xlink:href="https://github.com/Blaizzy/mlx-embeddings">MLX-Embeddings
      repo</ext-link>
          </p>
        </list-item>
        <list-item>
          <p>
            <ext-link ext-link-type="uri" xlink:href="https://github.com/ml-explore/mlx">MLX
      Repositories</ext-link>
          </p>
        </list-item>
        <list-item>
          <p>
            <ext-link ext-link-type="uri" xlink:href="https://github.com/abeleinin/mlx-xLSTM">MLX
      Implementation of xLSTM</ext-link>
          </p>
        </list-item>
        <list-item>
          <p>
            <ext-link ext-link-type="uri" xlink:href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LSTM.html">Pytorch
      implementaino of LSTM</ext-link>
          </p>
        </list-item>
      </list>
      <p/>
    </sec>
  </body>
  <back>
</back>
  <sub-article article-type="notebook" id="nb-3-nb-article">
    <front-stub>
      <title-group>
        <article-title>REP or DEM LSTM classification report</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Otto</surname>
            <given-names>Paul Elvis</given-names>
          </name>
          <string-name>Paul Elvis Otto</string-name>
          <role vocab="https://credit.niso.org" vocab-term="writing ‚Äì original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">writing</role>
          <xref ref-type="aff" rid="aff-1-nb-article">a</xref>
        </contrib>
      </contrib-group>
      <aff id="aff-1-nb-article">
        <institution-wrap>
          <institution>Duke University / Hertie School</institution>
        </institution-wrap>
      </aff>
    </front-stub>
    <body>
      <sec id="intro-nb-article">
        <title>Intro</title>
        <p>Classifying short social-media texts is a standard NLP task and a
  useful testbed for comparing lexical and sequence-based models. This
  project predicts whether a tweet posted by a member of the 112th U.S.
  Congress was authored by a Democrat or a Republican. The dataset,
  sourced from Harvard Dataverse, contains each post‚Äôs text as well as
  metadata on the author‚Äôs party and chamber (House vs.¬†Senate).</p>
        <p>The main model is a bidirectional LSTM classifier operating over
  contextual token embeddings. Instead of learning word representations
  from scratch, the pipeline uses a pretrained MiniLM Transformer as a
  frozen feature extractor (loaded via the MLX-Embeddings/Hugging Face
  adaptation). The BiLSTM and linear classification head are trained on
  top of these fixed embeddings. As a transparent baseline, a
  Multinomial Naive Bayes model with TF‚ÄìIDF n-gram features is
  implemented on the same train/validation/test split, and its
  performance is compared to the BiLSTM in the empirical assessment
  section.</p>
        <p>To situate the task, an exploratory data analysis is presented in
  <xref alt="Section¬†3" rid="sec-eda-nb-article">Section¬†3</xref>. The text
  preprocessing and label normalization used to construct the modeling
  dataset are described in
  <xref alt="Section¬†2" rid="sec-data-prep-nb-article">Section¬†2</xref>.</p>
      </sec>
      <sec id="sec-data-prep-nb-article">
        <title>Data Preparation</title>
        <p>To ensure the data is usable and not influenced by elements such as
  dates, mentions, or links, each post was cleaned using a set of
  regular expressions. The cleaning script is available in the model‚Äôs
  repository. The cleaned dataset excludes the following:</p>
        <list list-type="bullet">
          <list-item>
            <p>Dates</p>
          </list-item>
          <list-item>
            <p>Mentions</p>
          </list-item>
          <list-item>
            <p>Hashtags</p>
          </list-item>
          <list-item>
            <p>URLs</p>
          </list-item>
        </list>
        <p>The data is read in already cleaned into the model pipeline.</p>
      </sec>
      <sec id="sec-eda-nb-article">
        <title>EDA</title>
        <p>This section provides an initial overview of the dataset by
  examining its basic structure and several descriptive metrics.</p>
        <sec specific-use="notebook-content">
          <code language="r script">library(dplyr)</code>
          <boxed-text>
            <preformat>
Attaching package: 'dplyr'</preformat>
          </boxed-text>
          <boxed-text>
            <preformat>The following objects are masked from 'package:stats':

    filter, lag</preformat>
          </boxed-text>
          <boxed-text>
            <preformat>The following objects are masked from 'package:base':

    intersect, setdiff, setequal, union</preformat>
          </boxed-text>
          <code language="r script">df &lt;- read.csv("./data/congress_complete_clean.csv")
shape &lt;- dim(df)</code>
        </sec>
        <p>The dataset has a dimension of 334606, 5. To begin, we look at how
  posts are distributed across chambers and parties. The overall
  distribution is visualized in
  <xref alt="Figure¬†1" rid="fig-post-dist-nb-article">Figure¬†1</xref>.</p>
        <sec id="cell-fig-post-dist-nb-article" specific-use="notebook-content">
          <code language="r script">library(dplyr)
library(ggplot2)

df_counts &lt;- df %&gt;%
  count(organ, party)

ggplot(df_counts, aes(x = organ, y = n, fill = party)) +
  geom_col(position = "dodge") +
  labs(x = "Chamber", y = "Number of posts") +
  theme_classic()</code>
          <fig id="fig-post-dist-nb-article">
            <caption>
              <p>Figure¬†1: Distribution of posts by chamber and
    party</p>
            </caption>
            <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-post-dist-1.png"/>
          </fig>
        </sec>
        <p>A more detailed breakdown of total posts per chamber and party is
  provided in the table below:</p>
        <sec specific-use="notebook-content">
          <code language="r script">library(dplyr)
library(tidyr)
library(gt)

df_table &lt;- df %&gt;%
  count(organ, party) %&gt;%
  pivot_wider(
    names_from = party,
    values_from = n,
    values_fill = 0
  )

df_table %&gt;%
  gt(rowname_col = "organ") %&gt;%
  tab_header(
    title = "Number of Posts by Chamber and Party"
  ) %&gt;%
  cols_label(
    organ = "Chamber"
  )</code>
          <table-wrap>
            <table>
              <thead>
                <tr>
                  <th colspan="3">Number of Posts by Chamber and Party</th>
                </tr>
                <tr>
                  <th id="aU003AU003Astub-nb-article" scope="col"/>
                  <th id="democrat-nb-article" scope="col">democrat</th>
                  <th id="republican-nb-article" scope="col">republican</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td id="stub_1_1-nb-article" scope="row">house</td>
                  <td headers="stub_1_1 democrat">89808</td>
                  <td headers="stub_1_1 republican">167004</td>
                </tr>
                <tr>
                  <td id="stub_1_2-nb-article" scope="row">senate</td>
                  <td headers="stub_1_2 democrat">37890</td>
                  <td headers="stub_1_2 republican">39904</td>
                </tr>
              </tbody>
            </table>
          </table-wrap>
        </sec>
        <p>Next, the distribution of post lengths is examined. To limit the
  influence of outliers, the top one percent of longest posts are
  excluded:</p>
        <sec specific-use="notebook-content">
          <code language="r script">library(dplyr)
library(ggplot2)
library(stringr)

df_clean &lt;- df %&gt;%
  mutate(length = str_length(post)) %&gt;%
  group_by(party, organ) %&gt;%
  mutate(cutoff = quantile(length, 0.99, na.rm = TRUE)) %&gt;%
  ungroup() %&gt;%
  filter(length &lt;= cutoff)

ggplot(df_clean, aes(x = party, y = length, fill = party)) +
  geom_violin(trim = TRUE) +
  facet_wrap(~organ) +
  coord_cartesian(ylim = c(0, 200)) +
  labs(
    x = "Party",
    y = "Post length",
    title = "Distribution of post lengths (top 1% removed)"
  ) +
  theme_classic() +
  theme(legend.position = "none")</code>
          <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-2-1.png"/>
        </sec>
        <p>The dataset also allows identifying the most active members. The
  figure below displays the top five posters for each chamber‚Äìparty
  combination:</p>
        <sec specific-use="notebook-content">
          <code language="r script">library(dplyr)
library(ggplot2)

df_top &lt;- df %&gt;%
  count(organ, party, member, name = "posts") %&gt;%
  group_by(organ, party) %&gt;%
  slice_max(order_by = posts, n = 5, with_ties = FALSE) %&gt;%
  ungroup()

ggplot(df_top, aes(x = reorder(member, posts), y = posts, fill = party)) +
  geom_col() +
  coord_flip() +
  facet_grid(organ ~ party, scales = "free_y") +
  labs(
    x = "Member",
    y = "Number of posts",
    title = "Top 5 posters per chamber and party"
  ) +
  theme_classic() +
  theme(legend.position = "none")</code>
          <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-3-1.png"/>
        </sec>
        <p>Finally, the most frequent words used by each party are explored.
  After tokenizing posts, removing stopwords, and excluding a small set
  of custom stop terms, the top twenty terms per party are
  identified:</p>
        <sec specific-use="notebook-content">
          <code language="r script">library(dplyr)
library(tidytext)
library(ggplot2)
library(stringr)

custom_stop &lt;- tibble(word = c("rt"))

df_words &lt;- df %&gt;%
  mutate(post = str_to_lower(post)) %&gt;%
  unnest_tokens(word, post) %&gt;%
  anti_join(stop_words, by = "word") %&gt;%
  anti_join(custom_stop, by = "word") %&gt;%
  filter(word != "") %&gt;%
  count(party, word, sort = TRUE)

top_n_words &lt;- 20

df_top &lt;- df_words %&gt;%
  group_by(party) %&gt;%
  slice_max(order_by = n, n = top_n_words) %&gt;%
  ungroup()

ggplot(df_top, aes(x = reorder_within(word, n, party), y = n, fill = party)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~party, scales = "free_y") +
  scale_x_reordered() +
  labs(
    x = "Word",
    y = "Frequency",
    title = "Top word frequencies by party"
  ) +
  theme_classic() +
  theme(legend.position = "none")</code>
          <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-4-1.png"/>
        </sec>
      </sec>
      <sec id="model-specification-nb-article">
        <title>Model specification</title>
        <sec id="tokenization-and-feature-extraction-nb-article">
          <title>Tokenization and feature extraction</title>
          <p>The input pipeline leverages a pre-trained Transformer model,
    specifically <monospace>all-MiniLM-L6-v2</monospace>, to generate
    contextualized semantic features. Unlike traditional pipelines that
    feed token IDs directly to the classifier, this architecture
    utilizes the Transformer as a frozen feature extractor.</p>
          <p>Raw text is tokenized and padded to a fixed sequence length
    <inline-formula><alternatives><tex-math><![CDATA[T=128]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.
    Let <inline-formula><alternatives><tex-math><![CDATA[\mathbf{t}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê≠</mml:mi></mml:math></alternatives></inline-formula>
    be the sequence of token identifiers. These are passed through the
    quantized (4-bit) Transformer model <inline-formula><alternatives><tex-math><![CDATA[\mathcal{F}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>‚Ñ±</mml:mi></mml:math></alternatives></inline-formula>
    to yield a sequence of dense vectors:</p>
          <p>
            <styled-content id="eq-transformer-out-nb-article">
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
    \mathbf{X} = \mathcal{F}(\mathbf{t}) \in \mathbb{R}^{T \times D},
     \qquad(1)]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mi>ùêó</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mi>‚Ñ±</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>ùê≠</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>‚àà</mml:mo>
                      <mml:msup>
                        <mml:mi>‚Ñù</mml:mi>
                        <mml:mrow>
                          <mml:mi>T</mml:mi>
                          <mml:mo>√ó</mml:mo>
                          <mml:mi>D</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mo>,</mml:mo>
                      <mml:mspace width="2.0em"/>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>1</mml:mn>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </styled-content>
          </p>
          <p>where <inline-formula><alternatives><tex-math><![CDATA[D]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>D</mml:mi></mml:math></alternatives></inline-formula>
    is the embedding dimension (inferred from the Transformer output,
    typically 384 for MiniLM). Crucially, a stop-gradient operation is
    applied to <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>:</p>
          <p>
            <styled-content id="eq-stop-grad-nb-article">
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
    \mathbf{X}_{\text{fixed}} = \text{StopGradient}(\mathbf{X}),
     \qquad(2)]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:msub>
                        <mml:mi>ùêó</mml:mi>
                        <mml:mtext mathvariant="normal">fixed</mml:mtext>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mtext mathvariant="normal">StopGradient</mml:mtext>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>ùêó</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>,</mml:mo>
                      <mml:mspace width="2.0em"/>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>2</mml:mn>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </styled-content>
          </p>
          <p>ensuring that gradients are not backpropagated into the
    Transformer layers, treating the embeddings as static inputs to the
    downstream LSTM.</p>
        </sec>
        <sec id="bidirectional-lstm-encoder-nb-article">
          <title>Bidirectional LSTM encoder</title>
          <p>A custom implementation of a Bidirectional LSTM is employed to
    model the temporal dependencies within the sequence of embeddings
    <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}_{\text{fixed}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>ùêó</mml:mi><mml:mtext mathvariant="normal">fixed</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>.</p>
          <p>The architecture consists of two independent LSTM layers: a
    forward pass (<inline-formula><alternatives><tex-math><![CDATA[\overrightarrow{\text{LSTM}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mover><mml:mtext mathvariant="normal">LSTM</mml:mtext><mml:mo accent="true">‚Üí</mml:mo></mml:mover></mml:math></alternatives></inline-formula>)
    and a backward pass (<inline-formula><alternatives><tex-math><![CDATA[\overleftarrow{\text{LSTM}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mover><mml:mtext mathvariant="normal">LSTM</mml:mtext><mml:mo accent="true">‚Éñ</mml:mo></mml:mover></mml:math></alternatives></inline-formula>).
    For a single direction, the state update at timestep
    <inline-formula><alternatives><tex-math><![CDATA[t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>,
    given input <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}_t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>ùê±</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
    and previous hidden state <inline-formula><alternatives><tex-math><![CDATA[\mathbf{h}_{t-1}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>ùê°</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>,
    is governed by the standard gate equations:</p>
          <p>
            <styled-content id="eq-lstm-gates-nb-article">
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
    \begin{align}
    \mathbf{i}_t &= \sigma(W_i\mathbf{x}_t + U_i\mathbf{h}_{t-1} + \mathbf{b}_i) \\
    \mathbf{f}_t &= \sigma(W_f\mathbf{x}_t + U_f\mathbf{h}_{t-1} + \mathbf{b}_f) \\
    \mathbf{o}_t &= \sigma(W_o\mathbf{x}_t + U_o\mathbf{h}_{t-1} + \mathbf{b}_o) \\
    \mathbf{g}_t &= \tanh(W_g\mathbf{x}_t + U_g\mathbf{h}_{t-1} + \mathbf{b}_g) \\
    \mathbf{c}_t &= \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \mathbf{g}_t \\
    \mathbf{h}_t &= \mathbf{o}_t \odot \tanh(\mathbf{c}_t)
    \end{align}
     \qquad(3)]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mtable>
                        <mml:mtr>
                          <mml:mtd columnalign="right" style="text-align: right">
                            <mml:msub>
                              <mml:mi>ùê¢</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="left" style="text-align: left">
                            <mml:mo>=</mml:mo>
                            <mml:mi>œÉ</mml:mi>
                            <mml:mrow>
                              <mml:mo stretchy="true" form="prefix">(</mml:mo>
                              <mml:msub>
                                <mml:mi>W</mml:mi>
                                <mml:mi>i</mml:mi>
                              </mml:msub>
                              <mml:msub>
                                <mml:mi>ùê±</mml:mi>
                                <mml:mi>t</mml:mi>
                              </mml:msub>
                              <mml:mo>+</mml:mo>
                              <mml:msub>
                                <mml:mi>U</mml:mi>
                                <mml:mi>i</mml:mi>
                              </mml:msub>
                              <mml:msub>
                                <mml:mi>ùê°</mml:mi>
                                <mml:mrow>
                                  <mml:mi>t</mml:mi>
                                  <mml:mo>‚àí</mml:mo>
                                  <mml:mn>1</mml:mn>
                                </mml:mrow>
                              </mml:msub>
                              <mml:mo>+</mml:mo>
                              <mml:msub>
                                <mml:mi>ùêõ</mml:mi>
                                <mml:mi>i</mml:mi>
                              </mml:msub>
                              <mml:mo stretchy="true" form="postfix">)</mml:mo>
                            </mml:mrow>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="right" style="text-align: right">
                            <mml:msub>
                              <mml:mi>ùêü</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="left" style="text-align: left">
                            <mml:mo>=</mml:mo>
                            <mml:mi>œÉ</mml:mi>
                            <mml:mrow>
                              <mml:mo stretchy="true" form="prefix">(</mml:mo>
                              <mml:msub>
                                <mml:mi>W</mml:mi>
                                <mml:mi>f</mml:mi>
                              </mml:msub>
                              <mml:msub>
                                <mml:mi>ùê±</mml:mi>
                                <mml:mi>t</mml:mi>
                              </mml:msub>
                              <mml:mo>+</mml:mo>
                              <mml:msub>
                                <mml:mi>U</mml:mi>
                                <mml:mi>f</mml:mi>
                              </mml:msub>
                              <mml:msub>
                                <mml:mi>ùê°</mml:mi>
                                <mml:mrow>
                                  <mml:mi>t</mml:mi>
                                  <mml:mo>‚àí</mml:mo>
                                  <mml:mn>1</mml:mn>
                                </mml:mrow>
                              </mml:msub>
                              <mml:mo>+</mml:mo>
                              <mml:msub>
                                <mml:mi>ùêõ</mml:mi>
                                <mml:mi>f</mml:mi>
                              </mml:msub>
                              <mml:mo stretchy="true" form="postfix">)</mml:mo>
                            </mml:mrow>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="right" style="text-align: right">
                            <mml:msub>
                              <mml:mi>ùê®</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="left" style="text-align: left">
                            <mml:mo>=</mml:mo>
                            <mml:mi>œÉ</mml:mi>
                            <mml:mrow>
                              <mml:mo stretchy="true" form="prefix">(</mml:mo>
                              <mml:msub>
                                <mml:mi>W</mml:mi>
                                <mml:mi>o</mml:mi>
                              </mml:msub>
                              <mml:msub>
                                <mml:mi>ùê±</mml:mi>
                                <mml:mi>t</mml:mi>
                              </mml:msub>
                              <mml:mo>+</mml:mo>
                              <mml:msub>
                                <mml:mi>U</mml:mi>
                                <mml:mi>o</mml:mi>
                              </mml:msub>
                              <mml:msub>
                                <mml:mi>ùê°</mml:mi>
                                <mml:mrow>
                                  <mml:mi>t</mml:mi>
                                  <mml:mo>‚àí</mml:mo>
                                  <mml:mn>1</mml:mn>
                                </mml:mrow>
                              </mml:msub>
                              <mml:mo>+</mml:mo>
                              <mml:msub>
                                <mml:mi>ùêõ</mml:mi>
                                <mml:mi>o</mml:mi>
                              </mml:msub>
                              <mml:mo stretchy="true" form="postfix">)</mml:mo>
                            </mml:mrow>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="right" style="text-align: right">
                            <mml:msub>
                              <mml:mi>ùê†</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="left" style="text-align: left">
                            <mml:mo>=</mml:mo>
                            <mml:mo>tanh</mml:mo>
                            <mml:mrow>
                              <mml:mo stretchy="true" form="prefix">(</mml:mo>
                              <mml:msub>
                                <mml:mi>W</mml:mi>
                                <mml:mi>g</mml:mi>
                              </mml:msub>
                              <mml:msub>
                                <mml:mi>ùê±</mml:mi>
                                <mml:mi>t</mml:mi>
                              </mml:msub>
                              <mml:mo>+</mml:mo>
                              <mml:msub>
                                <mml:mi>U</mml:mi>
                                <mml:mi>g</mml:mi>
                              </mml:msub>
                              <mml:msub>
                                <mml:mi>ùê°</mml:mi>
                                <mml:mrow>
                                  <mml:mi>t</mml:mi>
                                  <mml:mo>‚àí</mml:mo>
                                  <mml:mn>1</mml:mn>
                                </mml:mrow>
                              </mml:msub>
                              <mml:mo>+</mml:mo>
                              <mml:msub>
                                <mml:mi>ùêõ</mml:mi>
                                <mml:mi>g</mml:mi>
                              </mml:msub>
                              <mml:mo stretchy="true" form="postfix">)</mml:mo>
                            </mml:mrow>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="right" style="text-align: right">
                            <mml:msub>
                              <mml:mi>ùêú</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="left" style="text-align: left">
                            <mml:mo>=</mml:mo>
                            <mml:msub>
                              <mml:mi>ùêü</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                            <mml:mo>‚äô</mml:mo>
                            <mml:msub>
                              <mml:mi>ùêú</mml:mi>
                              <mml:mrow>
                                <mml:mi>t</mml:mi>
                                <mml:mo>‚àí</mml:mo>
                                <mml:mn>1</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                            <mml:mo>+</mml:mo>
                            <mml:msub>
                              <mml:mi>ùê¢</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                            <mml:mo>‚äô</mml:mo>
                            <mml:msub>
                              <mml:mi>ùê†</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="right" style="text-align: right">
                            <mml:msub>
                              <mml:mi>ùê°</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="left" style="text-align: left">
                            <mml:mo>=</mml:mo>
                            <mml:msub>
                              <mml:mi>ùê®</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                            <mml:mo>‚äô</mml:mo>
                            <mml:mo>tanh</mml:mo>
                            <mml:mrow>
                              <mml:mo stretchy="true" form="prefix">(</mml:mo>
                              <mml:msub>
                                <mml:mi>ùêú</mml:mi>
                                <mml:mi>t</mml:mi>
                              </mml:msub>
                              <mml:mo stretchy="true" form="postfix">)</mml:mo>
                            </mml:mrow>
                          </mml:mtd>
                        </mml:mtr>
                      </mml:mtable>
                      <mml:mspace width="2.0em"/>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>3</mml:mn>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </styled-content>
          </p>
          <p>where <inline-formula><alternatives><tex-math><![CDATA[\sigma]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>œÉ</mml:mi></mml:math></alternatives></inline-formula>
    is the sigmoid function and <inline-formula><alternatives><tex-math><![CDATA[\odot]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>‚äô</mml:mi></mml:math></alternatives></inline-formula>
    is the Hadamard product. The hidden state dimension is
    <inline-formula><alternatives><tex-math><![CDATA[H=128]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.
    The backward LSTM processes the sequence in reverse order. The final
    representation at each timestep is the concatenation of both
    directional states:</p>
          <p>
            <styled-content id="eq-bilstm-out-nb-article">
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
    \mathbf{h}_t = [\overrightarrow{\mathbf{h}}_t ; \overleftarrow{\mathbf{h}}_t] \in \mathbb{R}^{2H}.
     \qquad(4)]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:msub>
                        <mml:mi>ùê°</mml:mi>
                        <mml:mi>t</mml:mi>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">[</mml:mo>
                        <mml:msub>
                          <mml:mover>
                            <mml:mi>ùê°</mml:mi>
                            <mml:mo accent="true">‚Üí</mml:mo>
                          </mml:mover>
                          <mml:mi>t</mml:mi>
                        </mml:msub>
                        <mml:mo>;</mml:mo>
                        <mml:msub>
                          <mml:mover>
                            <mml:mi>ùê°</mml:mi>
                            <mml:mo accent="true">‚Éñ</mml:mo>
                          </mml:mover>
                          <mml:mi>t</mml:mi>
                        </mml:msub>
                        <mml:mo stretchy="true" form="postfix">]</mml:mo>
                      </mml:mrow>
                      <mml:mo>‚àà</mml:mo>
                      <mml:msup>
                        <mml:mi>‚Ñù</mml:mi>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                          <mml:mi>H</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mi>.</mml:mi>
                      <mml:mspace width="2.0em"/>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>4</mml:mn>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </styled-content>
          </p>
        </sec>
        <sec id="masked-mean-pooling-nb-article">
          <title>Masked mean pooling</title>
          <p>To handle the padding artifacts present in the fixed-length
    sequences, a masked mean pooling operation is applied. Let
    <inline-formula><alternatives><tex-math><![CDATA[\mathbf{m} \in \{0, 1\}^T]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùê¶</mml:mi><mml:mo>‚àà</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false" form="postfix">}</mml:mo><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
    be the attention mask where <inline-formula><alternatives><tex-math><![CDATA[1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula>
    denotes a valid token. The fixed-size sentence representation
    <inline-formula><alternatives><tex-math><![CDATA[\mathbf{z}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê≥</mml:mi></mml:math></alternatives></inline-formula>
    is computed as:</p>
          <p>
            <styled-content id="eq-masked-pool-nb-article">
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
    \mathbf{z} = \frac{\sum_{t=1}^{T} m_t \mathbf{h}_t}{\sum_{t=1}^{T} m_t + \epsilon} \in \mathbb{R}^{2H},
     \qquad(5)]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mi>ùê≥</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:munderover>
                            <mml:mo>‚àë</mml:mo>
                            <mml:mrow>
                              <mml:mi>t</mml:mi>
                              <mml:mo>=</mml:mo>
                              <mml:mn>1</mml:mn>
                            </mml:mrow>
                            <mml:mi>T</mml:mi>
                          </mml:munderover>
                          <mml:msub>
                            <mml:mi>m</mml:mi>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                          <mml:msub>
                            <mml:mi>ùê°</mml:mi>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:munderover>
                            <mml:mo>‚àë</mml:mo>
                            <mml:mrow>
                              <mml:mi>t</mml:mi>
                              <mml:mo>=</mml:mo>
                              <mml:mn>1</mml:mn>
                            </mml:mrow>
                            <mml:mi>T</mml:mi>
                          </mml:munderover>
                          <mml:msub>
                            <mml:mi>m</mml:mi>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                          <mml:mo>+</mml:mo>
                          <mml:mi>œµ</mml:mi>
                        </mml:mrow>
                      </mml:mfrac>
                      <mml:mo>‚àà</mml:mo>
                      <mml:msup>
                        <mml:mi>‚Ñù</mml:mi>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                          <mml:mi>H</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mo>,</mml:mo>
                      <mml:mspace width="2.0em"/>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>5</mml:mn>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </styled-content>
          </p>
          <p>where <inline-formula><alternatives><tex-math><![CDATA[\epsilon = 10^{-6}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>œµ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
    ensures numerical stability. This collapses the temporal dimension,
    resulting in a single vector capturing the global context of the
    post.</p>
        </sec>
        <sec id="classification-head-nb-article">
          <title>Classification head</title>
          <p>The pooled vector <inline-formula><alternatives><tex-math><![CDATA[\mathbf{z}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê≥</mml:mi></mml:math></alternatives></inline-formula>
    serves as the input to the classification head. Regularization is
    applied via Dropout with probability <inline-formula><alternatives><tex-math><![CDATA[p=0.1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>:</p>
          <p>
            <styled-content id="eq-dropout-nb-article">
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
    \tilde{\mathbf{z}} = \text{Dropout}(\mathbf{z}).
     \qquad(6)]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mover>
                        <mml:mi>ùê≥</mml:mi>
                        <mml:mo accent="true">ÃÉ</mml:mo>
                      </mml:mover>
                      <mml:mo>=</mml:mo>
                      <mml:mtext mathvariant="normal">Dropout</mml:mtext>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>ùê≥</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mi>.</mml:mi>
                      <mml:mspace width="2.0em"/>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>6</mml:mn>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </styled-content>
          </p>
          <p>A linear projection layer maps the features to the unnormalized
    logits for the two classes:</p>
          <p>
            <styled-content id="eq-linear-head-nb-article">
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
    \mathbf{o} = W_{out}\tilde{\mathbf{z}} + \mathbf{b}_{out} \in \mathbb{R}^2.
     \qquad(7)]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mi>ùê®</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:msub>
                        <mml:mi>W</mml:mi>
                        <mml:mrow>
                          <mml:mi>o</mml:mi>
                          <mml:mi>u</mml:mi>
                          <mml:mi>t</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mover>
                        <mml:mi>ùê≥</mml:mi>
                        <mml:mo accent="true">ÃÉ</mml:mo>
                      </mml:mover>
                      <mml:mo>+</mml:mo>
                      <mml:msub>
                        <mml:mi>ùêõ</mml:mi>
                        <mml:mrow>
                          <mml:mi>o</mml:mi>
                          <mml:mi>u</mml:mi>
                          <mml:mi>t</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>‚àà</mml:mo>
                      <mml:msup>
                        <mml:mi>‚Ñù</mml:mi>
                        <mml:mn>2</mml:mn>
                      </mml:msup>
                      <mml:mi>.</mml:mi>
                      <mml:mspace width="2.0em"/>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>7</mml:mn>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </styled-content>
          </p>
          <p>The predicted class <inline-formula><alternatives><tex-math><![CDATA[\hat{y}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mover><mml:mi>y</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover></mml:math></alternatives></inline-formula>
    is derived via the argmax of the logits.</p>
        </sec>
        <sec id="training-objective-nb-article">
          <title>Training objective</title>
          <p>The model is trained using the Cross-Entropy loss function. For a
    batch of <inline-formula><alternatives><tex-math><![CDATA[N]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula>
    samples, the objective is to minimize:</p>
          <p>
            <styled-content id="eq-jross-entropy-nb-article">
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
    \mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} \text{CrossEntropy}(\mathbf{o}^{(i)}, y^{(i)}).
     \qquad(8)]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mi>‚Ñí</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mn>1</mml:mn>
                        <mml:mi>N</mml:mi>
                      </mml:mfrac>
                      <mml:munderover>
                        <mml:mo>‚àë</mml:mo>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:munderover>
                      <mml:mtext mathvariant="normal">CrossEntropy</mml:mtext>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:msup>
                          <mml:mi>ùê®</mml:mi>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">(</mml:mo>
                            <mml:mi>i</mml:mi>
                            <mml:mo stretchy="true" form="postfix">)</mml:mo>
                          </mml:mrow>
                        </mml:msup>
                        <mml:mo>,</mml:mo>
                        <mml:msup>
                          <mml:mi>y</mml:mi>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">(</mml:mo>
                            <mml:mi>i</mml:mi>
                            <mml:mo stretchy="true" form="postfix">)</mml:mo>
                          </mml:mrow>
                        </mml:msup>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mi>.</mml:mi>
                      <mml:mspace width="2.0em"/>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>8</mml:mn>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </styled-content>
          </p>
          <p>Optimization is performed using the <bold>Adam</bold> optimizer
    with a learning rate <inline-formula><alternatives><tex-math><![CDATA[\eta = 10^{-3}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>Œ∑</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.
    The training step, including the loss calculation and gradient
    update, is Just-In-Time (JIT) compiled into a comprehensive
    computation graph using <monospace>mx.compile</monospace> to
    maximize execution speed.</p>
        </sec>
        <sec id="computational-considerations-nb-article">
          <title>Computational considerations</title>
          <p>The implementation is optimized for Apple Silicon using the MLX
    framework. Several specific strategies are employed for
    efficiency:</p>
          <list list-type="order">
            <list-item>
              <p><bold>Lazy Evaluation &amp; Materialization:</bold> MLX uses
        lazy evaluation. To prevent the computation graph from growing
        indefinitely during the iterative data loading process,
        <monospace>mx.eval</monospace> is explicitly called on batch
        inputs and loss values to force materialization.</p>
            </list-item>
            <list-item>
              <p><bold>Quantized Feature Extraction:</bold> The embedding
        model (<monospace>all-MiniLM-L6-v2</monospace>) utilizes 4-bit
        quantization, significantly reducing memory bandwidth
        requirements during the feature extraction phase.</p>
            </list-item>
            <list-item>
              <p><bold>Precision:</bold> The embeddings are cast to
        <monospace>float16</monospace>
        (<monospace>mx.float16</monospace>) before entering the LSTM,
        halving the memory footprint of the batch tensor compared to
        <monospace>float32</monospace> and leveraging the hardware‚Äôs
        native half-precision performance.</p>
            </list-item>
          </list>
        </sec>
      </sec>
      <sec id="data-pipeline-nb-article">
        <title>Data Pipeline</title>
        <p>The pipeline proceeds as:</p>
        <list list-type="bullet">
          <list-item>
            <p>Read the cleaned CSV
      (<monospace>congress_complete.csv</monospace>) and keep the
      relevant columns (<monospace>post</monospace> and
      <monospace>party</monospace>).</p>
          </list-item>
          <list-item>
            <p>Drop rows with missing text or party labels.</p>
          </list-item>
          <list-item>
            <p>Normalize party strings and map them to binary labels (Democrat
      = 0, Republican = 1), discarding any other labels.</p>
          </list-item>
          <list-item>
            <p>Perform a stratified train/validation/test split with
      proportions 80/10/10 using a fixed random seed.</p>
          </list-item>
          <list-item>
            <p>During training and evaluation, tokenize <bold>on the
      fly</bold> per batch and pad/truncate to a fixed length
      (T=128).</p>
          </list-item>
          <list-item>
            <p>For each minibatch, pass token IDs and attention masks through
      a <bold>frozen, quantized MiniLM encoder</bold> to obtain
      contextual token embeddings. Gradients are stopped at this stage,
      so only the downstream BiLSTM is trained.</p>
          </list-item>
          <list-item>
            <p>Cast embeddings to <monospace>float16</monospace> to reduce
      memory use; keep attention masks to zero out padding tokens before
      recurrence and for masked mean pooling.</p>
          </list-item>
        </list>
        <p>This design avoids storing large precomputed embedding tensors in
  RAM. Instead, embeddings are computed batch-wise and materialized with
  <monospace>mx.eval</monospace> so MLX graphs do not accumulate across
  iterations.</p>
        <sec id="training-loop-nb-article">
          <title>Training Loop</title>
          <p>The training routine runs for 10 epochs with a fixed batch size
    of 64. In each epoch:</p>
          <list list-type="bullet">
            <list-item>
              <p>Set the model to training mode.</p>
            </list-item>
            <list-item>
              <p>Iterate over batches produced by
        <monospace>batch_iterate_text(..., drop_last=True)</monospace>
        so every training step has identical shape (required for
        <monospace>mx.compile</monospace>).</p>
            </list-item>
            <list-item>
              <p>For each batch:</p>
              <list list-type="bullet">
                <list-item>
                  <p>Compute logits with the BiLSTM classifier.</p>
                </list-item>
                <list-item>
                  <p>Compute mean cross-entropy loss.</p>
                </list-item>
                <list-item>
                  <p>Backpropagate to obtain gradients for the BiLSTM
            parameters.</p>
                </list-item>
                <list-item>
                  <p>Update parameters using the <bold>Adam</bold> optimizer
            with learning rate (10^{-3}).</p>
                </list-item>
              </list>
            </list-item>
            <list-item>
              <p>After each epoch, switch to evaluation mode and compute
        validation loss and accuracy over the full validation split
        (<monospace>drop_last=False</monospace>).</p>
            </list-item>
          </list>
          <p>The training step is JIT-compiled via
    <monospace>mx.compile</monospace> for speed. There is no
    checkpointing or best-model reload in the current code; the reported
    test results come from the final epoch‚Äôs parameters.</p>
        </sec>
        <sec id="implementation-in-code-nb-article">
          <title>Implementation in Code</title>
          <p>The biLStM model has been in apples mlx framework to enable a
    bare metal run of the model on apple silicon hardware. The
    implementation has ben performd on top of the mlx framework provided
    <monospace>nn.Module</monospace> function.</p>
          <p>for that a LSTM cell has been set up as follows</p>
          <code language="markdown">```{python}
class LSTMCell(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.xh_to_gates = nn.Linear(input_size + hidden_size, 4 * hidden_size)

    def __call__(self, x_t, state):
        h_prev, c_prev = state
        xh = mx.concatenate([x_t, h_prev], axis=-1)
        gates = self.xh_to_gates(xh)
        i, f, o, g = mx.split(gates, 4, axis=-1)

        i = mx.sigmoid(i)
        f = mx.sigmoid(f)
        o = mx.sigmoid(o)
        g = mx.tanh(g)

        c_t = f * c_prev + i * g
        h_t = o * mx.tanh(c_t)
        return h_t, c_t
```</code>
          <p>Used in a single lstm model as follows:</p>
          <code language="markdown">```{python}
class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.cell = LSTMCell(input_size, hidden_size)

    def __call__(self, x):
        B, T, _ = x.shape
        h = mx.zeros((B, self.hidden_size), dtype=mx.float32)
        c = mx.zeros((B, self.hidden_size), dtype=mx.float32)

        outputs = []
        for t in range(T):
            h, c = self.cell(x[:, t, :], (h, c))
            outputs.append(h)

        return mx.stack(outputs, axis=1), (h, c)

```</code>
          <p>And finally the bidirectional lstm is set up along the text
    classification:</p>
          <code language="markdown">```{python}

class BiLSTM(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.fwd = LSTM(input_size, hidden_size)
        self.bwd = LSTM(input_size, hidden_size)

    def __call__(self, x):
        B, T, _ = x.shape

        fwd_out, _ = self.fwd(x)

        rev_idx = mx.arange(T - 1, -1, -1, dtype=mx.int32)
        x_rev = mx.take(x, rev_idx, axis=1)

        bwd_out_rev, _ = self.bwd(x_rev)
        bwd_out = mx.take(bwd_out_rev, rev_idx, axis=1)

        return mx.concatenate([fwd_out, bwd_out], axis=-1)  # (B, T, 2H)


class BiLSTMTextClassifier(nn.Module):
    def __init__(self, embedding_dim, hidden_size, num_classes, dropout=0.1):
        super().__init__()
        self.bilstm = BiLSTM(embedding_dim, hidden_size)
        self.dropout = nn.Dropout(dropout)
        self.head = nn.Linear(2 * hidden_size, num_classes)

    def pool(self, seq_out, mask):
        mask_f = mask.astype(mx.float32)[..., None]
        summed = mx.sum(seq_out * mask_f, axis=1)
        denom = mx.maximum(mx.sum(mask_f, axis=1), 1e-6)
        return summed / denom

    def __call__(self, x, mask):
        # zero out padding before recurrence
        x = x * mask.astype(mx.float32)[..., None]
        seq_out = self.bilstm(x)
        pooled = self.pool(seq_out, mask)
        pooled = self.dropout(pooled)
        return self.head(pooled)


```</code>
          <p>To use the framework to it‚Äôs full poterntial the model its is
    compiled</p>
          <code language="markdown">```{python}
@partial(mx.compile, inputs=state, outputs=state)
def step(Xb, yb, Mb):
    loss, grads = loss_and_grad(model, Xb, yb, Mb)
    optimizer.update(model, grads)
    return loss

```</code>
          <p>The description of the loss function is omitted but can be found
    in the complete code in the repository</p>
        </sec>
      </sec>
      <sec id="sec-nbb-nb-article">
        <title>Naive Bayes Baseline</title>
        <p>As a classical probabilistic benchmark, a Naive Bayes classifier is
  employed alongside the neural sequence model. To ensure comparability,
  all preprocessing steps prior to feature extraction‚Äîdata cleaning,
  label normalization, and the exact train/validation/test split‚Äîare
  identical to those used for the BiLSTM classifier.</p>
        <sec id="model-description-nb-article">
          <title>Model description</title>
          <p>In contrast to the sequence-based neural architecture, the Naive
    Bayes model operates on a sparse bag-of-ngrams representation of the
    text. Each document <inline-formula><alternatives><tex-math><![CDATA[x]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
    is mapped to a TF‚ÄìIDF feature vector
    <styled-content id="eq-eq-tfidf-nb-article"><disp-formula><alternatives><tex-math><![CDATA[
    \mathbf{f}(x) = (f_1, f_2, \ldots, f_d),
     \qquad(9)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mrow><mml:mi>ùêü</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>‚Ä¶</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="2.0em"/><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>9</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></styled-content></p>
          <p>where each <inline-formula><alternatives><tex-math><![CDATA[f_j]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
    reflects the TF‚ÄìIDF weight of term <inline-formula><alternatives><tex-math><![CDATA[j]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>j</mml:mi></mml:math></alternatives></inline-formula>
    after vocabulary truncation and filtering. The vectorizer uses
    standard preprocessing settings for text classification:</p>
          <list list-type="bullet">
            <list-item>
              <p>lowercasing</p>
            </list-item>
            <list-item>
              <p>English stop-word removal</p>
            </list-item>
            <list-item>
              <p>unigram and bigram features <inline-formula><alternatives><tex-math><![CDATA[(1,2)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula></p>
            </list-item>
            <list-item>
              <p>a maximum feature cap of <inline-formula><alternatives><tex-math><![CDATA[d = 50{,}000]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></p>
            </list-item>
            <list-item>
              <p>a minimum document frequency of
        <inline-formula><alternatives><tex-math><![CDATA[2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>2</mml:mn></mml:math></alternatives></inline-formula></p>
            </list-item>
          </list>
          <p>The classifier itself is a Multinomial Naive Bayes model, which
    assumes conditional independence of features given a class label
    <inline-formula><alternatives><tex-math><![CDATA[y\in{0,1}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>y</mml:mi><mml:mo>‚àà</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.
    Under this assumption, the likelihood of observing the feature
    vector <inline-formula><alternatives><tex-math><![CDATA[\mathbf{f}(x)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêü</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
    given class <inline-formula><alternatives><tex-math><![CDATA[k]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>k</mml:mi></mml:math></alternatives></inline-formula>
    factorizes as</p>
          <p>
            <styled-content id="eq-nb-likelihood-nb-article">
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
    p(\mathbf{f}(x)\mid y=k)
    ;\propto;
    \prod_{j=1}^{d}
    \theta_{kj}^{, f_j(x)},
     \qquad(10)]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mi>p</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>ùêü</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mi>x</mml:mi>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                        <mml:mo>‚à£</mml:mo>
                        <mml:mi>y</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mi>k</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>;</mml:mo>
                      <mml:mo>‚àù</mml:mo>
                      <mml:mo>;</mml:mo>
                      <mml:munderover>
                        <mml:mo>‚àè</mml:mo>
                        <mml:mrow>
                          <mml:mi>j</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mi>d</mml:mi>
                      </mml:munderover>
                      <mml:msubsup>
                        <mml:mi>Œ∏</mml:mi>
                        <mml:mrow>
                          <mml:mi>k</mml:mi>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>,</mml:mo>
                          <mml:msub>
                            <mml:mi>f</mml:mi>
                            <mml:mi>j</mml:mi>
                          </mml:msub>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">(</mml:mo>
                            <mml:mi>x</mml:mi>
                            <mml:mo stretchy="true" form="postfix">)</mml:mo>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:mo>,</mml:mo>
                      <mml:mspace width="2.0em"/>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>10</mml:mn>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </styled-content>
          </p>
          <p>where <inline-formula><alternatives><tex-math><![CDATA[\theta_{kj}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>Œ∏</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
    are class-conditional feature probabilities estimated from the
    training set. Bayes‚Äô rule then yields the posterior</p>
          <p>
            <styled-content id="eq-nb-posterior-nb-article">
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
    p(y=k \mid x)
    ;\propto;
    p(y=k)\prod_{j=1}^{d}\theta_{kj}^{, f_j(x)},
     \qquad(11)]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mi>p</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>y</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mi>k</mml:mi>
                        <mml:mo>‚à£</mml:mo>
                        <mml:mi>x</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>;</mml:mo>
                      <mml:mo>‚àù</mml:mo>
                      <mml:mo>;</mml:mo>
                      <mml:mi>p</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>y</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mi>k</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:munderover>
                        <mml:mo>‚àè</mml:mo>
                        <mml:mrow>
                          <mml:mi>j</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mi>d</mml:mi>
                      </mml:munderover>
                      <mml:msubsup>
                        <mml:mi>Œ∏</mml:mi>
                        <mml:mrow>
                          <mml:mi>k</mml:mi>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>,</mml:mo>
                          <mml:msub>
                            <mml:mi>f</mml:mi>
                            <mml:mi>j</mml:mi>
                          </mml:msub>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">(</mml:mo>
                            <mml:mi>x</mml:mi>
                            <mml:mo stretchy="true" form="postfix">)</mml:mo>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:mo>,</mml:mo>
                      <mml:mspace width="2.0em"/>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>11</mml:mn>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </styled-content>
          </p>
          <p>and prediction is made via</p>
          <p>
            <styled-content id="eq-nb-prediction-nb-article">
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
    \hat{y} = \arg\max_{k\in{0,1}} p(y=k\mid x).
     \qquad(12)]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mover>
                        <mml:mi>y</mml:mi>
                        <mml:mo accent="true">ÃÇ</mml:mo>
                      </mml:mover>
                      <mml:mo>=</mml:mo>
                      <mml:mo>arg</mml:mo>
                      <mml:munder>
                        <mml:mo>max</mml:mo>
                        <mml:mrow>
                          <mml:mi>k</mml:mi>
                          <mml:mo>‚àà</mml:mo>
                          <mml:mrow>
                            <mml:mn>0</mml:mn>
                            <mml:mo>,</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:munder>
                      <mml:mi>p</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>y</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mi>k</mml:mi>
                        <mml:mo>‚à£</mml:mo>
                        <mml:mi>x</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mi>.</mml:mi>
                      <mml:mspace width="2.0em"/>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>12</mml:mn>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </styled-content>
          </p>
          <p>Although the multinomial model is derived for count data, it is
    widely used with TF‚ÄìIDF features and remains a strong linear
    baseline in high-dimensional text classification.</p>
        </sec>
      </sec>
      <sec id="empirical-assessments-nb-article">
        <title>Empirical Assessments</title>
        <p>This section reports the empirical performance of the two main
  classifiers: the BiLSTM sequence model and the Naive Bayes
  baseline.</p>
        <sec id="bilstm-nb-article">
          <title>BiLSTM</title>
          <sec id="headline-results-nb-article">
            <title>Headline results</title>
            <p>On the held-out test partition, the BiLSTM achieves:</p>
            <p><disp-formula><alternatives><tex-math><![CDATA[
      \text{Test loss} = 0.5229,\qquad
      \text{Test accuracy} = 0.7952.
      ]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mrow><mml:mtext mathvariant="normal">Test loss</mml:mtext><mml:mo>=</mml:mo><mml:mn>0.5229</mml:mn><mml:mo>,</mml:mo><mml:mspace width="2.0em"/><mml:mtext mathvariant="normal">Test accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mn>0.7952</mml:mn><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>{#ewq-bilstm-acc}</p>
            <p>Validation- and test-set performance are closely aligned,
      suggesting stable generalization without pronounced
      overfitting.</p>
          </sec>
          <sec id="class-wise-metrics-nb-article">
            <title>Class-wise metrics</title>
            <sec specific-use="notebook-content">
              <code language="r script">library(tibble)

clsrep &lt;- tribble(
  ~Class,
  ~Precision,
  ~Recall,
  ~F1,
  ~Support,
  "0",
  0.7621,
  0.6748,
  0.7158,
  12729,
  "1",
  0.8121,
  0.8697,
  0.8399,
  20573,
  "Accuracy",
  NA,
  NA,
  0.7952,
  33302,
  "Macro avg",
  0.7871,
  0.7723,
  0.7779,
  33302,
  "Weighted avg",
  0.7930,
  0.7952,
  0.7925,
  33302
)

library(knitr)
kable(clsrep, caption = "BiLSTM: test-set classification report.")</code>
              <table-wrap>
                <caption>
                  <p>BiLSTM: test-set classification report.</p>
                </caption>
                <table>
                  <thead>
                    <tr>
                      <th align="left">Class</th>
                      <th align="right">Precision</th>
                      <th align="right">Recall</th>
                      <th align="right">F1</th>
                      <th align="right">Support</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td align="left">0</td>
                      <td align="right">0.7621</td>
                      <td align="right">0.6748</td>
                      <td align="right">0.7158</td>
                      <td align="right">12729</td>
                    </tr>
                    <tr>
                      <td align="left">1</td>
                      <td align="right">0.8121</td>
                      <td align="right">0.8697</td>
                      <td align="right">0.8399</td>
                      <td align="right">20573</td>
                    </tr>
                    <tr>
                      <td align="left">Accuracy</td>
                      <td align="right">NA</td>
                      <td align="right">NA</td>
                      <td align="right">0.7952</td>
                      <td align="right">33302</td>
                    </tr>
                    <tr>
                      <td align="left">Macro avg</td>
                      <td align="right">0.7871</td>
                      <td align="right">0.7723</td>
                      <td align="right">0.7779</td>
                      <td align="right">33302</td>
                    </tr>
                    <tr>
                      <td align="left">Weighted avg</td>
                      <td align="right">0.7930</td>
                      <td align="right">0.7952</td>
                      <td align="right">0.7925</td>
                      <td align="right">33302</td>
                    </tr>
                  </tbody>
                </table>
              </table-wrap>
              <code language="r script">test_acc_lstm &lt;- 0.7952074950453426
test_loss_lstm &lt;- 0.5228507898691046</code>
            </sec>
            <p>Performance is systematically stronger for class 1, which is
      also the majority class in the data and therefore this behaviour
      is somewhat expected. The macro-averaged
      <inline-formula><alternatives><tex-math><![CDATA[F_1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
      is slightly lower than the weighted average, reflecting the impact
      of class imbalance on aggregate metrics.</p>
          </sec>
          <sec id="confusion-matrix-and-derived-rates-nb-article">
            <title>Confusion matrix and derived rates</title>
            <p>The test-set confusion matrix for the BiLSTM is:</p>
            <p>
              <styled-content id="eq-bilstm-conf-mat-nb-article">
                <disp-formula>
                  <alternatives>
                    <tex-math><![CDATA[
      \mathbf{C} =
      \begin{pmatrix}
      8590 & 4139 \\
      2681 & 17892
      \end{pmatrix}.
       \qquad(13)]]></tex-math>
                    <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                      <mml:mrow>
                        <mml:mi>ùêÇ</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mtable>
                            <mml:mtr>
                              <mml:mtd columnalign="center" style="text-align: center">
                                <mml:mn>8590</mml:mn>
                              </mml:mtd>
                              <mml:mtd columnalign="center" style="text-align: center">
                                <mml:mn>4139</mml:mn>
                              </mml:mtd>
                            </mml:mtr>
                            <mml:mtr>
                              <mml:mtd columnalign="center" style="text-align: center">
                                <mml:mn>2681</mml:mn>
                              </mml:mtd>
                              <mml:mtd columnalign="center" style="text-align: center">
                                <mml:mn>17892</mml:mn>
                              </mml:mtd>
                            </mml:mtr>
                          </mml:mtable>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                        <mml:mi>.</mml:mi>
                        <mml:mspace width="2.0em"/>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mn>13</mml:mn>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:math>
                  </alternatives>
                </disp-formula>
              </styled-content>
            </p>
            <p>Treating class 1 as the positive class, the following
      quantities are obtained:</p>
            <p>
              <styled-content id="eq-bilstm-derived-metrics-nb-article">
                <disp-formula>
                  <alternatives>
                    <tex-math><![CDATA[
      \begin{aligned}
      \text{TPR (recall$_1$)} &= \frac{17892}{17892+2681}=0.8697, \\
      \text{TNR (specificity)} &= \frac{8590}{8590+4139}=0.6748, \\
      \text{FPR} &= \frac{4139}{8590+4139}=0.3252, \\
      \text{FNR} &= \frac{2681}{17892+2681}=0.1303, \\
      \text{Balanced accuracy} &= \frac{0.8697+0.6748}{2}=0.7723, \\
      \text{MCC} &= 0.5592.
      \end{aligned}
       \qquad(14)]]></tex-math>
                    <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                      <mml:mrow>
                        <mml:mtable>
                          <mml:mtr>
                            <mml:mtd columnalign="right" style="text-align: right">
                              <mml:mtext mathvariant="normal">TPR (recall</mml:mtext>
                              <mml:msub>
                                <mml:mi/>
                                <mml:mn>1</mml:mn>
                              </mml:msub>
                              <mml:mtext mathvariant="normal">)</mml:mtext>
                            </mml:mtd>
                            <mml:mtd columnalign="left" style="text-align: left">
                              <mml:mo>=</mml:mo>
                              <mml:mfrac>
                                <mml:mn>17892</mml:mn>
                                <mml:mrow>
                                  <mml:mn>17892</mml:mn>
                                  <mml:mo>+</mml:mo>
                                  <mml:mn>2681</mml:mn>
                                </mml:mrow>
                              </mml:mfrac>
                              <mml:mo>=</mml:mo>
                              <mml:mn>0.8697</mml:mn>
                              <mml:mo>,</mml:mo>
                            </mml:mtd>
                          </mml:mtr>
                          <mml:mtr>
                            <mml:mtd columnalign="right" style="text-align: right">
                              <mml:mtext mathvariant="normal">TNR (specificity)</mml:mtext>
                            </mml:mtd>
                            <mml:mtd columnalign="left" style="text-align: left">
                              <mml:mo>=</mml:mo>
                              <mml:mfrac>
                                <mml:mn>8590</mml:mn>
                                <mml:mrow>
                                  <mml:mn>8590</mml:mn>
                                  <mml:mo>+</mml:mo>
                                  <mml:mn>4139</mml:mn>
                                </mml:mrow>
                              </mml:mfrac>
                              <mml:mo>=</mml:mo>
                              <mml:mn>0.6748</mml:mn>
                              <mml:mo>,</mml:mo>
                            </mml:mtd>
                          </mml:mtr>
                          <mml:mtr>
                            <mml:mtd columnalign="right" style="text-align: right">
                              <mml:mtext mathvariant="normal">FPR</mml:mtext>
                            </mml:mtd>
                            <mml:mtd columnalign="left" style="text-align: left">
                              <mml:mo>=</mml:mo>
                              <mml:mfrac>
                                <mml:mn>4139</mml:mn>
                                <mml:mrow>
                                  <mml:mn>8590</mml:mn>
                                  <mml:mo>+</mml:mo>
                                  <mml:mn>4139</mml:mn>
                                </mml:mrow>
                              </mml:mfrac>
                              <mml:mo>=</mml:mo>
                              <mml:mn>0.3252</mml:mn>
                              <mml:mo>,</mml:mo>
                            </mml:mtd>
                          </mml:mtr>
                          <mml:mtr>
                            <mml:mtd columnalign="right" style="text-align: right">
                              <mml:mtext mathvariant="normal">FNR</mml:mtext>
                            </mml:mtd>
                            <mml:mtd columnalign="left" style="text-align: left">
                              <mml:mo>=</mml:mo>
                              <mml:mfrac>
                                <mml:mn>2681</mml:mn>
                                <mml:mrow>
                                  <mml:mn>17892</mml:mn>
                                  <mml:mo>+</mml:mo>
                                  <mml:mn>2681</mml:mn>
                                </mml:mrow>
                              </mml:mfrac>
                              <mml:mo>=</mml:mo>
                              <mml:mn>0.1303</mml:mn>
                              <mml:mo>,</mml:mo>
                            </mml:mtd>
                          </mml:mtr>
                          <mml:mtr>
                            <mml:mtd columnalign="right" style="text-align: right">
                              <mml:mtext mathvariant="normal">Balanced accuracy</mml:mtext>
                            </mml:mtd>
                            <mml:mtd columnalign="left" style="text-align: left">
                              <mml:mo>=</mml:mo>
                              <mml:mfrac>
                                <mml:mrow>
                                  <mml:mn>0.8697</mml:mn>
                                  <mml:mo>+</mml:mo>
                                  <mml:mn>0.6748</mml:mn>
                                </mml:mrow>
                                <mml:mn>2</mml:mn>
                              </mml:mfrac>
                              <mml:mo>=</mml:mo>
                              <mml:mn>0.7723</mml:mn>
                              <mml:mo>,</mml:mo>
                            </mml:mtd>
                          </mml:mtr>
                          <mml:mtr>
                            <mml:mtd columnalign="right" style="text-align: right">
                              <mml:mtext mathvariant="normal">MCC</mml:mtext>
                            </mml:mtd>
                            <mml:mtd columnalign="left" style="text-align: left">
                              <mml:mo>=</mml:mo>
                              <mml:mn>0.5592</mml:mn>
                              <mml:mi>.</mml:mi>
                            </mml:mtd>
                          </mml:mtr>
                        </mml:mtable>
                        <mml:mspace width="2.0em"/>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mn>14</mml:mn>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:math>
                  </alternatives>
                </disp-formula>
              </styled-content>
            </p>
          </sec>
          <sec id="error-profile-nb-article">
            <title>Error profile</title>
            <p>The confusion matrix indicates two main error patterns:</p>
            <list list-type="bullet">
              <list-item>
                <p><bold>Class 0 ‚Üí Class 1</bold>: A comparatively large
          number of false positives (items from class 0 predicted as
          class 1) likely arises from shared lexical or stylistic
          features between the two parties, amplified by class imbalance
          which then leads to a bigger set of possible patterns are
          beeing labeled as class 1.</p>
              </list-item>
              <list-item>
                <p><bold>Class 1 ‚Üí Class 0</bold>: Fewer false negatives for
          class 1, which may correspond to more moderate or
          cross-partisan language that is less prototypical of the
          majority class.</p>
              </list-item>
            </list>
            <p>The false-positive rate is roughly twice the false-negative
      rate, implying that the classifier tends to err toward predicting
      class 1. In applications with asymmetric error costs, this
      tendency could be counteracted by threshold adjustment,
      class-weighted loss functions, or post-hoc calibration.</p>
            <p>The distribution of posts lengths does not have an impact on
      the error profile as throughout the dataset the post length
      distribution is similar across classes as shown in
      <xref alt="Section¬†3" rid="sec-eda-nb-article">Section¬†3</xref> .</p>
          </sec>
        </sec>
        <sec id="qualitative-analysis-nb-article">
          <title>Qualitative Analysis</title>
          <p>To show the qualitative implications of the model a sample of
    posts is classified here.</p>
          <sec id="single-words-nb-article">
            <title>Single words</title>
            <p>The model is given a list of the most comon english words and
      their predicted probabilities for each class. The results are
      visualized in
      <xref alt="Figure¬†2" rid="fig-word-bias-nb-article">Figure¬†2</xref>, which
      plots the margin (p_dem ‚àí p_repub) for each word against its rank
      in the sorted list. Points are colored by margin and sized by
      confidence (maximum predicted probability). The twenty most
      extreme words on either end of the spectrum are labeled.</p>
            <sec id="cell-fig-word-bias-nb-article" specific-use="notebook-content">
              <code language="r script">library(readr)
library(dplyr)
library(ggplot2)
library(ggrepel)

# read predictions
df &lt;- read_csv("eval/words.csv", show_col_types = FALSE) %&gt;%
  mutate(
    p_dem = as.numeric(p_dem),
    p_repup = as.numeric(p_repub),
    margin = p_dem - p_repub,
    confidence = pmax(p_dem, p_repub)
  ) %&gt;%
  arrange(margin) %&gt;%
  mutate(rank = row_number())

# label the extreme words
k &lt;- 20
to_label &lt;- bind_rows(
  df %&gt;% slice_head(n = k),
  df %&gt;% slice_tail(n = k)
)

ggplot(df, aes(x = rank, y = margin)) +
  geom_hline(yintercept = 0, linetype = "dashed", linewidth = 0.4) +
  geom_point(aes(size = confidence, color = margin), alpha = 0.8) +
  scale_color_gradient2(
    low = "#b2182b",
    mid = "grey70",
    high = "#2166ac",
    midpoint = 0,
    name = "p_dem - p_repub"
  ) +
  scale_size_continuous(range = c(1.2, 4), name = "confidence") +
  geom_text_repel(
    data = to_label,
    aes(label = word),
    size = 3,
    max.overlaps = Inf,
    box.padding = 0.25,
    point.padding = 0.2
  ) +
  labs(
    title = "Model bias on common words",
    subtitle = "Each point is a word; margin = p_dem ‚àí p_repub. Extremes labeled.",
    x = "Words sorted by margin (Rep-leaning ‚Üí Dem-leaning)",
    y = "Margin (p_dem ‚àí p_repub)"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "right",
    plot.title = element_text(face = "bold")
  )</code>
              <fig id="fig-word-bias-nb-article">
                <caption>
                  <p>Figure¬†2: Model bias on common words</p>
                </caption>
                <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-word-bias-1.png"/>
              </fig>
            </sec>
            <p>As Figure
      <xref alt="Figure¬†2" rid="fig-word-bias-nb-article">Figure¬†2</xref>
      indicates, the classifier shows pronounced partisan leanings even
      when evaluated on isolated tokens.</p>
            <p>A natural next step is to test whether this behavior is stable
      over time. In particular, one could examine whether applying the
      model to contemporary Republican and Democratic posts changes its
      performance, given well-documented shifts in political
      communication and issue framing over recent years.</p>
          </sec>
        </sec>
        <sec id="naive-bayes-baseline-nb-article">
          <title>Naive Bayes Baseline</title>
          <sec id="headline-results-1-nb-article">
            <title>Headline results</title>
            <p>As a bag-of-words baseline, the Naive Bayes classifier attains
      the following accuracy on the test set:</p>
            <p>
              <styled-content id="eq-nb-acc-nb-article">
                <disp-formula>
                  <alternatives>
                    <tex-math><![CDATA[
      \text{Test accuracy} = 0.7843
       \qquad(15)]]></tex-math>
                    <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                      <mml:mrow>
                        <mml:mtext mathvariant="normal">Test accuracy</mml:mtext>
                        <mml:mo>=</mml:mo>
                        <mml:mn>0.7843</mml:mn>
                        <mml:mspace width="2.0em"/>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mn>15</mml:mn>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:math>
                  </alternatives>
                </disp-formula>
              </styled-content>
            </p>
            <p>Given its simplicity and lack of contextual or sequential
      modeling, this level of performance is comparatively strong, but
      still below that of the BiLSTM.</p>
          </sec>
          <sec id="class-wise-metrics-1-nb-article">
            <title>Class-wise metrics</title>
            <sec specific-use="notebook-content">
              <code language="r script">library(tibble)

nb &lt;- tribble(
  ~Class,
  ~Precision,
  ~Recall,
  ~F1,
  ~Support,
  "0",
  0.7698,
  0.6214,
  0.6877,
  12729,
  "1",
  0.7907,
  0.8850,
  0.8352,
  20573,
  "Accuracy",
  NA,
  NA,
  0.7843,
  33302,
  "Macro avg",
  0.7803,
  0.7532,
  0.7615,
  33302,
  "Weighted avg",
  0.7827,
  0.7843,
  0.7788,
  33302
)

library(knitr)
kable(nb, caption = "Naive Bayes: test-set classification report.")</code>
              <table-wrap>
                <caption>
                  <p>Naive Bayes: test-set classification report.</p>
                </caption>
                <table>
                  <thead>
                    <tr>
                      <th align="left">Class</th>
                      <th align="right">Precision</th>
                      <th align="right">Recall</th>
                      <th align="right">F1</th>
                      <th align="right">Support</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td align="left">0</td>
                      <td align="right">0.7698</td>
                      <td align="right">0.6214</td>
                      <td align="right">0.6877</td>
                      <td align="right">12729</td>
                    </tr>
                    <tr>
                      <td align="left">1</td>
                      <td align="right">0.7907</td>
                      <td align="right">0.8850</td>
                      <td align="right">0.8352</td>
                      <td align="right">20573</td>
                    </tr>
                    <tr>
                      <td align="left">Accuracy</td>
                      <td align="right">NA</td>
                      <td align="right">NA</td>
                      <td align="right">0.7843</td>
                      <td align="right">33302</td>
                    </tr>
                    <tr>
                      <td align="left">Macro avg</td>
                      <td align="right">0.7803</td>
                      <td align="right">0.7532</td>
                      <td align="right">0.7615</td>
                      <td align="right">33302</td>
                    </tr>
                    <tr>
                      <td align="left">Weighted avg</td>
                      <td align="right">0.7827</td>
                      <td align="right">0.7843</td>
                      <td align="right">0.7788</td>
                      <td align="right">33302</td>
                    </tr>
                  </tbody>
                </table>
              </table-wrap>
            </sec>
            <p>As with the BiLSTM, performance is higher for class 1 than for
      class 0. Precision and recall for class 1 are clearly stronger,
      indicating a tendency to assign documents to the majority class
      and to capture its lexical cues more reliably.</p>
          </sec>
          <sec id="confusion-matrix-and-error-profile-nb-article">
            <title>Confusion matrix and error profile</title>
            <p>The corresponding test-set confusion matrix for the Naive Bayes
      classifier is</p>
            <p>
              <styled-content id="eq-nb-conf-mat-nb-article">
                <disp-formula>
                  <alternatives>
                    <tex-math><![CDATA[
      \begin{pmatrix}
      9599 & 3171 \\
      2025 & 18666
      \end{pmatrix}.
       \qquad(16)]]></tex-math>
                    <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mtable>
                            <mml:mtr>
                              <mml:mtd columnalign="center" style="text-align: center">
                                <mml:mn>9599</mml:mn>
                              </mml:mtd>
                              <mml:mtd columnalign="center" style="text-align: center">
                                <mml:mn>3171</mml:mn>
                              </mml:mtd>
                            </mml:mtr>
                            <mml:mtr>
                              <mml:mtd columnalign="center" style="text-align: center">
                                <mml:mn>2025</mml:mn>
                              </mml:mtd>
                              <mml:mtd columnalign="center" style="text-align: center">
                                <mml:mn>18666</mml:mn>
                              </mml:mtd>
                            </mml:mtr>
                          </mml:mtable>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                        <mml:mi>.</mml:mi>
                        <mml:mspace width="2.0em"/>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mn>16</mml:mn>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:math>
                  </alternatives>
                </disp-formula>
              </styled-content>
            </p>
            <p>False positives for class 1 (3171 cases) are moderately more
      frequent than in the BiLSTM. Overall, the baseline captures strong
      lexical signals at low computational cost but offers less balanced
      performance across classes than the sequence model.</p>
          </sec>
        </sec>
        <sec id="comparative-perspective-nb-article">
          <title>Comparative perspective</title>
          <p>To assess whether the BiLSTM improves upon the Naive Bayes
    baseline in a systematic way, a side-by-side comparison of key
    metrics is provided in
    <xref alt="Table¬†1" rid="tbl-comp-nb-article">Table¬†1</xref>. The BiLSTM offers
    modest gains in overall accuracy and more favorable error balance
    across classes, while the Naive Bayes classifier remains a
    competitive and interpretable lexical baseline.</p>
          <sec specific-use="notebook-content">
            <code language="r script">library(tibble)
library(dplyr)
library(gt)

# 1. Setup Data -----------------------------------------------------------
# (Same data setup as before)

nb %&gt;%
  mutate(Class = clsrep$Class) %&gt;%
  invisible()

# Merge data wide
df_wide &lt;- clsrep %&gt;%
  rename_with(~ paste0(., "_BiLSTM"), -Class) %&gt;%
  left_join(nb %&gt;% rename_with(~ paste0(., "_NB"), -Class), by = "Class")

# 2. The GT Table ---------------------------------------------------------

df_wide %&gt;%
  gt() %&gt;%
  # --- 1. Structure &amp; Labels ---
  tab_spanner(label = "BiLSTM", columns = ends_with("_BiLSTM")) %&gt;%
  tab_spanner(label = "Naive Bayes", columns = ends_with("_NB")) %&gt;%
  cols_label(
    Precision_BiLSTM = "Precision",
    Recall_BiLSTM = "Recall",
    F1_BiLSTM = "F1",
    Support_BiLSTM = "Support",
    Precision_NB = "Precision",
    Recall_NB = "Recall",
    F1_NB = "F1",
    Support_NB = "Support"
  ) %&gt;%
  sub_missing(missing_text = "-") %&gt;%

  # --- 2. Visual Separation ---
  # Add a thick border to the right of the BiLSTM section to separate the models
  tab_style(
    style = cell_borders(sides = "right", color = "#d3d3d3", weight = px(2)),
    locations = cells_body(columns = "Support_BiLSTM")
  ) %&gt;%

  # --- 3. Highlight Logic (The Advantage Check) ---

  # &gt;&gt;&gt; PRECISION COMPARISON &lt;&lt;&lt;
  # Highlight BiLSTM if it is higher
  tab_style(
    style = cell_fill(color = "#d4f0d4"),
    locations = cells_body(
      columns = "Precision_BiLSTM",
      rows = Precision_BiLSTM &gt; Precision_NB
    )
  ) %&gt;%
  # Highlight NB if it is higher
  tab_style(
    style = cell_fill(color = "#d4f0d4"),
    locations = cells_body(
      columns = "Precision_NB",
      rows = Precision_NB &gt; Precision_BiLSTM
    )
  ) %&gt;%

  # &gt;&gt;&gt; RECALL COMPARISON &lt;&lt;&lt;
  tab_style(
    style = cell_fill(color = "#d4f0d4"),
    locations = cells_body(
      columns = "Recall_BiLSTM",
      rows = Recall_BiLSTM &gt; Recall_NB
    )
  ) %&gt;%
  tab_style(
    style = cell_fill(color = "#d4f0d4"),
    locations = cells_body(
      columns = "Recall_NB",
      rows = Recall_NB &gt; Recall_BiLSTM
    )
  ) %&gt;%

  # &gt;&gt;&gt; F1 COMPARISON &lt;&lt;&lt;
  tab_style(
    style = cell_fill(color = "#d4f0d4"),
    locations = cells_body(
      columns = "F1_BiLSTM",
      rows = F1_BiLSTM &gt; F1_NB
    )
  ) %&gt;%
  tab_style(
    style = cell_fill(color = "#d4f0d4"),
    locations = cells_body(
      columns = "F1_NB",
      rows = F1_NB &gt; F1_BiLSTM
    )
  ) %&gt;%

  # Add Headers
  tab_header(
    title = "Model Comparison",
    subtitle = "Cells highlighted indicate the winning model for that specific metric"
  )</code>
            <fig id="tbl-comp-nb-article">
              <caption>
                <p>Table¬†1: Model Comparison</p>
              </caption>
              <table-wrap>
                <table>
                  <colgroup>
                    <col width="11%"/>
                    <col width="11%"/>
                    <col width="11%"/>
                    <col width="11%"/>
                    <col width="11%"/>
                    <col width="11%"/>
                    <col width="11%"/>
                    <col width="11%"/>
                    <col width="11%"/>
                  </colgroup>
                  <thead>
                    <tr>
                      <th colspan="9">Model Comparison</th>
                    </tr>
                    <tr>
                      <th colspan="9">Cells highlighted indicate the winning
              model for that specific metric</th>
                    </tr>
                    <tr>
                      <th rowspan="2" id="Class-nb-article" scope="col">Class</th>
                      <th colspan="4" id="BiLSTM-nb-article" scope="colgroup">BiLSTM</th>
                      <th colspan="4" id="NaiveU0020Bayes-nb-article" scope="colgroup">Naive
              Bayes</th>
                    </tr>
                    <tr>
                      <th id="Precision_BiLSTM-nb-article" scope="col">Precision</th>
                      <th id="Recall_BiLSTM-nb-article" scope="col">Recall</th>
                      <th id="F1_BiLSTM-nb-article" scope="col">F1</th>
                      <th id="Support_BiLSTM-nb-article" scope="col">Support</th>
                      <th id="Precision_NB-nb-article" scope="col">Precision</th>
                      <th id="Recall_NB-nb-article" scope="col">Recall</th>
                      <th id="F1_NB-nb-article" scope="col">F1</th>
                      <th id="Support_NB-nb-article" scope="col">Support</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td headers="Class">0</td>
                      <td headers="Precision_BiLSTM">0.7621</td>
                      <td headers="Recall_BiLSTM" style="background-color: #D4F0D4">0.6748</td>
                      <td headers="F1_BiLSTM" style="background-color: #D4F0D4">0.7158</td>
                      <td headers="Support_BiLSTM" style="border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3">12729</td>
                      <td headers="Precision_NB" style="background-color: #D4F0D4">0.7698</td>
                      <td headers="Recall_NB">0.6214</td>
                      <td headers="F1_NB">0.6877</td>
                      <td headers="Support_NB">12729</td>
                    </tr>
                    <tr>
                      <td headers="Class">1</td>
                      <td headers="Precision_BiLSTM" style="background-color: #D4F0D4">0.8121</td>
                      <td headers="Recall_BiLSTM">0.8697</td>
                      <td headers="F1_BiLSTM" style="background-color: #D4F0D4">0.8399</td>
                      <td headers="Support_BiLSTM" style="border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3">20573</td>
                      <td headers="Precision_NB">0.7907</td>
                      <td headers="Recall_NB" style="background-color: #D4F0D4">0.8850</td>
                      <td headers="F1_NB">0.8352</td>
                      <td headers="Support_NB">20573</td>
                    </tr>
                    <tr>
                      <td headers="Class">Accuracy</td>
                      <td headers="Precision_BiLSTM">-</td>
                      <td headers="Recall_BiLSTM">-</td>
                      <td headers="F1_BiLSTM" style="background-color: #D4F0D4">0.7952</td>
                      <td headers="Support_BiLSTM" style="border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3">33302</td>
                      <td headers="Precision_NB">-</td>
                      <td headers="Recall_NB">-</td>
                      <td headers="F1_NB">0.7843</td>
                      <td headers="Support_NB">33302</td>
                    </tr>
                    <tr>
                      <td headers="Class">Macro avg</td>
                      <td headers="Precision_BiLSTM" style="background-color: #D4F0D4">0.7871</td>
                      <td headers="Recall_BiLSTM" style="background-color: #D4F0D4">0.7723</td>
                      <td headers="F1_BiLSTM" style="background-color: #D4F0D4">0.7779</td>
                      <td headers="Support_BiLSTM" style="border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3">33302</td>
                      <td headers="Precision_NB">0.7803</td>
                      <td headers="Recall_NB">0.7532</td>
                      <td headers="F1_NB">0.7615</td>
                      <td headers="Support_NB">33302</td>
                    </tr>
                    <tr>
                      <td headers="Class">Weighted avg</td>
                      <td headers="Precision_BiLSTM" style="background-color: #D4F0D4">0.7930</td>
                      <td headers="Recall_BiLSTM" style="background-color: #D4F0D4">0.7952</td>
                      <td headers="F1_BiLSTM" style="background-color: #D4F0D4">0.7925</td>
                      <td headers="Support_BiLSTM" style="border-right-width: 2px; border-right-style: solid; border-right-color: #d3d3d3">33302</td>
                      <td headers="Precision_NB">0.7827</td>
                      <td headers="Recall_NB">0.7843</td>
                      <td headers="F1_NB">0.7788</td>
                      <td headers="Support_NB">33302</td>
                    </tr>
                  </tbody>
                </table>
              </table-wrap>
            </fig>
          </sec>
        </sec>
      </sec>
      <sec id="limitations-and-possible-extensions-nb-article">
        <title>Limitations and possible extensions</title>
        <sec id="limitations-nb-article">
          <title>Limitations</title>
          <p>The primary constraint of the current model is its reliance on
    frozen feature extraction. Because the Transformer
    <inline-formula><alternatives><tex-math><![CDATA[\mathcal{F}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>‚Ñ±</mml:mi></mml:math></alternatives></inline-formula>
    functions purely as a static input generator with stop-gradient
    operations, the embedding space is unable to adapt to the specific
    political vernacular of the congressional dataset during
    backpropagation. This lack of domain adaptation is compounded by
    precision limitations; the utilization of a 4-bit quantized
    Transformer and the subsequent casting of embeddings to
    <monospace>float16</monospace> introduces quantization noise that,
    while efficient, may discard subtle semantic distinctions found in
    full-precision representations. Furthermore, the architecture
    exhibits potential redundancy by stacking a BiLSTM on top of a
    powerful contextual encoder. Since the MiniLM Transformer already
    captures long-range dependencies via self-attention, the addition of
    recurrence introduces a sequential bottleneck without necessarily
    increasing modeling capacity significantly. Finally, the masked mean
    pooling strategy treats all valid tokens as equally important, a
    simplification that risks diluting strong, localized discriminative
    signals within longer posts.</p>
        </sec>
        <sec id="extensions-nb-article">
          <title>Extensions</title>
          <p>To address these limitations, several architectural and training
    modifications could be implemented. A significant performance boost
    could be achieved through end-to-end fine-tuning, where the
    Transformer layers are unfrozen to allow gradients to flow through
    <inline-formula><alternatives><tex-math><![CDATA[\mathcal{F}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>‚Ñ±</mml:mi></mml:math></alternatives></inline-formula>.
    To manage the memory overhead of this approach on Apple Silicon,
    techniques such as Low-Rank Adaptation (LoRA) could be employed to
    fine-tune the encoder efficiently. Alternatively, efficiency could
    be prioritized by moving to a Transformer-only architecture,
    removing the BiLSTM layer entirely and projecting the Transformer‚Äôs
    <monospace>[CLS]</monospace> token directly to the classification
    head; this would simplify the compute graph and eliminate the
    sequential dependency. Regarding feature aggregation, replacing mean
    pooling with a learnable attention mechanism would allow the model
    to weigh informative keywords more heavily than neutral connective
    text. Finally, while the current model truncates inputs at 128
    tokens, implementing a sliding window approach or utilizing the full
    512-token capacity of MiniLM would better capture tail-end context
    in lengthy statements.</p>
        </sec>
      </sec>
      <sec id="conclusion-nb-article">
        <title>Conclusion</title>
        <p>The implemented pipeline offers a computationally efficient neural
  baseline for binary party attribution in political texts. By combining
  subword token IDs with frozen embeddings and a BiLSTM encoder, the
  system captures sequential patterns and achieves solid test
  performance. Evaluation reveals a mild bias toward predicting the
  majority class, visible in an elevated false-positive rate for class
  <inline-formula><alternatives><tex-math><![CDATA[0]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>0</mml:mn></mml:math></alternatives></inline-formula>.</p>
      </sec>
      <sec id="external-sources-used-nb-article">
        <title>External sources used</title>
        <list list-type="bullet">
          <list-item>
            <p>
              <ext-link ext-link-type="uri" xlink:href="https://gist.githubusercontent.com/deekayen/4148741/raw/98d35708fa344717d8eee15d11987de6c8e26d7d/1-1000.txt">list
      of most common english words</ext-link>
            </p>
          </list-item>
          <list-item>
            <p>
              <ext-link ext-link-type="uri" xlink:href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2FBOK1CF&amp;utm_source=chatgpt.com">Dataset</ext-link>
            </p>
          </list-item>
          <list-item>
            <p>
              <ext-link ext-link-type="uri" xlink:href="https://github.com/Blaizzy/mlx-embeddings">MLX-Embeddings
      repo</ext-link>
            </p>
          </list-item>
          <list-item>
            <p>
              <ext-link ext-link-type="uri" xlink:href="https://github.com/ml-explore/mlx">MLX
      Repositories</ext-link>
            </p>
          </list-item>
          <list-item>
            <p>
              <ext-link ext-link-type="uri" xlink:href="https://github.com/abeleinin/mlx-xLSTM">MLX
      Implementation of xLSTM</ext-link>
            </p>
          </list-item>
          <list-item>
            <p>
              <ext-link ext-link-type="uri" xlink:href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LSTM.html">Pytorch
      implementaino of LSTM</ext-link>
            </p>
          </list-item>
        </list>
        <p/>
      </sec>
    </body>
    <back>
</back>
  </sub-article>
</article>
